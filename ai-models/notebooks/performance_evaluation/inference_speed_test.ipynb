{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1861579",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BatteryMind - Inference Speed Test Notebook\n",
    "\n",
    "Comprehensive inference speed testing and performance benchmarking for all\n",
    "BatteryMind AI models including transformers, federated learning, reinforcement\n",
    "learning, and ensemble models.\n",
    "\n",
    "This notebook provides:\n",
    "- Batch inference speed testing\n",
    "- Real-time inference latency measurement\n",
    "- Throughput analysis under different loads\n",
    "- Memory usage during inference\n",
    "- Model comparison across different architectures\n",
    "- Edge deployment performance evaluation\n",
    "\n",
    "Author: BatteryMind Development Team\n",
    "Version: 1.0.0\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import psutil\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Memory profiling\n",
    "import tracemalloc\n",
    "from memory_profiler import profile\n",
    "import gc\n",
    "\n",
    "# Model imports\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from transformers.battery_health_predictor.predictor import BatteryHealthPredictor\n",
    "from transformers.degradation_forecaster.forecaster import DegradationForecaster\n",
    "from transformers.optimization_recommender.recommender import OptimizationRecommender\n",
    "from transformers.ensemble_model.ensemble import EnsembleModel\n",
    "from reinforcement_learning.agents.charging_agent import ChargingAgent\n",
    "from federated_learning.client_models.local_trainer import LocalTrainer\n",
    "\n",
    "# Utility imports\n",
    "from utils.data_utils import generate_test_data\n",
    "from utils.model_utils import load_model_artifacts\n",
    "from utils.visualization import plot_performance_metrics\n",
    "\n",
    "print(\"BatteryMind Inference Speed Test Notebook\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Configuration\n",
    "TEST_CONFIG = {\n",
    "    'batch_sizes': [1, 8, 16, 32, 64, 128],\n",
    "    'sequence_lengths': [100, 500, 1000, 2000],\n",
    "    'num_iterations': 100,\n",
    "    'warmup_iterations': 10,\n",
    "    'models_to_test': ['transformer', 'federated', 'rl', 'ensemble'],\n",
    "    'test_types': ['single_inference', 'batch_inference', 'concurrent_inference'],\n",
    "    'device_types': ['cpu', 'gpu'] if torch.cuda.is_available() else ['cpu']\n",
    "}\n",
    "\n",
    "class InferenceSpeedTester:\n",
    "    \"\"\"\n",
    "    Comprehensive inference speed testing framework for BatteryMind models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.results = {}\n",
    "        self.models = {}\n",
    "        self.test_data = {}\n",
    "        \n",
    "    def load_models(self):\n",
    "        \"\"\"Load all BatteryMind models for testing.\"\"\"\n",
    "        print(\"Loading BatteryMind models...\")\n",
    "        \n",
    "        # Load transformer models\n",
    "        self.models['battery_health'] = BatteryHealthPredictor.load_model(\n",
    "            '../../model-artifacts/trained_models/transformer_v1.0/model.pkl'\n",
    "        )\n",
    "        \n",
    "        self.models['degradation_forecaster'] = DegradationForecaster.load_model(\n",
    "            '../../model-artifacts/trained_models/transformer_v1.0/model.pkl'\n",
    "        )\n",
    "        \n",
    "        self.models['optimization_recommender'] = OptimizationRecommender.load_model(\n",
    "            '../../model-artifacts/trained_models/transformer_v1.0/model.pkl'\n",
    "        )\n",
    "        \n",
    "        # Load ensemble model\n",
    "        self.models['ensemble'] = EnsembleModel.load_model(\n",
    "            '../../model-artifacts/trained_models/ensemble_v1.0/ensemble_model.pkl'\n",
    "        )\n",
    "        \n",
    "        # Load RL agent\n",
    "        self.models['rl_agent'] = ChargingAgent.load_model(\n",
    "            '../../model-artifacts/trained_models/rl_agent_v1.0/policy_network.pt'\n",
    "        )\n",
    "        \n",
    "        # Load federated model\n",
    "        self.models['federated'] = LocalTrainer.load_model(\n",
    "            '../../model-artifacts/trained_models/federated_v1.0/global_model.pkl'\n",
    "        )\n",
    "        \n",
    "        print(f\"Loaded {len(self.models)} models successfully\")\n",
    "    \n",
    "    def generate_test_data(self):\n",
    "        \"\"\"Generate test data for different batch sizes and sequence lengths.\"\"\"\n",
    "        print(\"Generating test data...\")\n",
    "        \n",
    "        for batch_size in self.config['batch_sizes']:\n",
    "            for seq_len in self.config['sequence_lengths']:\n",
    "                # Battery telemetry data\n",
    "                battery_data = np.random.randn(batch_size, seq_len, 10)  # 10 features\n",
    "                \n",
    "                # Environmental data\n",
    "                env_data = np.random.randn(batch_size, seq_len, 5)  # 5 features\n",
    "                \n",
    "                # State data for RL\n",
    "                state_data = np.random.randn(batch_size, 20)  # 20 state features\n",
    "                \n",
    "                key = f\"batch_{batch_size}_seq_{seq_len}\"\n",
    "                self.test_data[key] = {\n",
    "                    'battery_data': battery_data,\n",
    "                    'env_data': env_data,\n",
    "                    'state_data': state_data\n",
    "                }\n",
    "        \n",
    "        print(f\"Generated test data for {len(self.test_data)} configurations\")\n",
    "    \n",
    "    def measure_inference_time(self, model, data, num_iterations=100, warmup=10):\n",
    "        \"\"\"\n",
    "        Measure inference time for a model with given data.\n",
    "        \n",
    "        Returns:\n",
    "            Dict with timing statistics\n",
    "        \"\"\"\n",
    "        # Warmup\n",
    "        for _ in range(warmup):\n",
    "            try:\n",
    "                _ = model.predict(data)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Clear cache\n",
    "        if hasattr(torch, 'cuda') and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Measure inference time\n",
    "        times = []\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            try:\n",
    "                output = model.predict(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during inference: {e}\")\n",
    "                continue\n",
    "            \n",
    "            end_time = time.perf_counter()\n",
    "            times.append(end_time - start_time)\n",
    "        \n",
    "        if not times:\n",
    "            return {'mean': 0, 'std': 0, 'min': 0, 'max': 0, 'p95': 0, 'p99': 0}\n",
    "        \n",
    "        times = np.array(times)\n",
    "        \n",
    "        return {\n",
    "            'mean': np.mean(times),\n",
    "            'std': np.std(times),\n",
    "            'min': np.min(times),\n",
    "            'max': np.max(times),\n",
    "            'p95': np.percentile(times, 95),\n",
    "            'p99': np.percentile(times, 99),\n",
    "            'throughput': len(times) / np.sum(times)\n",
    "        }\n",
    "    \n",
    "    def test_single_inference(self):\n",
    "        \"\"\"Test single inference speed for all models.\"\"\"\n",
    "        print(\"Testing single inference speed...\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for model_name, model in self.models.items():\n",
    "            print(f\"Testing {model_name}...\")\n",
    "            model_results = {}\n",
    "            \n",
    "            for batch_size in self.config['batch_sizes']:\n",
    "                for seq_len in self.config['sequence_lengths']:\n",
    "                    key = f\"batch_{batch_size}_seq_{seq_len}\"\n",
    "                    \n",
    "                    if key not in self.test_data:\n",
    "                        continue\n",
    "                    \n",
    "                    # Select appropriate data based on model type\n",
    "                    if model_name == 'rl_agent':\n",
    "                        data = self.test_data[key]['state_data']\n",
    "                    else:\n",
    "                        data = self.test_data[key]['battery_data']\n",
    "                    \n",
    "                    # Measure inference time\n",
    "                    timing_stats = self.measure_inference_time(\n",
    "                        model, data, \n",
    "                        num_iterations=self.config['num_iterations'],\n",
    "                        warmup=self.config['warmup_iterations']\n",
    "                    )\n",
    "                    \n",
    "                    model_results[key] = timing_stats\n",
    "            \n",
    "            results[model_name] = model_results\n",
    "        \n",
    "        self.results['single_inference'] = results\n",
    "        return results\n",
    "    \n",
    "    def test_batch_inference(self):\n",
    "        \"\"\"Test batch inference performance.\"\"\"\n",
    "        print(\"Testing batch inference performance...\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for model_name, model in self.models.items():\n",
    "            print(f\"Testing batch inference for {model_name}...\")\n",
    "            model_results = {}\n",
    "            \n",
    "            for batch_size in self.config['batch_sizes']:\n",
    "                # Use a fixed sequence length for batch testing\n",
    "                seq_len = 1000\n",
    "                key = f\"batch_{batch_size}_seq_{seq_len}\"\n",
    "                \n",
    "                if key not in self.test_data:\n",
    "                    continue\n",
    "                \n",
    "                # Select appropriate data\n",
    "                if model_name == 'rl_agent':\n",
    "                    data = self.test_data[key]['state_data']\n",
    "                else:\n",
    "                    data = self.test_data[key]['battery_data']\n",
    "                \n",
    "                # Measure batch inference time\n",
    "                start_time = time.perf_counter()\n",
    "                \n",
    "                try:\n",
    "                    output = model.predict(data)\n",
    "                    end_time = time.perf_counter()\n",
    "                    \n",
    "                    inference_time = end_time - start_time\n",
    "                    throughput = batch_size / inference_time\n",
    "                    \n",
    "                    model_results[f'batch_{batch_size}'] = {\n",
    "                        'inference_time': inference_time,\n",
    "                        'throughput': throughput,\n",
    "                        'time_per_sample': inference_time / batch_size\n",
    "                    }\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in batch inference for {model_name}: {e}\")\n",
    "                    model_results[f'batch_{batch_size}'] = {\n",
    "                        'inference_time': 0,\n",
    "                        'throughput': 0,\n",
    "                        'time_per_sample': 0\n",
    "                    }\n",
    "            \n",
    "            results[model_name] = model_results\n",
    "        \n",
    "        self.results['batch_inference'] = results\n",
    "        return results\n",
    "    \n",
    "    def test_concurrent_inference(self):\n",
    "        \"\"\"Test concurrent inference performance.\"\"\"\n",
    "        print(\"Testing concurrent inference performance...\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        def run_inference(model, data, num_requests=100):\n",
    "            \"\"\"Run multiple inference requests concurrently.\"\"\"\n",
    "            times = []\n",
    "            \n",
    "            def single_request():\n",
    "                start_time = time.perf_counter()\n",
    "                try:\n",
    "                    _ = model.predict(data)\n",
    "                    end_time = time.perf_counter()\n",
    "                    return end_time - start_time\n",
    "                except:\n",
    "                    return 0\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "                futures = [executor.submit(single_request) for _ in range(num_requests)]\n",
    "                times = [future.result() for future in futures]\n",
    "            \n",
    "            return times\n",
    "        \n",
    "        for model_name, model in self.models.items():\n",
    "            print(f\"Testing concurrent inference for {model_name}...\")\n",
    "            \n",
    "            # Use medium batch size and sequence length\n",
    "            batch_size = 32\n",
    "            seq_len = 1000\n",
    "            key = f\"batch_{batch_size}_seq_{seq_len}\"\n",
    "            \n",
    "            if key not in self.test_data:\n",
    "                continue\n",
    "            \n",
    "            if model_name == 'rl_agent':\n",
    "                data = self.test_data[key]['state_data']\n",
    "            else:\n",
    "                data = self.test_data[key]['battery_data']\n",
    "            \n",
    "            # Run concurrent requests\n",
    "            concurrent_times = run_inference(model, data, num_requests=50)\n",
    "            \n",
    "            if concurrent_times:\n",
    "                results[model_name] = {\n",
    "                    'mean_time': np.mean(concurrent_times),\n",
    "                    'std_time': np.std(concurrent_times),\n",
    "                    'min_time': np.min(concurrent_times),\n",
    "                    'max_time': np.max(concurrent_times),\n",
    "                    'p95_time': np.percentile(concurrent_times, 95),\n",
    "                    'requests_per_second': len(concurrent_times) / np.sum(concurrent_times)\n",
    "                }\n",
    "        \n",
    "        self.results['concurrent_inference'] = results\n",
    "        return results\n",
    "    \n",
    "    def measure_memory_usage(self):\n",
    "        \"\"\"Measure memory usage during inference.\"\"\"\n",
    "        print(\"Measuring memory usage...\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for model_name, model in self.models.items():\n",
    "            print(f\"Measuring memory usage for {model_name}...\")\n",
    "            \n",
    "            # Use large batch for memory testing\n",
    "            batch_size = 64\n",
    "            seq_len = 2000\n",
    "            key = f\"batch_{batch_size}_seq_{seq_len}\"\n",
    "            \n",
    "            if key not in self.test_data:\n",
    "                continue\n",
    "            \n",
    "            if model_name == 'rl_agent':\n",
    "                data = self.test_data[key]['state_data']\n",
    "            else:\n",
    "                data = self.test_data[key]['battery_data']\n",
    "            \n",
    "            # Measure memory before inference\n",
    "            gc.collect()\n",
    "            memory_before = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "            \n",
    "            # Start memory tracing\n",
    "            tracemalloc.start()\n",
    "            \n",
    "            try:\n",
    "                # Run inference\n",
    "                output = model.predict(data)\n",
    "                \n",
    "                # Measure memory after inference\n",
    "                current, peak = tracemalloc.get_traced_memory()\n",
    "                memory_after = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "                \n",
    "                results[model_name] = {\n",
    "                    'memory_before_mb': memory_before,\n",
    "                    'memory_after_mb': memory_after,\n",
    "                    'memory_delta_mb': memory_after - memory_before,\n",
    "                    'peak_memory_mb': peak / 1024 / 1024,\n",
    "                    'current_memory_mb': current / 1024 / 1024\n",
    "                }\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error measuring memory for {model_name}: {e}\")\n",
    "                results[model_name] = {\n",
    "                    'memory_before_mb': 0,\n",
    "                    'memory_after_mb': 0,\n",
    "                    'memory_delta_mb': 0,\n",
    "                    'peak_memory_mb': 0,\n",
    "                    'current_memory_mb': 0\n",
    "                }\n",
    "            \n",
    "            finally:\n",
    "                tracemalloc.stop()\n",
    "        \n",
    "        self.results['memory_usage'] = results\n",
    "        return results\n",
    "    \n",
    "    def generate_performance_report(self):\n",
    "        \"\"\"Generate comprehensive performance report.\"\"\"\n",
    "        print(\"Generating performance report...\")\n",
    "        \n",
    "        report = {\n",
    "            'test_configuration': self.config,\n",
    "            'models_tested': list(self.models.keys()),\n",
    "            'test_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'results': self.results\n",
    "        }\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        summary = {}\n",
    "        \n",
    "        if 'single_inference' in self.results:\n",
    "            summary['fastest_model'] = self._find_fastest_model()\n",
    "            summary['most_efficient_model'] = self._find_most_efficient_model()\n",
    "        \n",
    "        if 'memory_usage' in self.results:\n",
    "            summary['lowest_memory_model'] = self._find_lowest_memory_model()\n",
    "        \n",
    "        report['summary'] = summary\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def _find_fastest_model(self):\n",
    "        \"\"\"Find the fastest model across all configurations.\"\"\"\n",
    "        fastest_times = {}\n",
    "        \n",
    "        for model_name, model_results in self.results['single_inference'].items():\n",
    "            avg_times = []\n",
    "            for config, timing_stats in model_results.items():\n",
    "                avg_times.append(timing_stats['mean'])\n",
    "            \n",
    "            if avg_times:\n",
    "                fastest_times[model_name] = np.mean(avg_times)\n",
    "        \n",
    "        if fastest_times:\n",
    "            return min(fastest_times, key=fastest_times.get)\n",
    "        return None\n",
    "    \n",
    "    def _find_most_efficient_model(self):\n",
    "        \"\"\"Find the most efficient model (highest throughput).\"\"\"\n",
    "        throughputs = {}\n",
    "        \n",
    "        for model_name, model_results in self.results['single_inference'].items():\n",
    "            avg_throughputs = []\n",
    "            for config, timing_stats in model_results.items():\n",
    "                avg_throughputs.append(timing_stats['throughput'])\n",
    "            \n",
    "            if avg_throughputs:\n",
    "                throughputs[model_name] = np.mean(avg_throughputs)\n",
    "        \n",
    "        if throughputs:\n",
    "            return max(throughputs, key=throughputs.get)\n",
    "        return None\n",
    "    \n",
    "    def _find_lowest_memory_model(self):\n",
    "        \"\"\"Find the model with lowest memory usage.\"\"\"\n",
    "        memory_usage = {}\n",
    "        \n",
    "        for model_name, memory_stats in self.results['memory_usage'].items():\n",
    "            memory_usage[model_name] = memory_stats['memory_delta_mb']\n",
    "        \n",
    "        if memory_usage:\n",
    "            return min(memory_usage, key=memory_usage.get)\n",
    "        return None\n",
    "    \n",
    "    def visualize_results(self):\n",
    "        \"\"\"Create visualizations of the inference speed test results.\"\"\"\n",
    "        print(\"Creating visualizations...\")\n",
    "        \n",
    "        # Set up plotting style\n",
    "        plt.style.use('seaborn-v0_8')\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # 1. Single inference latency comparison\n",
    "        if 'single_inference' in self.results:\n",
    "            ax1 = axes[0, 0]\n",
    "            self._plot_single_inference_latency(ax1)\n",
    "            ax1.set_title('Single Inference Latency by Model')\n",
    "            ax1.set_xlabel('Model')\n",
    "            ax1.set_ylabel('Latency (seconds)')\n",
    "        \n",
    "        # 2. Batch inference throughput\n",
    "        if 'batch_inference' in self.results:\n",
    "            ax2 = axes[0, 1]\n",
    "            self._plot_batch_throughput(ax2)\n",
    "            ax2.set_title('Batch Inference Throughput')\n",
    "            ax2.set_xlabel('Batch Size')\n",
    "            ax2.set_ylabel('Throughput (samples/second)')\n",
    "        \n",
    "        # 3. Memory usage comparison\n",
    "        if 'memory_usage' in self.results:\n",
    "            ax3 = axes[1, 0]\n",
    "            self._plot_memory_usage(ax3)\n",
    "            ax3.set_title('Memory Usage by Model')\n",
    "            ax3.set_xlabel('Model')\n",
    "            ax3.set_ylabel('Memory Usage (MB)')\n",
    "        \n",
    "        # 4. Concurrent inference performance\n",
    "        if 'concurrent_inference' in self.results:\n",
    "            ax4 = axes[1, 1]\n",
    "            self._plot_concurrent_performance(ax4)\n",
    "            ax4.set_title('Concurrent Inference Performance')\n",
    "            ax4.set_xlabel('Model')\n",
    "            ax4.set_ylabel('Requests per Second')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('inference_speed_test_results.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def _plot_single_inference_latency(self, ax):\n",
    "        \"\"\"Plot single inference latency comparison.\"\"\"\n",
    "        model_names = []\n",
    "        latencies = []\n",
    "        \n",
    "        for model_name, model_results in self.results['single_inference'].items():\n",
    "            avg_latency = np.mean([stats['mean'] for stats in model_results.values()])\n",
    "            model_names.append(model_name)\n",
    "            latencies.append(avg_latency)\n",
    "        \n",
    "        ax.bar(model_names, latencies)\n",
    "        ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    \n",
    "    def _plot_batch_throughput(self, ax):\n",
    "        \"\"\"Plot batch inference throughput.\"\"\"\n",
    "        for model_name, model_results in self.results['batch_inference'].items():\n",
    "            batch_sizes = []\n",
    "            throughputs = []\n",
    "            \n",
    "            for batch_config, stats in model_results.items():\n",
    "                batch_size = int(batch_config.split('_')[1])\n",
    "                batch_sizes.append(batch_size)\n",
    "                throughputs.append(stats['throughput'])\n",
    "            \n",
    "            ax.plot(batch_sizes, throughputs, marker='o', label=model_name)\n",
    "        \n",
    "        ax.legend()\n",
    "        ax.set_xscale('log')\n",
    "    \n",
    "    def _plot_memory_usage(self, ax):\n",
    "        \"\"\"Plot memory usage comparison.\"\"\"\n",
    "        model_names = []\n",
    "        memory_usage = []\n",
    "        \n",
    "        for model_name, memory_stats in self.results['memory_usage'].items():\n",
    "            model_names.append(model_name)\n",
    "            memory_usage.append(memory_stats['memory_delta_mb'])\n",
    "        \n",
    "        ax.bar(model_names, memory_usage)\n",
    "        ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    \n",
    "    def _plot_concurrent_performance(self, ax):\n",
    "        \"\"\"Plot concurrent inference performance.\"\"\"\n",
    "        model_names = []\n",
    "        rps = []\n",
    "        \n",
    "        for model_name, stats in self.results['concurrent_inference'].items():\n",
    "            model_names.append(model_name)\n",
    "            rps.append(stats['requests_per_second'])\n",
    "        \n",
    "        ax.bar(model_names, rps)\n",
    "        ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "\n",
    "# Run the inference speed tests\n",
    "print(\"Initializing Inference Speed Tester...\")\n",
    "tester = InferenceSpeedTester(TEST_CONFIG)\n",
    "\n",
    "print(\"Loading models...\")\n",
    "tester.load_models()\n",
    "\n",
    "print(\"Generating test data...\")\n",
    "tester.generate_test_data()\n",
    "\n",
    "print(\"Running single inference tests...\")\n",
    "single_results = tester.test_single_inference()\n",
    "\n",
    "print(\"Running batch inference tests...\")\n",
    "batch_results = tester.test_batch_inference()\n",
    "\n",
    "print(\"Running concurrent inference tests...\")\n",
    "concurrent_results = tester.test_concurrent_inference()\n",
    "\n",
    "print(\"Measuring memory usage...\")\n",
    "memory_results = tester.measure_memory_usage()\n",
    "\n",
    "print(\"Generating performance report...\")\n",
    "performance_report = tester.generate_performance_report()\n",
    "\n",
    "print(\"Creating visualizations...\")\n",
    "tester.visualize_results()\n",
    "\n",
    "# Save results\n",
    "import json\n",
    "with open('inference_speed_test_results.json', 'w') as f:\n",
    "    json.dump(performance_report, f, indent=2, default=str)\n",
    "\n",
    "print(\"Inference speed test completed!\")\n",
    "print(\"Results saved to 'inference_speed_test_results.json'\")\n",
    "print(\"Visualizations saved to 'inference_speed_test_results.png'\")\n",
    "\n",
    "# Display summary results\n",
    "print(\"\\nSUMMARY RESULTS:\")\n",
    "print(\"=\"*50)\n",
    "if 'summary' in performance_report:\n",
    "    summary = performance_report['summary']\n",
    "    print(f\"Fastest Model: {summary.get('fastest_model', 'N/A')}\")\n",
    "    print(f\"Most Efficient Model: {summary.get('most_efficient_model', 'N/A')}\")\n",
    "    print(f\"Lowest Memory Model: {summary.get('lowest_memory_model', 'N/A')}\")\n",
    "\n",
    "# Display detailed results table\n",
    "print(\"\\nDETAILED RESULTS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if 'single_inference' in performance_report['results']:\n",
    "    print(\"\\nSingle Inference Latency (seconds):\")\n",
    "    for model_name, model_results in performance_report['results']['single_inference'].items():\n",
    "        avg_latency = np.mean([stats['mean'] for stats in model_results.values()])\n",
    "        print(f\"{model_name}: {avg_latency:.4f}\")\n",
    "\n",
    "if 'memory_usage' in performance_report['results']:\n",
    "    print(\"\\nMemory Usage (MB):\")\n",
    "    for model_name, memory_stats in performance_report['results']['memory_usage'].items():\n",
    "        print(f\"{model_name}: {memory_stats['memory_delta_mb']:.2f}\")\n",
    "\n",
    "print(\"\\nInference Speed Test Completed Successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
