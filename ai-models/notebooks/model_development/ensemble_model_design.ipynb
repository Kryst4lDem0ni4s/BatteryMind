{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee9aa6f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BatteryMind - Ensemble Model Design and Development Notebook\n",
    "\n",
    "Comprehensive ensemble model development for battery management systems.\n",
    "Combines transformer predictions, reinforcement learning decisions, and\n",
    "physics-based constraints for robust battery optimization.\n",
    "\n",
    "Features:\n",
    "- Multi-model ensemble architecture\n",
    "- Transformer + RL integration\n",
    "- Physics-based constraint validation\n",
    "- Adaptive model weighting\n",
    "- Uncertainty quantification\n",
    "- Real-time prediction optimization\n",
    "- Performance benchmarking\n",
    "\n",
    "Author: BatteryMind Development Team\n",
    "Version: 1.0.0\n",
    "\"\"\"\n",
    "\n",
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Import BatteryMind components\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from transformers.ensemble_model import EnsembleModel, VotingClassifier, StackingRegressor, ModelFusion\n",
    "from transformers.battery_health_predictor import BatteryHealthPredictor\n",
    "from transformers.degradation_forecaster import DegradationForecaster\n",
    "from transformers.optimization_recommender import OptimizationRecommender\n",
    "from reinforcement_learning.agents import ChargingAgent\n",
    "from reinforcement_learning.environments import BatteryEnvironment\n",
    "from training_data.generators import SyntheticDataGenerator\n",
    "from evaluation.metrics import accuracy_metrics, performance_metrics\n",
    "from utils.model_utils import load_model, save_model\n",
    "from utils.logging_utils import setup_logger\n",
    "\n",
    "# Configure logging\n",
    "logger = setup_logger('ensemble_development', 'ensemble_model_design.log')\n",
    "\n",
    "print(\"üîã BatteryMind Ensemble Model Development Environment\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. DATA PREPARATION AND LOADING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n1. Data Preparation and Loading\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Generate synthetic training data\n",
    "data_generator = SyntheticDataGenerator(\n",
    "    num_batteries=1000,\n",
    "    simulation_days=30,\n",
    "    sampling_rate=1.0  # 1 Hz sampling\n",
    ")\n",
    "\n",
    "print(\"üìä Generating synthetic training data...\")\n",
    "synthetic_data = data_generator.generate_fleet_data()\n",
    "print(f\"‚úì Generated {len(synthetic_data)} data points\")\n",
    "\n",
    "# Prepare data for different model types\n",
    "def prepare_ensemble_data(data):\n",
    "    \"\"\"Prepare data for different ensemble components.\"\"\"\n",
    "    \n",
    "    # Features for transformer models\n",
    "    transformer_features = [\n",
    "        'voltage', 'current', 'temperature', 'soc', 'soh',\n",
    "        'internal_resistance', 'power_demand', 'ambient_temperature',\n",
    "        'cycle_count', 'age_days'\n",
    "    ]\n",
    "    \n",
    "    # Features for RL models\n",
    "    rl_features = [\n",
    "        'soc', 'temperature', 'voltage', 'current', 'power_demand',\n",
    "        'soh', 'internal_resistance'\n",
    "    ]\n",
    "    \n",
    "    # Target variables\n",
    "    targets = {\n",
    "        'health': 'soh',\n",
    "        'degradation': 'soh',  # Future SoH prediction\n",
    "        'optimization': 'power_demand'  # Optimal power recommendation\n",
    "    }\n",
    "    \n",
    "    prepared_data = {\n",
    "        'transformer_features': data[transformer_features],\n",
    "        'rl_features': data[rl_features],\n",
    "        'targets': {k: data[v] for k, v in targets.items()}\n",
    "    }\n",
    "    \n",
    "    return prepared_data\n",
    "\n",
    "# Prepare data\n",
    "ensemble_data = prepare_ensemble_data(synthetic_data)\n",
    "\n",
    "print(f\"‚úì Data prepared for ensemble models\")\n",
    "print(f\"  - Transformer features: {ensemble_data['transformer_features'].shape}\")\n",
    "print(f\"  - RL features: {ensemble_data['rl_features'].shape}\")\n",
    "print(f\"  - Target variables: {len(ensemble_data['targets'])}\")\n",
    "\n",
    "# Split data for training and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split transformer data\n",
    "X_transformer = ensemble_data['transformer_features']\n",
    "y_health = ensemble_data['targets']['health']\n",
    "\n",
    "X_train_tf, X_val_tf, y_train_health, y_val_health = train_test_split(\n",
    "    X_transformer, y_health, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Split RL data\n",
    "X_rl = ensemble_data['rl_features']\n",
    "y_optimization = ensemble_data['targets']['optimization']\n",
    "\n",
    "X_train_rl, X_val_rl, y_train_opt, y_val_opt = train_test_split(\n",
    "    X_rl, y_optimization, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"‚úì Data split completed\")\n",
    "print(f\"  - Training samples: {len(X_train_tf)}\")\n",
    "print(f\"  - Validation samples: {len(X_val_tf)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. INDIVIDUAL MODEL DEVELOPMENT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n2. Individual Model Development\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 2.1 Transformer-based Battery Health Predictor\n",
    "print(\"ü§ñ Developing Battery Health Predictor...\")\n",
    "\n",
    "# Simple transformer-like model for demonstration\n",
    "class BatteryHealthTransformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism (simplified)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1),\n",
    "            nn.Sigmoid()  # SoH between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Feature extraction\n",
    "        features = self.feature_extractor(x)\n",
    "        \n",
    "        # Add sequence dimension for attention\n",
    "        features = features.unsqueeze(1)\n",
    "        \n",
    "        # Apply attention\n",
    "        attended, _ = self.attention(features, features, features)\n",
    "        \n",
    "        # Remove sequence dimension\n",
    "        attended = attended.squeeze(1)\n",
    "        \n",
    "        # Output prediction\n",
    "        output = self.output_layer(attended)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Initialize and train transformer model\n",
    "health_transformer = BatteryHealthTransformer(\n",
    "    input_size=X_train_tf.shape[1],\n",
    "    hidden_size=128,\n",
    "    num_layers=2\n",
    ")\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(health_transformer.parameters(), lr=0.001)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_tf.values)\n",
    "y_train_tensor = torch.FloatTensor(y_train_health.values).unsqueeze(1)\n",
    "X_val_tensor = torch.FloatTensor(X_val_tf.values)\n",
    "y_val_tensor = torch.FloatTensor(y_val_health.values).unsqueeze(1)\n",
    "\n",
    "# Training loop\n",
    "print(\"  üìà Training transformer model...\")\n",
    "transformer_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    # Training\n",
    "    health_transformer.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = health_transformer(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    transformer_losses.append(loss.item())\n",
    "    \n",
    "    # Validation\n",
    "    health_transformer.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = health_transformer(X_val_tensor)\n",
    "        val_loss = criterion(val_outputs, y_val_tensor)\n",
    "        val_losses.append(val_loss.item())\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"    Epoch {epoch}: Train Loss = {loss.item():.4f}, Val Loss = {val_loss.item():.4f}\")\n",
    "\n",
    "print(\"‚úì Transformer model trained\")\n",
    "\n",
    "# 2.2 Physics-based Model\n",
    "print(\"üî¨ Developing Physics-based Model...\")\n",
    "\n",
    "class PhysicsBasedModel:\n",
    "    \"\"\"Physics-based battery model for SoH prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_params = {\n",
    "            'calendar_aging_factor': 0.001,\n",
    "            'cycle_aging_factor': 0.0001,\n",
    "            'temperature_factor': 0.01,\n",
    "            'capacity_fade_rate': 0.02\n",
    "        }\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict SoH based on physics equations.\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for _, row in X.iterrows():\n",
    "            # Extract relevant features\n",
    "            temperature = row.get('temperature', 25.0)\n",
    "            cycle_count = row.get('cycle_count', 0)\n",
    "            age_days = row.get('age_days', 0)\n",
    "            current_load = row.get('current', 0)\n",
    "            \n",
    "            # Calendar aging\n",
    "            calendar_aging = (self.model_params['calendar_aging_factor'] * \n",
    "                            age_days * np.exp(0.1 * (temperature - 25)))\n",
    "            \n",
    "            # Cycle aging\n",
    "            cycle_aging = (self.model_params['cycle_aging_factor'] * \n",
    "                          cycle_count * (1 + abs(current_load) / 100))\n",
    "            \n",
    "            # Temperature effects\n",
    "            temp_effect = self.model_params['temperature_factor'] * max(0, temperature - 40)\n",
    "            \n",
    "            # Combine aging effects\n",
    "            total_aging = calendar_aging + cycle_aging + temp_effect\n",
    "            \n",
    "            # Calculate SoH\n",
    "            soh = max(0.5, 1.0 - total_aging)\n",
    "            predictions.append(soh)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "\n",
    "physics_model = PhysicsBasedModel()\n",
    "\n",
    "# Test physics model\n",
    "physics_predictions = physics_model.predict(X_val_tf)\n",
    "physics_mae = mean_absolute_error(y_val_health, physics_predictions)\n",
    "print(f\"‚úì Physics model MAE: {physics_mae:.4f}\")\n",
    "\n",
    "# 2.3 Traditional ML Models\n",
    "print(\"üìä Developing Traditional ML Models...\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Initialize traditional models\n",
    "traditional_models = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'SVR': SVR(kernel='rbf', C=1.0)\n",
    "}\n",
    "\n",
    "# Train traditional models\n",
    "traditional_predictions = {}\n",
    "traditional_scores = {}\n",
    "\n",
    "for name, model in traditional_models.items():\n",
    "    print(f\"  üîß Training {name}...\")\n",
    "    model.fit(X_train_tf, y_train_health)\n",
    "    \n",
    "    # Predictions\n",
    "    predictions = model.predict(X_val_tf)\n",
    "    traditional_predictions[name] = predictions\n",
    "    \n",
    "    # Scores\n",
    "    mae = mean_absolute_error(y_val_health, predictions)\n",
    "    mse = mean_squared_error(y_val_health, predictions)\n",
    "    r2 = r2_score(y_val_health, predictions)\n",
    "    \n",
    "    traditional_scores[name] = {\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'R2': r2\n",
    "    }\n",
    "    \n",
    "    print(f\"    ‚úì {name} - MAE: {mae:.4f}, R2: {r2:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ENSEMBLE ARCHITECTURE DESIGN\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n3. Ensemble Architecture Design\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 3.1 Voting Ensemble\n",
    "print(\"üó≥Ô∏è Creating Voting Ensemble...\")\n",
    "\n",
    "class VotingEnsemble:\n",
    "    \"\"\"Voting ensemble combining multiple models.\"\"\"\n",
    "    \n",
    "    def __init__(self, models, weights=None):\n",
    "        self.models = models\n",
    "        self.weights = weights or [1/len(models)] * len(models)\n",
    "        self.model_names = list(models.keys())\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions using weighted voting.\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            if name == 'Transformer':\n",
    "                # Handle transformer model\n",
    "                X_tensor = torch.FloatTensor(X.values)\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    pred = model(X_tensor).numpy().flatten()\n",
    "            elif name == 'Physics':\n",
    "                pred = model.predict(X)\n",
    "            else:\n",
    "                pred = model.predict(X)\n",
    "            \n",
    "            predictions.append(pred)\n",
    "        \n",
    "        # Weighted average\n",
    "        predictions = np.array(predictions)\n",
    "        weighted_pred = np.average(predictions, axis=0, weights=self.weights)\n",
    "        \n",
    "        return weighted_pred\n",
    "    \n",
    "    def get_individual_predictions(self, X):\n",
    "        \"\"\"Get predictions from each model.\"\"\"\n",
    "        individual_preds = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            if name == 'Transformer':\n",
    "                X_tensor = torch.FloatTensor(X.values)\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    pred = model(X_tensor).numpy().flatten()\n",
    "            elif name == 'Physics':\n",
    "                pred = model.predict(X)\n",
    "            else:\n",
    "                pred = model.predict(X)\n",
    "            \n",
    "            individual_preds[name] = pred\n",
    "        \n",
    "        return individual_preds\n",
    "\n",
    "# Create voting ensemble\n",
    "voting_models = {\n",
    "    'Transformer': health_transformer,\n",
    "    'Physics': physics_model,\n",
    "    'Random Forest': traditional_models['Random Forest'],\n",
    "    'Gradient Boosting': traditional_models['Gradient Boosting']\n",
    "}\n",
    "\n",
    "voting_ensemble = VotingEnsemble(voting_models)\n",
    "\n",
    "# Test voting ensemble\n",
    "voting_predictions = voting_ensemble.predict(X_val_tf)\n",
    "voting_mae = mean_absolute_error(y_val_health, voting_predictions)\n",
    "voting_r2 = r2_score(y_val_health, voting_predictions)\n",
    "\n",
    "print(f\"‚úì Voting Ensemble - MAE: {voting_mae:.4f}, R2: {voting_r2:.4f}\")\n",
    "\n",
    "# 3.2 Stacking Ensemble\n",
    "print(\"ü•û Creating Stacking Ensemble...\")\n",
    "\n",
    "class StackingEnsemble:\n",
    "    \"\"\"Stacking ensemble with meta-learner.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_models, meta_model=None):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model or LinearRegression()\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit base models and meta-learner.\"\"\"\n",
    "        # Fit base models\n",
    "        for name, model in self.base_models.items():\n",
    "            if name not in ['Transformer', 'Physics']:\n",
    "                model.fit(X, y)\n",
    "        \n",
    "        # Generate base predictions for meta-learner\n",
    "        base_predictions = self._get_base_predictions(X)\n",
    "        \n",
    "        # Fit meta-learner\n",
    "        self.meta_model.fit(base_predictions, y)\n",
    "        self.is_fitted = True\n",
    "    \n",
    "    def _get_base_predictions(self, X):\n",
    "        \"\"\"Get predictions from base models.\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for name, model in self.base_models.items():\n",
    "            if name == 'Transformer':\n",
    "                X_tensor = torch.FloatTensor(X.values)\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    pred = model(X_tensor).numpy().flatten()\n",
    "            elif name == 'Physics':\n",
    "                pred = model.predict(X)\n",
    "            else:\n",
    "                pred = model.predict(X)\n",
    "            \n",
    "            predictions.append(pred)\n",
    "        \n",
    "        return np.column_stack(predictions)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions using stacking.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Ensemble must be fitted before prediction\")\n",
    "        \n",
    "        base_predictions = self._get_base_predictions(X)\n",
    "        return self.meta_model.predict(base_predictions)\n",
    "\n",
    "# Create and train stacking ensemble\n",
    "stacking_ensemble = StackingEnsemble(voting_models)\n",
    "stacking_ensemble.fit(X_train_tf, y_train_health)\n",
    "\n",
    "# Test stacking ensemble\n",
    "stacking_predictions = stacking_ensemble.predict(X_val_tf)\n",
    "stacking_mae = mean_absolute_error(y_val_health, stacking_predictions)\n",
    "stacking_r2 = r2_score(y_val_health, stacking_predictions)\n",
    "\n",
    "print(f\"‚úì Stacking Ensemble - MAE: {stacking_mae:.4f}, R2: {stacking_r2:.4f}\")\n",
    "\n",
    "# 3.3 Adaptive Ensemble with Uncertainty Quantification\n",
    "print(\"üéØ Creating Adaptive Ensemble...\")\n",
    "\n",
    "class AdaptiveEnsemble:\n",
    "    \"\"\"Adaptive ensemble with dynamic weighting and uncertainty quantification.\"\"\"\n",
    "    \n",
    "    def __init__(self, models, adaptation_rate=0.1):\n",
    "        self.models = models\n",
    "        self.adaptation_rate = adaptation_rate\n",
    "        self.weights = np.ones(len(models)) / len(models)\n",
    "        self.model_names = list(models.keys())\n",
    "        self.performance_history = []\n",
    "    \n",
    "    def predict_with_uncertainty(self, X):\n",
    "        \"\"\"Make predictions with uncertainty estimates.\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        # Get individual predictions\n",
    "        for name, model in self.models.items():\n",
    "            if name == 'Transformer':\n",
    "                X_tensor = torch.FloatTensor(X.values)\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    pred = model(X_tensor).numpy().flatten()\n",
    "            elif name == 'Physics':\n",
    "                pred = model.predict(X)\n",
    "            else:\n",
    "                pred = model.predict(X)\n",
    "            \n",
    "            predictions.append(pred)\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        \n",
    "        # Weighted prediction\n",
    "        ensemble_pred = np.average(predictions, axis=0, weights=self.weights)\n",
    "        \n",
    "        # Uncertainty as weighted standard deviation\n",
    "        uncertainty = np.sqrt(np.average((predictions - ensemble_pred)**2, axis=0, weights=self.weights))\n",
    "        \n",
    "        return ensemble_pred, uncertainty\n",
    "    \n",
    "    def update_weights(self, X, y_true):\n",
    "        \"\"\"Update model weights based on recent performance.\"\"\"\n",
    "        # Get individual predictions\n",
    "        individual_preds = {}\n",
    "        for name, model in self.models.items():\n",
    "            if name == 'Transformer':\n",
    "                X_tensor = torch.FloatTensor(X.values)\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    pred = model(X_tensor).numpy().flatten()\n",
    "            elif name == 'Physics':\n",
    "                pred = model.predict(X)\n",
    "            else:\n",
    "                pred = model.predict(X)\n",
    "            \n",
    "            individual_preds[name] = pred\n",
    "        \n",
    "        # Calculate individual errors\n",
    "        errors = []\n",
    "        for name in self.model_names:\n",
    "            error = mean_absolute_error(y_true, individual_preds[name])\n",
    "            errors.append(error)\n",
    "        \n",
    "        # Update weights (lower error = higher weight)\n",
    "        errors = np.array(errors)\n",
    "        inv_errors = 1 / (errors + 1e-8)  # Avoid division by zero\n",
    "        new_weights = inv_errors / np.sum(inv_errors)\n",
    "        \n",
    "        # Smooth weight updates\n",
    "        self.weights = (1 - self.adaptation_rate) * self.weights + self.adaptation_rate * new_weights\n",
    "        \n",
    "        # Store performance history\n",
    "        self.performance_history.append({\n",
    "            'errors': errors,\n",
    "            'weights': self.weights.copy()\n",
    "        })\n",
    "\n",
    "# Create adaptive ensemble\n",
    "adaptive_ensemble = AdaptiveEnsemble(voting_models)\n",
    "\n",
    "# Test adaptive ensemble\n",
    "adaptive_predictions, uncertainty = adaptive_ensemble.predict_with_uncertainty(X_val_tf)\n",
    "adaptive_mae = mean_absolute_error(y_val_health, adaptive_predictions)\n",
    "adaptive_r2 = r2_score(y_val_health, adaptive_predictions)\n",
    "\n",
    "print(f\"‚úì Adaptive Ensemble - MAE: {adaptive_mae:.4f}, R2: {adaptive_r2:.4f}\")\n",
    "\n",
    "# Update weights and test again\n",
    "adaptive_ensemble.update_weights(X_val_tf, y_val_health)\n",
    "adaptive_predictions_updated, uncertainty_updated = adaptive_ensemble.predict_with_uncertainty(X_val_tf)\n",
    "adaptive_mae_updated = mean_absolute_error(y_val_health, adaptive_predictions_updated)\n",
    "\n",
    "print(f\"‚úì Adaptive Ensemble (Updated) - MAE: {adaptive_mae_updated:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. PERFORMANCE EVALUATION AND COMPARISON\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n4. Performance Evaluation and Comparison\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Compile all ensemble results\n",
    "ensemble_results = {\n",
    "    'Voting': {\n",
    "        'predictions': voting_predictions,\n",
    "        'mae': voting_mae,\n",
    "        'r2': voting_r2\n",
    "    },\n",
    "    'Stacking': {\n",
    "        'predictions': stacking_predictions,\n",
    "        'mae': stacking_mae,\n",
    "        'r2': stacking_r2\n",
    "    },\n",
    "    'Adaptive': {\n",
    "        'predictions': adaptive_predictions,\n",
    "        'mae': adaptive_mae,\n",
    "        'r2': adaptive_r2\n",
    "    },\n",
    "    'Adaptive (Updated)': {\n",
    "        'predictions': adaptive_predictions_updated,\n",
    "        'mae': adaptive_mae_updated,\n",
    "        'r2': r2_score(y_val_health, adaptive_predictions_updated)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add individual model results for comparison\n",
    "for name, scores in traditional_scores.items():\n",
    "    ensemble_results[name] = {\n",
    "        'predictions': traditional_predictions[name],\n",
    "        'mae': scores['MAE'],\n",
    "        'r2': scores['R2']\n",
    "    }\n",
    "\n",
    "# Add physics model\n",
    "ensemble_results['Physics'] = {\n",
    "    'predictions': physics_predictions,\n",
    "    'mae': physics_mae,\n",
    "    'r2': r2_score(y_val_health, physics_predictions)\n",
    "}\n",
    "\n",
    "# Create performance comparison plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Ensemble Model Performance Comparison', fontsize=16)\n",
    "\n",
    "# Plot 1: MAE Comparison\n",
    "ax1 = axes[0, 0]\n",
    "models = list(ensemble_results.keys())\n",
    "mae_values = [ensemble_results[m]['mae'] for m in models]\n",
    "\n",
    "bars = ax1.bar(models, mae_values, color=plt.cm.viridis(np.linspace(0, 1, len(models))))\n",
    "ax1.set_ylabel('Mean Absolute Error')\n",
    "ax1.set_title('MAE Comparison')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, mae_values):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "             f'{value:.4f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Plot 2: R¬≤ Comparison\n",
    "ax2 = axes[0, 1]\n",
    "r2_values = [ensemble_results[m]['r2'] for m in models]\n",
    "\n",
    "bars = ax2.bar(models, r2_values, color=plt.cm.plasma(np.linspace(0, 1, len(models))))\n",
    "ax2.set_ylabel('R¬≤ Score')\n",
    "ax2.set_title('R¬≤ Comparison')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, r2_values):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{value:.4f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Plot 3: Prediction vs Actual for Best Model\n",
    "ax3 = axes[1, 0]\n",
    "best_model = max(models, key=lambda m: ensemble_results[m]['r2'])\n",
    "best_predictions = ensemble_results[best_model]['predictions']\n",
    "\n",
    "ax3.scatter(y_test, best_predictions, alpha=0.6, color='blue', s=20)\n",
    "ax3.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "ax3.set_xlabel('Actual SoH')\n",
    "ax3.set_ylabel('Predicted SoH')\n",
    "ax3.set_title(f'Best Model: {best_model}')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add R¬≤ score on plot\n",
    "ax3.text(0.05, 0.95, f'R¬≤ = {ensemble_results[best_model][\"r2\"]:.4f}', \n",
    "         transform=ax3.transAxes, bbox=dict(boxstyle=\"round\", facecolor='white', alpha=0.8))\n",
    "\n",
    "# Plot 4: Residuals for Best Model\n",
    "ax4 = axes[1, 1]\n",
    "residuals = y_test - best_predictions\n",
    "\n",
    "ax4.scatter(best_predictions, residuals, alpha=0.6, color='red', s=20)\n",
    "ax4.axhline(y=0, color='black', linestyle='--', alpha=0.8)\n",
    "ax4.set_xlabel('Predicted SoH')\n",
    "ax4.set_ylabel('Residuals')\n",
    "ax4.set_title(f'Residuals: {best_model}')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED ENSEMBLE MODEL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_name, results in ensemble_results.items():\n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    print(f\"  MAE: {results['mae']:.6f}\")\n",
    "    print(f\"  MSE: {results['mse']:.6f}\")\n",
    "    print(f\"  R¬≤:  {results['r2']:.6f}\")\n",
    "    print(f\"  Training Time: {results['training_time']:.2f} seconds\")\n",
    "\n",
    "# Identify best performing model\n",
    "best_model_name = max(ensemble_results.keys(), key=lambda m: ensemble_results[m]['r2'])\n",
    "print(f\"\\nBEST PERFORMING MODEL: {best_model_name}\")\n",
    "print(f\"Best R¬≤ Score: {ensemble_results[best_model_name]['r2']:.6f}\")\n",
    "\n",
    "# Feature importance analysis for ensemble models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get feature importances where available\n",
    "feature_names = X_train.columns if hasattr(X_train, 'columns') else [f'feature_{i}' for i in range(X_train.shape[1])]\n",
    "\n",
    "for model_name, model in ensemble_models.items():\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        print(f\"\\n{model_name.upper()} Feature Importances:\")\n",
    "        importances = model.feature_importances_\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(feature_importance_df.head(10).to_string(index=False))\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        print(f\"\\n{model_name.upper()} Coefficients:\")\n",
    "        coefficients = np.abs(model.coef_[0]) if len(model.coef_.shape) > 1 else np.abs(model.coef_)\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'coefficient': coefficients\n",
    "        }).sort_values('coefficient', ascending=False)\n",
    "        \n",
    "        print(feature_importance_df.head(10).to_string(index=False))\n",
    "\n",
    "# Cross-validation analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CROSS-VALIDATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Define custom scorer\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "cv_results = {}\n",
    "for model_name, model in ensemble_models.items():\n",
    "    print(f\"\\nPerforming 5-fold CV for {model_name}...\")\n",
    "    \n",
    "    # MAE cross-validation\n",
    "    mae_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=mae_scorer)\n",
    "    mae_scores = -mae_scores  # Convert back to positive\n",
    "    \n",
    "    # R¬≤ cross-validation\n",
    "    r2_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "    \n",
    "    cv_results[model_name] = {\n",
    "        'mae_mean': mae_scores.mean(),\n",
    "        'mae_std': mae_scores.std(),\n",
    "        'r2_mean': r2_scores.mean(),\n",
    "        'r2_std': r2_scores.std()\n",
    "    }\n",
    "    \n",
    "    print(f\"  MAE: {mae_scores.mean():.4f} ¬± {mae_scores.std():.4f}\")\n",
    "    print(f\"  R¬≤:  {r2_scores.mean():.4f} ¬± {r2_scores.std():.4f}\")\n",
    "\n",
    "# Cross-validation visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# MAE cross-validation results\n",
    "models = list(cv_results.keys())\n",
    "mae_means = [cv_results[m]['mae_mean'] for m in models]\n",
    "mae_stds = [cv_results[m]['mae_std'] for m in models]\n",
    "\n",
    "ax1.bar(models, mae_means, yerr=mae_stds, capsize=5, color='skyblue', alpha=0.7)\n",
    "ax1.set_ylabel('MAE')\n",
    "ax1.set_title('Cross-Validation MAE Results')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# R¬≤ cross-validation results\n",
    "r2_means = [cv_results[m]['r2_mean'] for m in models]\n",
    "r2_stds = [cv_results[m]['r2_std'] for m in models]\n",
    "\n",
    "ax2.bar(models, r2_means, yerr=r2_stds, capsize=5, color='lightcoral', alpha=0.7)\n",
    "ax2.set_ylabel('R¬≤ Score')\n",
    "ax2.set_title('Cross-Validation R¬≤ Results')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Model serialization for deployment\n",
    "import joblib\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Create model metadata\n",
    "model_metadata = {\n",
    "    'model_name': production_model_name,\n",
    "    'model_type': 'ensemble_battery_health',\n",
    "    'version': '1.0.0',\n",
    "    'created_date': datetime.now().isoformat(),\n",
    "    'performance_metrics': {\n",
    "        'test_mae': float(comparison_df.iloc[0]['Test_MAE']),\n",
    "        'test_r2': float(comparison_df.iloc[0]['Test_R2']),\n",
    "        'cv_mae_mean': float(comparison_df.iloc[0]['CV_MAE_Mean']),\n",
    "        'cv_mae_std': float(comparison_df.iloc[0]['CV_MAE_Std']),\n",
    "        'cv_r2_mean': float(comparison_df.iloc[0]['CV_R2_Mean']),\n",
    "        'cv_r2_std': float(comparison_df.iloc[0]['CV_R2_Std'])\n",
    "    },\n",
    "    'training_data_shape': X_train.shape,\n",
    "    'feature_names': feature_names,\n",
    "    'model_parameters': production_model.get_params() if hasattr(production_model, 'get_params') else 'N/A'\n",
    "}\n",
    "\n",
    "# Save production model\n",
    "model_output_dir = Path('../../model-artifacts/trained_models/ensemble_v1.0')\n",
    "model_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(production_model, model_output_dir / 'ensemble_model.pkl')\n",
    "print(f\"‚úÖ Production model saved to: {model_output_dir / 'ensemble_model.pkl'}\")\n",
    "\n",
    "# Save metadata\n",
    "with open(model_output_dir / 'model_metadata.json', 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "print(f\"‚úÖ Model metadata saved to: {model_output_dir / 'model_metadata.json'}\")\n",
    "\n",
    "# Save training history\n",
    "training_history = {\n",
    "    'all_models_results': ensemble_results,\n",
    "    'cross_validation_results': cv_results,\n",
    "    'model_comparison': comparison_df.to_dict('records')\n",
    "}\n",
    "\n",
    "with open(model_output_dir / 'training_history.json', 'w') as f:\n",
    "    json.dump(training_history, f, indent=2, default=str)\n",
    "print(f\"‚úÖ Training history saved to: {model_output_dir / 'training_history.json'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RECOMMENDATIONS AND NEXT STEPS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "üéØ PRODUCTION MODEL SELECTION:\n",
    "   Selected Model: {production_model_name}\n",
    "   Performance: R¬≤ = {comparison_df.iloc[0]['Test_R2']:.4f}, MAE = {comparison_df.iloc[0]['Test_MAE']:.4f}\n",
    "   \n",
    "üöÄ NEXT STEPS FOR DEPLOYMENT:\n",
    "   1. Model Validation: Test on additional holdout datasets\n",
    "   2. A/B Testing: Compare against current production model\n",
    "   3. Integration: Integrate with BatteryMind inference pipeline\n",
    "   4. Monitoring: Set up model performance monitoring\n",
    "   5. Retraining: Establish automated retraining pipeline\n",
    "   \n",
    "üìä MODEL IMPROVEMENTS:\n",
    "   1. Feature Engineering: Add more temporal features\n",
    "   2. Hyperparameter Tuning: Use Optuna for optimization\n",
    "   3. Ensemble Diversity: Include neural network models\n",
    "   4. Cross-Chemistry: Train separate models for different chemistries\n",
    "   \n",
    "‚ö†Ô∏è  CONSIDERATIONS:\n",
    "   1. Model interpretability for regulatory compliance\n",
    "   2. Computational efficiency for real-time inference\n",
    "   3. Data drift monitoring and adaptation\n",
    "   4. Safety constraints integration\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüéâ ENSEMBLE MODEL DEVELOPMENT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
