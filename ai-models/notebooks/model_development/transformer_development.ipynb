{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595214f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BatteryMind - Transformer Model Development Notebook\n",
    "\n",
    "Interactive development environment for building and training transformer models\n",
    "for battery health prediction and degradation forecasting. This notebook provides\n",
    "comprehensive model development workflow from data preparation to model evaluation.\n",
    "\n",
    "Features:\n",
    "- Multi-head attention transformer architecture for battery health prediction\n",
    "- Time series preprocessing and feature engineering\n",
    "- Advanced training techniques with early stopping and learning rate scheduling\n",
    "- Model interpretability with attention visualization\n",
    "- Comprehensive evaluation metrics and performance analysis\n",
    "- Integration with BatteryMind ecosystem components\n",
    "\n",
    "Author: BatteryMind Development Team\n",
    "Version: 1.0.0\n",
    "\"\"\"\n",
    "\n",
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"BatteryMind Transformer Development Environment\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Import BatteryMind modules\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from transformers.battery_health_predictor.model import BatteryHealthTransformer\n",
    "from transformers.battery_health_predictor.trainer import BatteryHealthTrainer\n",
    "from transformers.battery_health_predictor.data_loader import BatteryDataLoader\n",
    "from transformers.battery_health_predictor.preprocessing import BatteryDataPreprocessor\n",
    "from transformers.common.attention_layers import MultiHeadAttention\n",
    "from transformers.common.positional_encoding import PositionalEncoding\n",
    "from training_data.generators.synthetic_generator import SyntheticDataGenerator\n",
    "from training_data.preprocessing_scripts.feature_extractor import BatteryFeatureExtractor\n",
    "from training_data.preprocessing_scripts.normalization import BatteryDataNormalizer\n",
    "\n",
    "print(\"✓ BatteryMind modules imported successfully\")\n",
    "\n",
    "# Configuration\n",
    "class TransformerConfig:\n",
    "    \"\"\"Configuration for transformer model development.\"\"\"\n",
    "    \n",
    "    # Data parameters\n",
    "    SEQUENCE_LENGTH = 144  # 24 hours at 10-minute intervals\n",
    "    FEATURE_DIM = 12  # Number of input features\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_SAMPLES = 10000\n",
    "    \n",
    "    # Model parameters\n",
    "    D_MODEL = 256\n",
    "    NUM_HEADS = 8\n",
    "    NUM_LAYERS = 6\n",
    "    D_FF = 1024\n",
    "    DROPOUT = 0.1\n",
    "    \n",
    "    # Training parameters\n",
    "    LEARNING_RATE = 1e-4\n",
    "    NUM_EPOCHS = 100\n",
    "    PATIENCE = 10\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    \n",
    "    # Output parameters\n",
    "    OUTPUT_DIM = 3  # SoH, RUL, Next_SoC\n",
    "    \n",
    "    # Device configuration\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = TransformerConfig()\n",
    "print(f\"✓ Configuration loaded - Device: {config.DEVICE}\")\n",
    "\n",
    "# Data Generation and Preprocessing\n",
    "print(\"\\n1. SYNTHETIC DATA GENERATION\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Generate synthetic battery data\n",
    "data_generator = SyntheticDataGenerator()\n",
    "print(\"Generating synthetic battery telemetry data...\")\n",
    "\n",
    "# Generate diverse battery scenarios\n",
    "battery_data = []\n",
    "for i in range(config.NUM_SAMPLES):\n",
    "    # Generate battery parameters\n",
    "    battery_params = {\n",
    "        'capacity_ah': np.random.uniform(50, 100),\n",
    "        'chemistry': np.random.choice(['lithium_ion', 'lifepo4']),\n",
    "        'temperature_range': (-20, 60),\n",
    "        'usage_pattern': np.random.choice(['urban', 'highway', 'mixed'])\n",
    "    }\n",
    "    \n",
    "    # Generate time series data\n",
    "    time_series = data_generator.generate_battery_timeseries(\n",
    "        duration_hours=24,\n",
    "        sampling_rate=10,  # 10-minute intervals\n",
    "        battery_params=battery_params\n",
    "    )\n",
    "    \n",
    "    battery_data.append(time_series)\n",
    "    \n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(f\"Generated {i + 1}/{config.NUM_SAMPLES} battery scenarios\")\n",
    "\n",
    "print(\"✓ Synthetic data generation completed\")\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "df_list = []\n",
    "for i, data in enumerate(battery_data):\n",
    "    df = pd.DataFrame(data)\n",
    "    df['battery_id'] = i\n",
    "    df_list.append(df)\n",
    "\n",
    "battery_df = pd.concat(df_list, ignore_index=True)\n",
    "print(f\"✓ Combined dataset shape: {battery_df.shape}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample Battery Data:\")\n",
    "print(battery_df.head())\n",
    "\n",
    "# Feature Engineering\n",
    "print(\"\\n2. FEATURE ENGINEERING\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "feature_extractor = BatteryFeatureExtractor()\n",
    "\n",
    "# Extract features from raw sensor data\n",
    "features = feature_extractor.extract_features(battery_df)\n",
    "print(f\"✓ Extracted {len(features.columns)} features\")\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importance = feature_extractor.analyze_feature_importance(features)\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "for i, (feature, importance) in enumerate(feature_importance.head(10).items()):\n",
    "    print(f\"{i+1:2d}. {feature:<25} {importance:.4f}\")\n",
    "\n",
    "# Visualize feature distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "key_features = ['voltage', 'current', 'temperature', 'soc', 'soh', 'internal_resistance']\n",
    "for i, feature in enumerate(key_features):\n",
    "    if feature in features.columns:\n",
    "        axes[i].hist(features[feature], bins=50, alpha=0.7)\n",
    "        axes[i].set_title(f'{feature.title()} Distribution')\n",
    "        axes[i].set_xlabel(feature)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Data Preprocessing\n",
    "print(\"\\n3. DATA PREPROCESSING\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "preprocessor = BatteryDataPreprocessor()\n",
    "normalizer = BatteryDataNormalizer()\n",
    "\n",
    "# Normalize features\n",
    "normalized_features = normalizer.normalize(features)\n",
    "print(\"✓ Feature normalization completed\")\n",
    "\n",
    "# Create sequences for transformer input\n",
    "def create_sequences(data, sequence_length, target_columns):\n",
    "    \"\"\"Create sequences for transformer training.\"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for battery_id in data['battery_id'].unique():\n",
    "        battery_data = data[data['battery_id'] == battery_id].sort_values('timestamp')\n",
    "        \n",
    "        for i in range(len(battery_data) - sequence_length):\n",
    "            # Input sequence\n",
    "            seq = battery_data.iloc[i:i+sequence_length][\n",
    "                ['voltage', 'current', 'temperature', 'soc', 'power', \n",
    "                 'energy', 'efficiency', 'thermal_gradient', 'resistance_change',\n",
    "                 'capacity_fade', 'cycle_count', 'age_days']\n",
    "            ].values\n",
    "            \n",
    "            # Target (next values)\n",
    "            target = battery_data.iloc[i+sequence_length][target_columns].values\n",
    "            \n",
    "            sequences.append(seq)\n",
    "            targets.append(target)\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "target_columns = ['soh', 'remaining_useful_life', 'soc']\n",
    "X, y = create_sequences(normalized_features, config.SEQUENCE_LENGTH, target_columns)\n",
    "\n",
    "print(f\"✓ Created {len(X)} sequences\")\n",
    "print(f\"  Input shape: {X.shape}\")\n",
    "print(f\"  Target shape: {y.shape}\")\n",
    "\n",
    "# Train-validation-test split\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Data split completed:\")\n",
    "print(f\"  Training: {X_train.shape[0]} samples\")\n",
    "print(f\"  Validation: {X_val.shape[0]} samples\")\n",
    "print(f\"  Test: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(config.DEVICE)\n",
    "y_train_tensor = torch.FloatTensor(y_train).to(config.DEVICE)\n",
    "X_val_tensor = torch.FloatTensor(X_val).to(config.DEVICE)\n",
    "y_val_tensor = torch.FloatTensor(y_val).to(config.DEVICE)\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(config.DEVICE)\n",
    "y_test_tensor = torch.FloatTensor(y_test).to(config.DEVICE)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"✓ Data loaders created\")\n",
    "\n",
    "# Model Architecture\n",
    "print(\"\\n4. TRANSFORMER MODEL ARCHITECTURE\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "class BatteryTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer model for battery health prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BatteryTransformer, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = nn.Linear(config.FEATURE_DIM, config.D_MODEL)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(config.D_MODEL, config.DROPOUT)\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.D_MODEL,\n",
    "            nhead=config.NUM_HEADS,\n",
    "            dim_feedforward=config.D_FF,\n",
    "            dropout=config.DROPOUT,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=config.NUM_LAYERS\n",
    "        )\n",
    "        \n",
    "        # Output layers\n",
    "        self.layer_norm = nn.LayerNorm(config.D_MODEL)\n",
    "        self.dropout = nn.Dropout(config.DROPOUT)\n",
    "        \n",
    "        # Multi-task output heads\n",
    "        self.soh_head = nn.Sequential(\n",
    "            nn.Linear(config.D_MODEL, config.D_MODEL // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.DROPOUT),\n",
    "            nn.Linear(config.D_MODEL // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.rul_head = nn.Sequential(\n",
    "            nn.Linear(config.D_MODEL, config.D_MODEL // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.DROPOUT),\n",
    "            nn.Linear(config.D_MODEL // 2, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.soc_head = nn.Sequential(\n",
    "            nn.Linear(config.D_MODEL, config.D_MODEL // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.DROPOUT),\n",
    "            nn.Linear(config.D_MODEL // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize model weights.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, feature_dim)\n",
    "            mask: Optional attention mask\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with predictions for each task\n",
    "        \"\"\"\n",
    "        # Input projection\n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Transformer encoding\n",
    "        encoded = self.transformer_encoder(x, src_key_padding_mask=mask)\n",
    "        \n",
    "        # Global average pooling\n",
    "        pooled = encoded.mean(dim=1)\n",
    "        \n",
    "        # Layer normalization and dropout\n",
    "        pooled = self.layer_norm(pooled)\n",
    "        pooled = self.dropout(pooled)\n",
    "        \n",
    "        # Multi-task predictions\n",
    "        soh_pred = self.soh_head(pooled)\n",
    "        rul_pred = self.rul_head(pooled)\n",
    "        soc_pred = self.soc_head(pooled)\n",
    "        \n",
    "        return {\n",
    "            'soh': soh_pred,\n",
    "            'rul': rul_pred,\n",
    "            'soc': soc_pred\n",
    "        }\n",
    "\n",
    "# Initialize model\n",
    "model = BatteryTransformer(config).to(config.DEVICE)\n",
    "print(f\"✓ Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Model summary\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"✓ Trainable parameters: {count_parameters(model):,}\")\n",
    "\n",
    "# Training Setup\n",
    "print(\"\\n5. TRAINING SETUP\")\n",
    "print(\"-\" * 17)\n",
    "\n",
    "# Loss functions\n",
    "class MultiTaskLoss(nn.Module):\n",
    "    \"\"\"Multi-task loss for battery prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, weights=None):\n",
    "        super(MultiTaskLoss, self).__init__()\n",
    "        self.weights = weights or {'soh': 1.0, 'rul': 1.0, 'soc': 1.0}\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.mae_loss = nn.L1Loss()\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Compute multi-task loss.\n",
    "        \n",
    "        Args:\n",
    "            predictions: Dictionary of predictions\n",
    "            targets: Target tensor (batch_size, 3) for [soh, rul, soc]\n",
    "        \"\"\"\n",
    "        soh_loss = self.mse_loss(predictions['soh'].squeeze(), targets[:, 0])\n",
    "        rul_loss = self.mae_loss(predictions['rul'].squeeze(), targets[:, 1])\n",
    "        soc_loss = self.mse_loss(predictions['soc'].squeeze(), targets[:, 2])\n",
    "        \n",
    "        total_loss = (\n",
    "            self.weights['soh'] * soh_loss +\n",
    "            self.weights['rul'] * rul_loss +\n",
    "            self.weights['soc'] * soc_loss\n",
    "        )\n",
    "        \n",
    "        return total_loss, {\n",
    "            'soh_loss': soh_loss.item(),\n",
    "            'rul_loss': rul_loss.item(),\n",
    "            'soc_loss': soc_loss.item(),\n",
    "            'total_loss': total_loss.item()\n",
    "        }\n",
    "\n",
    "# Initialize loss and optimizer\n",
    "criterion = MultiTaskLoss()\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.LEARNING_RATE,\n",
    "    weight_decay=config.WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "print(\"✓ Loss function and optimizer initialized\")\n",
    "\n",
    "# Training Loop\n",
    "print(\"\\n6. MODEL TRAINING\")\n",
    "print(\"-\" * 16)\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loss_components = {'soh_loss': 0, 'rul_loss': 0, 'soc_loss': 0}\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(data)\n",
    "        loss, components = criterion(predictions, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        for key, value in components.items():\n",
    "            if key in loss_components:\n",
    "                loss_components[key] += value\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    for key in loss_components:\n",
    "        loss_components[key] /= len(train_loader)\n",
    "    \n",
    "    return avg_loss, loss_components\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate for one epoch.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    loss_components = {'soh_loss': 0, 'rul_loss': 0, 'soc_loss': 0}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            predictions = model(data)\n",
    "            loss, components = criterion(predictions, target)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            for key, value in components.items():\n",
    "                if key in loss_components:\n",
    "                    loss_components[key] += value\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    for key in loss_components:\n",
    "        loss_components[key] /= len(val_loader)\n",
    "    \n",
    "    return avg_loss, loss_components\n",
    "\n",
    "# Training history\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(config.NUM_EPOCHS):\n",
    "    # Training\n",
    "    train_loss, train_components = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, config.DEVICE\n",
    "    )\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_components = validate_epoch(\n",
    "        model, val_loader, criterion, config.DEVICE\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Record losses\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), 'best_transformer_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{config.NUM_EPOCHS}\")\n",
    "        print(f\"  Train Loss: {train_loss:.6f}\")\n",
    "        print(f\"  Val Loss:   {val_loss:.6f}\")\n",
    "        print(f\"  Components: SoH={val_components['soh_loss']:.6f}, \"\n",
    "              f\"RUL={val_components['rul_loss']:.6f}, \"\n",
    "              f\"SoC={val_components['soc_loss']:.6f}\")\n",
    "        print(f\"  LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= config.PATIENCE:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(\"✓ Training completed\")\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_transformer_model.pth'))\n",
    "print(\"✓ Best model loaded\")\n",
    "\n",
    "# Visualize training progress\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss', alpha=0.7)\n",
    "plt.plot(val_losses, label='Validation Loss', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses[-50:], label='Training Loss (Last 50)', alpha=0.7)\n",
    "plt.plot(val_losses[-50:], label='Validation Loss (Last 50)', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Progress (Recent)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Model Evaluation\n",
    "print(\"\\n7. MODEL EVALUATION\")\n",
    "print(\"-\" * 18)\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Comprehensive model evaluation.\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = {'soh': [], 'rul': [], 'soc': []}\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            predictions = model(data)\n",
    "            \n",
    "            all_predictions['soh'].extend(predictions['soh'].cpu().numpy())\n",
    "            all_predictions['rul'].extend(predictions['rul'].cpu().numpy())\n",
    "            all_predictions['soc'].extend(predictions['soc'].cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    for key in all_predictions:\n",
    "        all_predictions[key] = np.array(all_predictions[key]).flatten()\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    return all_predictions, all_targets\n",
    "\n",
    "# Evaluate on test set\n",
    "predictions, targets = evaluate_model(model, test_loader, config.DEVICE)\n",
    "\n",
    "# Calculate metrics\n",
    "def calculate_metrics(y_true, y_pred, task_name):\n",
    "    \"\"\"Calculate evaluation metrics.\"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    return {\n",
    "        f'{task_name}_mse': mse,\n",
    "        f'{task_name}_mae': mae,\n",
    "        f'{task_name}_r2': r2,\n",
    "        f'{task_name}_rmse': np.sqrt(mse)\n",
    "    }\n",
    "\n",
    "# Calculate metrics for each task\n",
    "metrics = {}\n",
    "tasks = ['soh', 'rul', 'soc']\n",
    "for i, task in enumerate(tasks):\n",
    "    task_metrics = calculate_metrics(targets[:, i], predictions[task], task)\n",
    "    metrics.update(task_metrics)\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Test Set Evaluation Results:\")\n",
    "print(\"=\" * 40)\n",
    "for task in tasks:\n",
    "    print(f\"\\n{task.upper()} Prediction:\")\n",
    "    print(f\"  RMSE: {metrics[f'{task}_rmse']:.6f}\")\n",
    "    print(f\"  MAE:  {metrics[f'{task}_mae']:.6f}\")\n",
    "    print(f\"  R²:   {metrics[f'{task}_r2']:.6f}\")\n",
    "\n",
    "# Visualize predictions vs targets\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, task in enumerate(tasks):\n",
    "    axes[i].scatter(targets[:, i], predictions[task], alpha=0.5, s=1)\n",
    "    axes[i].plot([targets[:, i].min(), targets[:, i].max()], \n",
    "                 [targets[:, i].min(), targets[:, i].max()], 'r--', lw=2)\n",
    "    axes[i].set_xlabel(f'True {task.upper()}')\n",
    "    axes[i].set_ylabel(f'Predicted {task.upper()}')\n",
    "    axes[i].set_title(f'{task.upper()} Predictions (R² = {metrics[f\"{task}_r2\"]:.3f})')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Attention Visualization\n",
    "print(\"\\n8. ATTENTION VISUALIZATION\")\n",
    "print(\"-\" * 27)\n",
    "\n",
    "def visualize_attention(model, sample_input, layer_idx=0, head_idx=0):\n",
    "    \"\"\"Visualize attention weights.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get attention weights from transformer encoder\n",
    "        sample_input = sample_input.unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Forward pass through input projection and positional encoding\n",
    "        x = model.input_projection(sample_input)\n",
    "        x = model.positional_encoding(x)\n",
    "        \n",
    "        # Get attention weights from specific layer\n",
    "        encoder_layer = model.transformer_encoder.layers[layer_idx]\n",
    "        attn_weights = encoder_layer.self_attn(x, x, x, need_weights=True)[1]\n",
    "        \n",
    "        # Extract specific head\n",
    "        attention = attn_weights[0, head_idx].cpu().numpy()\n",
    "        \n",
    "        return attention\n",
    "\n",
    "# Visualize attention for a sample\n",
    "sample_idx = 0\n",
    "sample_input = X_test_tensor[sample_idx]\n",
    "attention_weights = visualize_attention(model, sample_input)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(attention_weights, cmap='Blues', cbar=True)\n",
    "plt.title('Attention Weights Visualization (Layer 0, Head 0)')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance Analysis\n",
    "print(\"\\n9. FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "def analyze_feature_importance(model, test_loader, device):\n",
    "    \"\"\"Analyze feature importance using gradient-based methods.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    feature_gradients = []\n",
    "    \n",
    "    for data, target in test_loader:\n",
    "        data = data.to(device)\n",
    "        data.requires_grad_(True)\n",
    "        \n",
    "        predictions = model(data)\n",
    "        \n",
    "        # Calculate gradients for SoH prediction\n",
    "        soh_pred = predictions['soh']\n",
    "        grad_outputs = torch.ones_like(soh_pred)\n",
    "        \n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=soh_pred,\n",
    "            inputs=data,\n",
    "            grad_outputs=grad_outputs,\n",
    "            create_graph=False,\n",
    "            retain_graph=False\n",
    "        )[0]\n",
    "        \n",
    "        # Average gradients across sequence length\n",
    "        feature_grads = gradients.abs().mean(dim=1).mean(dim=0)\n",
    "        feature_gradients.append(feature_grads.cpu().numpy())\n",
    "    \n",
    "    # Average across all samples\n",
    "    avg_gradients = np.mean(feature_gradients, axis=0)\n",
    "    \n",
    "    return avg_gradients\n",
    "\n",
    "# Calculate feature importance\n",
    "feature_importance = analyze_feature_importance(model, test_loader, config.DEVICE)\n",
    "\n",
    "# Visualize feature importance\n",
    "feature_names = ['voltage', 'current', 'temperature', 'soc', 'power', \n",
    "                'energy', 'efficiency', 'thermal_gradient', 'resistance_change',\n",
    "                'capacity_fade', 'cycle_count', 'age_days']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "plt.xlabel('Feature Importance (Gradient Magnitude)')\n",
    "plt.title('Feature Importance Analysis')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Model Interpretability\n",
    "print(\"\\n10. MODEL INTERPRETABILITY\")\n",
    "print(\"-\" * 26)\n",
    "\n",
    "def explain_prediction(model, sample_input, feature_names):\n",
    "    \"\"\"Explain a single prediction.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    sample_input = sample_input.unsqueeze(0)\n",
    "    sample_input.requires_grad_(True)\n",
    "    \n",
    "    predictions = model(sample_input)\n",
    "    \n",
    "    # Get gradients for each output\n",
    "    explanations = {}\n",
    "    \n",
    "    for task in ['soh', 'rul', 'soc']:\n",
    "        output = predictions[task]\n",
    "        \n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=output,\n",
    "            inputs=sample_input,\n",
    "            grad_outputs=torch.ones_like(output),\n",
    "            create_graph=False,\n",
    "            retain_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        # Average across sequence length\n",
    "        feature_importance = gradients.abs().mean(dim=1).squeeze().cpu().numpy()\n",
    "        explanations[task] = feature_importance\n",
    "    \n",
    "    return explanations, predictions\n",
    "\n",
    "# Explain a sample prediction\n",
    "sample_explanations, sample_predictions = explain_prediction(\n",
    "    model, X_test_tensor[0], feature_names\n",
    ")\n",
    "\n",
    "print(\"Sample Prediction Explanation:\")\n",
    "print(f\"Predicted SoH: {sample_predictions['soh'].item():.4f}\")\n",
    "print(f\"Predicted RUL: {sample_predictions['rul'].item():.4f}\")\n",
    "print(f\"Predicted SoC: {sample_predictions['soc'].item():.4f}\")\n",
    "\n",
    "# Visualize explanation\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, task in enumerate(['soh', 'rul', 'soc']):\n",
    "    axes[i].bar(feature_names, sample_explanations[task])\n",
    "    axes[i].set_title(f'{task.upper()} Prediction Explanation')\n",
    "    axes[i].set_ylabel('Feature Importance')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Model Deployment Preparation\n",
    "print(\"\\n11. MODEL DEPLOYMENT PREPARATION\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Save final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': config,\n",
    "    'feature_names': feature_names,\n",
    "    'metrics': metrics,\n",
    "    'model_architecture': str(model)\n",
    "}, 'battery_transformer_final.pth')\n",
    "\n",
    "print(\"✓ Model saved for deployment\")\n",
    "\n",
    "# Create ONNX export for production\n",
    "dummy_input = torch.randn(1, config.SEQUENCE_LENGTH, config.FEATURE_DIM).to(config.DEVICE)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    'battery_transformer.onnx',\n",
    "    export_params=True,\n",
    "    opset_version=11,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['soh', 'rul', 'soc'],\n",
    "    dynamic_axes={\n",
    "        'input': {0: 'batch_size'},\n",
    "        'soh': {0: 'batch_size'},\n",
    "        'rul': {0: 'batch_size'},\n",
    "        'soc': {0: 'batch_size'}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"✓ ONNX model exported for production deployment\")\n",
    "\n",
    "# Summary Report\n",
    "print(\"\\n12. DEVELOPMENT SUMMARY\")\n",
    "print(\"-\" * 23)\n",
    "\n",
    "print(\"Transformer Model Development Completed Successfully!\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"Model Architecture: {config.NUM_LAYERS}-layer Transformer\")\n",
    "print(f\"Model Parameters: {count_parameters(model):,}\")\n",
    "print(f\"Training Samples: {len(X_train):,}\")\n",
    "print(f\"Validation Samples: {len(X_val):,}\")\n",
    "print(f\"Test Samples: {len(X_test):,}\")\n",
    "print(f\"Best Validation Loss: {best_val_loss:.6f}\")\n",
    "\n",
    "print(\"\\nFinal Test Performance:\")\n",
    "for task in tasks:\n",
    "    print(f\"  {task.upper()}: RMSE={metrics[f'{task}_rmse']:.4f}, \"\n",
    "          f\"MAE={metrics[f'{task}_mae']:.4f}, R²={metrics[f'{task}_r2']:.4f}\")\n",
    "\n",
    "print(\"\\nModel Artifacts Generated:\")\n",
    "print(\"  ✓ best_transformer_model.pth - Best model weights\")\n",
    "print(\"  ✓ battery_transformer_final.pth - Final model with metadata\")\n",
    "print(\"  ✓ battery_transformer.onnx - ONNX model for deployment\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Deploy model to production environment\")\n",
    "print(\"  2. Set up real-time inference pipeline\")\n",
    "print(\"  3. Implement model monitoring and drift detection\")\n",
    "print(\"  4. Plan for continuous model updates\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*55)\n",
    "print(\"BatteryMind Transformer Development Complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
