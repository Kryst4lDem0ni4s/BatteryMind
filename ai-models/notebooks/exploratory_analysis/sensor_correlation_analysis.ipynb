{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad4eec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BatteryMind - Sensor Correlation Analysis\n",
    "\n",
    "Advanced correlation analysis of battery sensor data to identify relationships,\n",
    "dependencies, and patterns across multiple sensor modalities for improved\n",
    "battery state estimation and predictive modeling.\n",
    "\n",
    "This notebook provides:\n",
    "- Multi-modal sensor correlation analysis\n",
    "- Feature importance and selection\n",
    "- Temporal correlation patterns\n",
    "- Cross-sensor dependency modeling\n",
    "- Sensor fusion optimization\n",
    "- Anomaly detection through correlation analysis\n",
    "\n",
    "Author: BatteryMind Development Team\n",
    "Version: 1.0.0\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scientific computing and ML libraries\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.feature_selection import mutual_info_regression, SelectKBest, f_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "from scipy import stats\n",
    "from scipy.signal import correlate, find_peaks, savgol_filter\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "import networkx as nx\n",
    "\n",
    "# Time series analysis\n",
    "from statsmodels.tsa.stattools import ccf, acf, pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Custom imports\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from training_data.generators.synthetic_generator import BatteryMultiModalDataGenerator\n",
    "from training_data.generators.physics_simulator import BatteryPhysicsSimulator\n",
    "from utils.visualization import SensorVisualization\n",
    "from utils.data_utils import SensorDataProcessor\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"BatteryMind Sensor Correlation Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. DATA GENERATION AND LOADING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n1. Generating Multi-Modal Sensor Data...\")\n",
    "\n",
    "# Initialize multi-modal data generator\n",
    "sensor_generator = BatteryMultiModalDataGenerator(\n",
    "    num_batteries=50,\n",
    "    simulation_hours=720,  # 30 days\n",
    "    sampling_rate=60,      # 1 minute intervals\n",
    "    sensor_types=['electrical', 'thermal', 'acoustic', 'chemical', 'mechanical'],\n",
    "    noise_levels={'low': 0.01, 'medium': 0.05, 'high': 0.1},\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# Generate comprehensive sensor dataset\n",
    "sensor_data = sensor_generator.generate_multimodal_data()\n",
    "sensor_metadata = sensor_generator.get_sensor_metadata()\n",
    "\n",
    "print(f\"Generated data for {len(sensor_data['battery_id'].unique())} batteries\")\n",
    "print(f\"Sensor types: {list(sensor_metadata.keys())}\")\n",
    "print(f\"Total data points: {len(sensor_data):,}\")\n",
    "\n",
    "# Data overview\n",
    "print(\"\\nSensor Data Overview:\")\n",
    "print(sensor_data.info())\n",
    "print(\"\\nSensor columns:\")\n",
    "sensor_columns = [col for col in sensor_data.columns if col not in ['battery_id', 'timestamp']]\n",
    "print(sensor_columns)\n",
    "\n",
    "# =============================================================================\n",
    "# 2. BASIC CORRELATION ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n2. Basic Correlation Analysis...\")\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = sensor_data[sensor_columns].corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(16, 14))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "            cmap='RdBu_r', center=0, square=True, linewidths=0.5,\n",
    "            cbar_kws={\"shrink\": .8})\n",
    "plt.title('Sensor Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify strong correlations\n",
    "strong_correlations = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.7:  # Strong correlation threshold\n",
    "            strong_correlations.append({\n",
    "                'sensor1': correlation_matrix.columns[i],\n",
    "                'sensor2': correlation_matrix.columns[j],\n",
    "                'correlation': corr_val\n",
    "            })\n",
    "\n",
    "strong_corr_df = pd.DataFrame(strong_correlations).sort_values('correlation', key=abs, ascending=False)\n",
    "print(\"\\nStrongest Correlations (|r| > 0.7):\")\n",
    "print(strong_corr_df.head(10))\n",
    "\n",
    "# Correlation distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "correlation_values = correlation_matrix.values[np.triu_indices_from(correlation_matrix.values, k=1)]\n",
    "plt.hist(correlation_values, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Correlation Coefficients')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "abs_correlation_values = np.abs(correlation_values)\n",
    "plt.hist(abs_correlation_values, bins=50, alpha=0.7, edgecolor='black', color='orange')\n",
    "plt.xlabel('Absolute Correlation Coefficient')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Absolute Correlation Coefficients')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 3. SENSOR GROUP ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n3. Sensor Group Analysis...\")\n",
    "\n",
    "# Define sensor groups based on metadata\n",
    "sensor_groups = {\n",
    "    'electrical': [col for col in sensor_columns if any(x in col.lower() for x in ['voltage', 'current', 'resistance', 'power', 'soc', 'soh'])],\n",
    "    'thermal': [col for col in sensor_columns if any(x in col.lower() for x in ['temperature', 'thermal', 'heat'])],\n",
    "    'acoustic': [col for col in sensor_columns if any(x in col.lower() for x in ['acoustic', 'sound', 'vibration', 'frequency'])],\n",
    "    'chemical': [col for col in sensor_columns if any(x in col.lower() for x in ['chemical', 'ph', 'concentration', 'gas'])],\n",
    "    'mechanical': [col for col in sensor_columns if any(x in col.lower() for x in ['pressure', 'strain', 'displacement', 'force'])]\n",
    "}\n",
    "\n",
    "# Remove empty groups and overlaps\n",
    "sensor_groups = {k: v for k, v in sensor_groups.items() if v}\n",
    "print(f\"\\nSensor Groups Identified:\")\n",
    "for group, sensors in sensor_groups.items():\n",
    "    print(f\"  {group}: {len(sensors)} sensors\")\n",
    "\n",
    "# Intra-group correlations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (group_name, group_sensors) in enumerate(sensor_groups.items()):\n",
    "    if idx >= len(axes):\n",
    "        break\n",
    "    \n",
    "    if len(group_sensors) > 1:\n",
    "        group_corr = sensor_data[group_sensors].corr()\n",
    "        sns.heatmap(group_corr, annot=True, fmt='.2f', cmap='RdBu_r', \n",
    "                   center=0, ax=axes[idx], cbar=True)\n",
    "        axes[idx].set_title(f'{group_name.title()} Sensor Correlations')\n",
    "    else:\n",
    "        axes[idx].text(0.5, 0.5, f'{group_name.title()}\\n(Single Sensor)', \n",
    "                      ha='center', va='center', transform=axes[idx].transAxes)\n",
    "        axes[idx].set_xticks([])\n",
    "        axes[idx].set_yticks([])\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(sensor_groups), len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Inter-group correlation analysis\n",
    "inter_group_correlations = {}\n",
    "for group1_name, group1_sensors in sensor_groups.items():\n",
    "    for group2_name, group2_sensors in sensor_groups.items():\n",
    "        if group1_name != group2_name:\n",
    "            group1_data = sensor_data[group1_sensors].mean(axis=1)\n",
    "            group2_data = sensor_data[group2_sensors].mean(axis=1)\n",
    "            corr, p_value = pearsonr(group1_data, group2_data)\n",
    "            inter_group_correlations[f\"{group1_name}-{group2_name}\"] = {\n",
    "                'correlation': corr,\n",
    "                'p_value': p_value\n",
    "            }\n",
    "\n",
    "inter_group_df = pd.DataFrame(inter_group_correlations).T\n",
    "print(\"\\nInter-Group Correlations:\")\n",
    "print(inter_group_df.sort_values('correlation', key=abs, ascending=False))\n",
    "\n",
    "# =============================================================================\n",
    "# 4. TEMPORAL CORRELATION ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n4. Temporal Correlation Analysis...\")\n",
    "\n",
    "# Select a representative battery for temporal analysis\n",
    "sample_battery = sensor_data[sensor_data['battery_id'] == sensor_data['battery_id'].iloc[0]].copy()\n",
    "sample_battery = sample_battery.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# Calculate lagged correlations for key sensor pairs\n",
    "key_pairs = [\n",
    "    ('voltage', 'current'),\n",
    "    ('temperature_core', 'voltage'),\n",
    "    ('soc', 'voltage'),\n",
    "    ('internal_resistance', 'temperature_core'),\n",
    "    ('acoustic_amplitude', 'current')\n",
    "]\n",
    "\n",
    "# Find actual column names that match our key pairs\n",
    "actual_pairs = []\n",
    "for pair in key_pairs:\n",
    "    sensor1_cols = [col for col in sensor_columns if pair[0] in col.lower()]\n",
    "    sensor2_cols = [col for col in sensor_columns if pair[1] in col.lower()]\n",
    "    if sensor1_cols and sensor2_cols:\n",
    "        actual_pairs.append((sensor1_cols[0], sensor2_cols[0]))\n",
    "\n",
    "fig, axes = plt.subplots(len(actual_pairs), 2, figsize=(15, 4*len(actual_pairs)))\n",
    "if len(actual_pairs) == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for idx, (sensor1, sensor2) in enumerate(actual_pairs):\n",
    "    # Time series plot\n",
    "    ax1 = axes[idx, 0]\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    time_index = range(len(sample_battery))\n",
    "    ax1.plot(time_index, sample_battery[sensor1], 'b-', label=sensor1, alpha=0.7)\n",
    "    ax2.plot(time_index, sample_battery[sensor2], 'r-', label=sensor2, alpha=0.7)\n",
    "    \n",
    "    ax1.set_xlabel('Time Index')\n",
    "    ax1.set_ylabel(sensor1, color='b')\n",
    "    ax2.set_ylabel(sensor2, color='r')\n",
    "    ax1.set_title(f'Time Series: {sensor1} vs {sensor2}')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Cross-correlation\n",
    "    max_lags = min(50, len(sample_battery) // 4)\n",
    "    lags = range(-max_lags, max_lags + 1)\n",
    "    cross_corr = [pearsonr(sample_battery[sensor1][max(0, lag):len(sample_battery)+min(0, lag)],\n",
    "                          sample_battery[sensor2][max(0, -lag):len(sample_battery)+min(0, -lag)])[0]\n",
    "                 for lag in lags]\n",
    "    \n",
    "    axes[idx, 1].plot(lags, cross_corr, 'g-', linewidth=2)\n",
    "    axes[idx, 1].axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "    axes[idx, 1].axvline(x=0, color='k', linestyle='--', alpha=0.5)\n",
    "    axes[idx, 1].set_xlabel('Lag')\n",
    "    axes[idx, 1].set_ylabel('Cross-Correlation')\n",
    "    axes[idx, 1].set_title(f'Cross-Correlation: {sensor1} vs {sensor2}')\n",
    "    axes[idx, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Find peak correlation and lag\n",
    "    max_corr_idx = np.argmax(np.abs(cross_corr))\n",
    "    max_corr = cross_corr[max_corr_idx]\n",
    "    max_lag = lags[max_corr_idx]\n",
    "    axes[idx, 1].plot(max_lag, max_corr, 'ro', markersize=8)\n",
    "    axes[idx, 1].text(max_lag, max_corr, f'  Peak: {max_corr:.3f}\\n  Lag: {max_lag}', \n",
    "                     verticalalignment='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 5. FEATURE IMPORTANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n5. Feature Importance Analysis...\")\n",
    "\n",
    "# Define target variables for feature importance analysis\n",
    "target_variables = ['soh', 'soc', 'temperature_core', 'internal_resistance']\n",
    "actual_targets = [col for col in sensor_columns if any(target in col.lower() for target in target_variables)]\n",
    "\n",
    "if not actual_targets:\n",
    "    # Create synthetic targets if not available\n",
    "    actual_targets = sensor_columns[:4]\n",
    "\n",
    "feature_importance_results = {}\n",
    "\n",
    "for target in actual_targets[:2]:  # Analyze first 2 targets to save time\n",
    "    print(f\"\\nAnalyzing feature importance for: {target}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = sensor_data[sensor_columns].drop(columns=[target])\n",
    "    y = sensor_data[target]\n",
    "    \n",
    "    # Remove any remaining NaN values\n",
    "    mask = ~(X.isna().any(axis=1) | y.isna())\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Random Forest Feature Importance\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    rf_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Mutual Information\n",
    "    mi_scores = mutual_info_regression(X, y, random_state=42)\n",
    "    mi_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'mutual_info': mi_scores\n",
    "    }).sort_values('mutual_info', ascending=False)\n",
    "    \n",
    "    # Lasso Feature Selection\n",
    "    lasso = LassoCV(cv=5, random_state=42)\n",
    "    lasso.fit(X, y)\n",
    "    lasso_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'lasso_coef': np.abs(lasso.coef_)\n",
    "    }).sort_values('lasso_coef', ascending=False)\n",
    "    \n",
    "    # Combine results\n",
    "    importance_combined = rf_importance.merge(mi_importance, on='feature').merge(lasso_importance, on='feature')\n",
    "    importance_combined['combined_score'] = (\n",
    "        importance_combined['importance'] * 0.4 +\n",
    "        importance_combined['mutual_info'] * 0.3 +\n",
    "        importance_combined['lasso_coef'] * 0.3\n",
    "    )\n",
    "    importance_combined = importance_combined.sort_values('combined_score', ascending=False)\n",
    "    \n",
    "    feature_importance_results[target] = importance_combined\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Random Forest Importance\n",
    "    top_rf = rf_importance.head(15)\n",
    "    axes[0, 0].barh(range(len(top_rf)), top_rf['importance'])\n",
    "    axes[0, 0].set_yticks(range(len(top_rf)))\n",
    "    axes[0, 0].set_yticklabels(top_rf['feature'], fontsize=8)\n",
    "    axes[0, 0].set_xlabel('Random Forest Importance')\n",
    "    axes[0, 0].set_title(f'RF Feature Importance - {target}')\n",
    "    axes[0, 0].invert_yaxis()\n",
    "    \n",
    "    # Mutual Information\n",
    "    top_mi = mi_importance.head(15)\n",
    "    axes[0, 1].barh(range(len(top_mi)), top_mi['mutual_info'])\n",
    "    axes[0, 1].set_yticks(range(len(top_mi)))\n",
    "    axes[0, 1].set_yticklabels(top_mi['feature'], fontsize=8)\n",
    "    axes[0, 1].set_xlabel('Mutual Information Score')\n",
    "    axes[0, 1].set_title(f'Mutual Information - {target}')\n",
    "    axes[0, 1].invert_yaxis()\n",
    "    \n",
    "    # Lasso Coefficients\n",
    "    top_lasso = lasso_importance.head(15)\n",
    "    axes[1, 0].barh(range(len(top_lasso)), top_lasso['lasso_coef'])\n",
    "    axes[1, 0].set_yticks(range(len(top_lasso)))\n",
    "    axes[1, 0].set_yticklabels(top_lasso['feature'], fontsize=8)\n",
    "    axes[1, 0].set_xlabel('Lasso Coefficient (Absolute)')\n",
    "    axes[1, 0].set_title(f'Lasso Feature Selection - {target}')\n",
    "    axes[1, 0].invert_yaxis()\n",
    "    \n",
    "    # Combined Score\n",
    "    top_combined = importance_combined.head(15)\n",
    "    axes[1, 1].barh(range(len(top_combined)), top_combined['combined_score'])\n",
    "    axes[1, 1].set_yticks(range(len(top_combined)))\n",
    "    axes[1, 1].set_yticklabels(top_combined['feature'], fontsize=8)\n",
    "    axes[1, 1].set_xlabel('Combined Importance Score')\n",
    "    axes[1, 1].set_title(f'Combined Feature Importance - {target}')\n",
    "    axes[1, 1].invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 6. SENSOR NETWORK ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n6. Sensor Network Analysis...\")\n",
    "\n",
    "# Create sensor network based on correlations\n",
    "correlation_threshold = 0.5\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes (sensors)\n",
    "for sensor in sensor_columns:\n",
    "    G.add_node(sensor)\n",
    "\n",
    "# Add edges (correlations above threshold)\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_val = abs(correlation_matrix.iloc[i, j])\n",
    "        if corr_val > correlation_threshold:\n",
    "            G.add_edge(correlation_matrix.columns[i], \n",
    "                      correlation_matrix.columns[j], \n",
    "                      weight=corr_val)\n",
    "\n",
    "# Network analysis\n",
    "print(f\"\\nSensor Network Statistics:\")\n",
    "print(f\"  Nodes (sensors): {G.number_of_nodes()}\")\n",
    "print(f\"  Edges (correlations > {correlation_threshold}): {G.number_of_edges()}\")\n",
    "print(f\"  Network density: {nx.density(G):.3f}\")\n",
    "\n",
    "# Centrality measures\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "\n",
    "centrality_df = pd.DataFrame({\n",
    "    'sensor': list(degree_centrality.keys()),\n",
    "    'degree_centrality': list(degree_centrality.values()),\n",
    "    'betweenness_centrality': list(betweenness_centrality.values()),\n",
    "    'closeness_centrality': list(closeness_centrality.values()),\n",
    "    'eigenvector_centrality': list(eigenvector_centrality.values())\n",
    "})\n",
    "\n",
    "print(\"\\nTop 10 Most Central Sensors:\")\n",
    "print(centrality_df.sort_values('degree_centrality', ascending=False).head(10))\n",
    "\n",
    "# Visualize network\n",
    "plt.figure(figsize=(16, 12))\n",
    "pos = nx.spring_layout(G, k=1, iterations=50)\n",
    "\n",
    "# Draw network\n",
    "nx.draw_networkx_nodes(G, pos, node_size=[degree_centrality[node]*3000 for node in G.nodes()],\n",
    "                      node_color=list(degree_centrality.values()), cmap='viridis', alpha=0.8)\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.3, width=[G[u][v]['weight']*2 for u, v in G.edges()])\n",
    "nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold')\n",
    "\n",
    "plt.title('Sensor Correlation Network\\n(Node size and color indicate degree centrality)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 7. DIMENSIONALITY REDUCTION ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n7. Dimensionality Reduction Analysis...\")\n",
    "\n",
    "# Prepare data for dimensionality reduction\n",
    "scaler = StandardScaler()\n",
    "sensor_data_scaled = scaler.fit_transform(sensor_data[sensor_columns])\n",
    "\n",
    "# PCA Analysis\n",
    "pca = PCA()\n",
    "pca_result = pca.fit_transform(sensor_data_scaled)\n",
    "\n",
    "# Explained variance\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "n_components_99 = np.argmax(cumulative_variance >= 0.99) + 1\n",
    "\n",
    "print(f\"\\nPCA Results:\")\n",
    "print(f\"  Components for 95% variance: {n_components_95}\")\n",
    "print(f\"  Components for 99% variance: {n_components_99}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Explained variance\n",
    "axes[0, 0].plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "               pca.explained_variance_ratio_, 'bo-', markersize=4)\n",
    "axes[0, 0].set_xlabel('Principal Component')\n",
    "axes[0, 0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0, 0].set_title('PCA Explained Variance')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative explained variance\n",
    "axes[0, 1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'ro-', markersize=4)\n",
    "axes[0, 1].axhline(y=0.95, color='g', linestyle='--', label='95%')\n",
    "axes[0, 1].axhline(y=0.99, color='b', linestyle='--', label='99%')\n",
    "axes[0, 1].set_xlabel('Principal Component')\n",
    "axes[0, 1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[0, 1].set_title('PCA Cumulative Explained Variance')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# PCA scatter plot (first 2 components)\n",
    "scatter = axes[1, 0].scatter(pca_result[:, 0], pca_result[:, 1], \n",
    "                           c=sensor_data[actual_targets[0]], cmap='viridis', alpha=0.6)\n",
    "axes[1, 0].set_xlabel('First Principal Component')\n",
    "axes[1, 0].set_ylabel('Second Principal Component')\n",
    "axes[1, 0].set_title('PCA Projection (First 2 Components)')\n",
    "plt.colorbar(scatter, ax=axes[1, 0], label=actual_targets[0])\n",
    "\n",
    "# Feature loadings for first 2 components\n",
    "loadings = pca.components_[:2].T\n",
    "axes[1, 1].scatter(loadings[:, 0], loadings[:, 1], alpha=0.7)\n",
    "for i, feature in enumerate(sensor_columns):\n",
    "    if abs(loadings[i, 0]) > 0.1 or abs(loadings[i, 1]) > 0.1:  # Only label significant loadings\n",
    "        axes[1, 1].annotate(feature, (loadings[i, 0], loadings[i, 1]), \n",
    "                           fontsize=8, alpha=0.8)\n",
    "axes[1, 1].set_xlabel('PC1 Loadings')\n",
    "axes[1, 1].set_ylabel('PC2 Loadings')\n",
    "axes[1, 1].set_title('PCA Feature Loadings')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 8. ANOMALY DETECTION THROUGH CORRELATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n8. Anomaly Detection Through Correlation...\")\n",
    "\n",
    "# Calculate rolling correlations for anomaly detection\n",
    "window_size = 100\n",
    "rolling_correlations = {}\n",
    "\n",
    "# Select key sensor pairs for anomaly detection\n",
    "key_sensor_pairs = actual_pairs[:3]  # Use first 3 pairs\n",
    "\n",
    "for sensor1, sensor2 in key_sensor_pairs:\n",
    "    rolling_corr = sensor_data[sensor1].rolling(window=window_size).corr(sensor_data[sensor2])\n",
    "    rolling_correlations[f\"{sensor1}_{sensor2}\"] = rolling_corr\n",
    "\n",
    "# Detect anomalies based on correlation deviations\n",
    "anomaly_threshold = 2  # Standard deviations\n",
    "anomalies = {}\n",
    "\n",
    "for pair_name, rolling_corr in rolling_correlations.items():\n",
    "    mean_corr = rolling_corr.mean()\n",
    "    std_corr = rolling_corr.std()\n",
    "    \n",
    "    anomaly_mask = abs(rolling_corr - mean_corr) > anomaly_threshold * std_corr\n",
    "    anomalies[pair_name] = {\n",
    "        'indices': anomaly_mask[anomaly_mask].index.tolist(),\n",
    "        'values': rolling_corr[anomaly_mask].tolist(),\n",
    "        'count': anomaly_mask.sum()\n",
    "    }\n",
    "\n",
    "print(f\"\\nCorrelation Anomalies Detected:\")\n",
    "for pair_name, anomaly_info in anomalies.items():\n",
    "    print(f\"  {pair_name}: {anomaly_info['count']} anomalies\")\n",
    "\n",
    "# Visualize anomalies\n",
    "fig, axes = plt.subplots(len(key_sensor_pairs), 1, figsize=(15, 4*len(key_sensor_pairs)))\n",
    "if len(key_sensor_pairs) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (pair_name, rolling_corr) in enumerate(rolling_correlations.items()):\n",
    "    axes[idx].plot(rolling_corr.index, rolling_corr.values, 'b-', alpha=0.7, label='Rolling Correlation')\n",
    "    \n",
    "    # Plot anomalies\n",
    "    anomaly_indices = anomalies[pair_name]['indices']\n",
    "    anomaly_values = anomalies[pair_name]['values']\n",
    "    if anomaly_indices:\n",
    "        axes[idx].scatter(anomaly_indices, anomaly_values, color='red', s=50, \n",
    "                         label=f'Anomalies ({len(anomaly_indices)})', zorder=5)\n",
    "    \n",
    "    # Plot mean and threshold lines\n",
    "    mean_corr = rolling_corr.mean()\n",
    "    std_corr = rolling_corr.std()\n",
    "    axes[idx].axhline(y=mean_corr, color='green', linestyle='--', alpha=0.7, label='Mean')\n",
    "    axes[idx].axhline(y=mean_corr + anomaly_threshold*std_corr, color='orange', \n",
    "                     linestyle='--', alpha=0.7, label=f'+{anomaly_threshold}σ')\n",
    "    axes[idx].axhline(y=mean_corr - anomaly_threshold*std_corr, color='orange', \n",
    "                     linestyle='--', alpha=0.7, label=f'-{anomaly_threshold}σ')\n",
    "    \n",
    "    axes[idx].set_xlabel('Data Point Index')\n",
    "    axes[idx].set_ylabel('Rolling Correlation')\n",
    "    axes[idx].set_title(f'Correlation Anomaly Detection: {pair_name}')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 9. SENSOR FUSION RECOMMENDATIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n9. Sensor Fusion Recommendations...\")\n",
    "\n",
    "# Identify optimal sensor combinations for different targets\n",
    "fusion_recommendations = {}\n",
    "\n",
    "for target in actual_targets[:2]:\n",
    "    if target in feature_importance_results:\n",
    "        importance_data = feature_importance_results[target]\n",
    "        \n",
    "        # Top sensors by combined importance\n",
    "        top_sensors = importance_data.head(10)['feature'].tolist()\n",
    "        \n",
    "        # Calculate redundancy among top sensors\n",
    "        top_sensor_corr = correlation_matrix.loc[top_sensors, top_sensors]\n",
    "        \n",
    "        # Select diverse sensors (low correlation among themselves)\n",
    "        selected_sensors = [top_sensors[0]]  # Start with most important\n",
    "        for sensor in top_sensors[1:]:\n",
    "            # Check correlation with already selected sensors\n",
    "            max_corr_with_selected = max([abs(top_sensor_corr.loc[sensor, selected]) \n",
    "                                        for selected in selected_sensors])\n",
    "            if max_corr_with_selected < 0.8:  # Diversity threshold\n",
    "                selected_sensors.append(sensor)\n",
    "                if len(selected_sensors) >= 5:  # Limit to 5 sensors\n",
    "                    break\n",
    "        \n",
    "        fusion_recommendations[target] = {\n",
    "            'recommended_sensors': selected_sensors,\n",
    "            'importance_scores': importance_data[importance_data['feature'].isin(selected_sensors)]['combined_score'].tolist(),\n",
    "            'diversity_score': 1 - np.mean([abs(top_sensor_corr.loc[s1, s2]) \n",
    "                                          for i, s1 in enumerate(selected_sensors) \n",
    "                                          for s2 in selected_sensors[i+1:]])\n",
    "        }\n",
    "\n",
    "print(\"\\nSensor Fusion Recommendations:\")\n",
    "for target, recommendations in fusion_recommendations.items():\n",
    "    print(f\"\\nTarget: {target}\")\n",
    "    print(f\"  Recommended sensors: {recommendations['recommended_sensors']}\")\n",
    "    print(f\"  Diversity score: {recommendations['diversity_score']:.3f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 10. SUMMARY AND INSIGHTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SENSOR CORRELATION ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n1. CORRELATION OVERVIEW:\")\n",
    "print(f\"   - Total sensors analyzed: {len(sensor_columns)}\")\n",
    "print(f\"   - Strong correlations (|r| > 0.7): {len(strong_corr_df)}\")\n",
    "print(f\"   - Average absolute correlation: {np.mean(abs_correlation_values):.3f}\")\n",
    "\n",
    "print(f\"\\n2. SENSOR GROUPS:\")\n",
    "for group, sensors in sensor_groups.items():\n",
    "    print(f\"   - {group.title()}: {len(sensors)} sensors\")\n",
    "\n",
    "print(f\"\\n3. FEATURE IMPORTANCE:\")\n",
    "for target, results in feature_importance_results.items():\n",
    "    top_feature = results.iloc[0]\n",
    "    print(f\"   - {target}: Most important = {top_feature['feature']} (score: {top_feature['combined_score']:.3f})\")\n",
    "\n",
    "print(f\"\\n4. NETWORK ANALYSIS:\")\n",
    "print(f\"   - Network density: {nx.density(G):.3f}\")\n",
    "print(f\"   - Most central sensor: {centrality_df.loc[centrality_df['degree_centrality'].idxmax(), 'sensor']}\")\n",
    "\n",
    "print(f\"\\n5. DIMENSIONALITY REDUCTION:\")\n",
    "print(f\"   - Components for 95% variance: {n_components_95}\")\n",
    "print(f\"   - Components for 99% variance: {n_components_99}\")\n",
    "\n",
    "print(f\"\\n6. ANOMALY DETECTION:\")\n",
    "total_anomalies = sum([info['count'] for info in anomalies.values()])\n",
    "print(f\"   - Total correlation anomalies: {total_anomalies}\")\n",
    "\n",
    "print(f\"\\n7. FUSION RECOMMENDATIONS:\")\n",
    "for target, recommendations in fusion_recommendations.items():\n",
    "    print(f\"   - {target}: {len(recommendations['recommended_sensors'])} sensors (diversity: {recommendations['diversity_score']:.3f})\")\n",
    "\n",
    "print(f\"\\nAnalysis completed successfully!\")\n",
    "print(\"Sensor correlation analysis provides insights for optimal sensor fusion and battery state estimation.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
