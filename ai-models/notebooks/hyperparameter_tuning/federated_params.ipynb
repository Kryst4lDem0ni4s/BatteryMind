{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e63450e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BatteryMind - Federated Learning Hyperparameter Tuning\n",
    "\n",
    "Advanced hyperparameter optimization for federated learning systems in battery \n",
    "management applications. This notebook provides comprehensive tuning capabilities \n",
    "for federated aggregation algorithms, privacy parameters, and client coordination.\n",
    "\n",
    "Features:\n",
    "- Multi-objective optimization for accuracy, privacy, and communication efficiency\n",
    "- Bayesian optimization with federated learning-specific priors\n",
    "- Privacy-utility tradeoff analysis\n",
    "- Communication cost optimization\n",
    "- Client heterogeneity handling\n",
    "- Differential privacy parameter tuning\n",
    "- Aggregation algorithm comparison and optimization\n",
    "\n",
    "Author: BatteryMind Development Team\n",
    "Version: 1.0.0\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "import logging\n",
    "import json\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Optimization libraries\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.suggest.optuna import OptunaSearch\n",
    "\n",
    "# Federated learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import flwr as fl\n",
    "from flwr.common import Parameters, Scalar\n",
    "from flwr.server.strategy import FedAvg, FedProx, FedOpt\n",
    "\n",
    "# Privacy libraries\n",
    "from opacus import PrivacyEngine\n",
    "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
    "\n",
    "# BatteryMind specific imports\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from federated_learning.server.federated_server import FederatedServer\n",
    "from federated_learning.client_models.local_trainer import LocalTrainer\n",
    "from federated_learning.client_models.client_manager import ClientManager\n",
    "from federated_learning.privacy_preserving.differential_privacy import DifferentialPrivacyEngine\n",
    "from federated_learning.server.aggregation_algorithms import FedAvgAggregator, FedProxAggregator\n",
    "from federated_learning.simulation_framework.federated_simulator import FederatedSimulator\n",
    "from training_data.synthetic_datasets import generate_battery_telemetry_data\n",
    "from evaluation.metrics.accuracy_metrics import BatteryHealthMetrics\n",
    "from utils.config_parser import ConfigParser\n",
    "from utils.logging_utils import setup_logging\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logging(__name__)\n",
    "\n",
    "class FederatedOptimizer:\n",
    "    \"\"\"\n",
    "    Advanced hyperparameter optimizer for federated learning systems.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data_config: Dict[str, Any],\n",
    "                 base_fed_config: Dict[str, Any],\n",
    "                 optimization_config: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Initialize the federated optimizer.\n",
    "        \n",
    "        Args:\n",
    "            data_config: Configuration for data distribution\n",
    "            base_fed_config: Base federated learning configuration\n",
    "            optimization_config: Optimization-specific configuration\n",
    "        \"\"\"\n",
    "        self.data_config = data_config\n",
    "        self.base_fed_config = base_fed_config\n",
    "        self.optimization_config = optimization_config\n",
    "        \n",
    "        # Initialize federated components\n",
    "        self.simulator = FederatedSimulator(data_config)\n",
    "        self.metrics = BatteryHealthMetrics()\n",
    "        \n",
    "        # Generate distributed datasets\n",
    "        self.client_datasets = self._generate_client_datasets()\n",
    "        \n",
    "        # Optimization tracking\n",
    "        self.best_params = None\n",
    "        self.best_score = float('-inf')\n",
    "        self.optimization_history = []\n",
    "        \n",
    "        logger.info(\"FederatedOptimizer initialized\")\n",
    "    \n",
    "    def _generate_client_datasets(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate distributed datasets for federated learning.\"\"\"\n",
    "        logger.info(\"Generating distributed datasets for federated optimization...\")\n",
    "        \n",
    "        # Generate base dataset\n",
    "        base_data = generate_battery_telemetry_data(\n",
    "            num_batteries=self.data_config.get('total_batteries', 1000),\n",
    "            duration_days=self.data_config.get('duration_days', 30)\n",
    "        )\n",
    "        \n",
    "        # Distribute data among clients\n",
    "        num_clients = self.data_config.get('num_clients', 10)\n",
    "        client_datasets = {}\n",
    "        \n",
    "        # Create heterogeneous data distribution\n",
    "        for client_id in range(num_clients):\n",
    "            # Simulate different battery types per client\n",
    "            battery_types = ['lithium_ion', 'lifepo4', 'nimh']\n",
    "            client_battery_type = np.random.choice(battery_types)\n",
    "            \n",
    "            # Filter data by battery type and add noise for heterogeneity\n",
    "            client_data = base_data[base_data['battery_type'] == client_battery_type].copy()\n",
    "            \n",
    "            # Add client-specific noise and bias\n",
    "            noise_level = np.random.uniform(0.01, 0.05)\n",
    "            client_data['voltage'] += np.random.normal(0, noise_level, len(client_data))\n",
    "            client_data['current'] += np.random.normal(0, noise_level * 2, len(client_data))\n",
    "            \n",
    "            # Simulate different data sizes per client\n",
    "            sample_fraction = np.random.uniform(0.5, 1.0)\n",
    "            client_data = client_data.sample(frac=sample_fraction).reset_index(drop=True)\n",
    "            \n",
    "            client_datasets[f'client_{client_id}'] = client_data\n",
    "        \n",
    "        return client_datasets\n",
    "    \n",
    "    def _objective_function(self, trial) -> float:\n",
    "        \"\"\"\n",
    "        Objective function for federated learning optimization.\n",
    "        \n",
    "        Args:\n",
    "            trial: Optuna trial object\n",
    "            \n",
    "        Returns:\n",
    "            float: Objective value to maximize\n",
    "        \"\"\"\n",
    "        # Sample federated learning hyperparameters\n",
    "        params = {\n",
    "            # Federated learning parameters\n",
    "            'num_rounds': trial.suggest_int('num_rounds', 10, 100),\n",
    "            'clients_per_round': trial.suggest_int('clients_per_round', 2, min(10, len(self.client_datasets))),\n",
    "            'local_epochs': trial.suggest_int('local_epochs', 1, 10),\n",
    "            'local_batch_size': trial.suggest_categorical('local_batch_size', [16, 32, 64, 128]),\n",
    "            'local_learning_rate': trial.suggest_float('local_learning_rate', 1e-5, 1e-2, log=True),\n",
    "            \n",
    "            # Aggregation parameters\n",
    "            'aggregation_method': trial.suggest_categorical('aggregation_method', ['fedavg', 'fedprox', 'fedopt']),\n",
    "            'server_learning_rate': trial.suggest_float('server_learning_rate', 1e-3, 1e-1, log=True),\n",
    "            'server_momentum': trial.suggest_float('server_momentum', 0.0, 0.9),\n",
    "            \n",
    "            # Privacy parameters\n",
    "            'use_differential_privacy': trial.suggest_categorical('use_differential_privacy', [True, False]),\n",
    "            'noise_multiplier': trial.suggest_float('noise_multiplier', 0.1, 2.0) if params.get('use_differential_privacy', False) else 0.0,\n",
    "            'max_grad_norm': trial.suggest_float('max_grad_norm', 0.1, 10.0),\n",
    "            \n",
    "            # Communication parameters\n",
    "            'compression_rate': trial.suggest_float('compression_rate', 0.1, 1.0),\n",
    "            'quantization_bits': trial.suggest_int('quantization_bits', 2, 32),\n",
    "            \n",
    "            # Client selection parameters\n",
    "            'client_selection_strategy': trial.suggest_categorical('client_selection_strategy', ['random', 'loss_based', 'contribution_based']),\n",
    "            'min_available_clients': trial.suggest_int('min_available_clients', 2, min(8, len(self.client_datasets))),\n",
    "            \n",
    "            # Convergence parameters\n",
    "            'convergence_threshold': trial.suggest_float('convergence_threshold', 1e-6, 1e-3, log=True),\n",
    "            'patience': trial.suggest_int('patience', 5, 20),\n",
    "            \n",
    "            # Regularization parameters\n",
    "            'proximal_term': trial.suggest_float('proximal_term', 0.0, 1.0) if params.get('aggregation_method') == 'fedprox' else 0.0,\n",
    "            'l2_regularization': trial.suggest_float('l2_regularization', 1e-6, 1e-3, log=True),\n",
    "        }\n",
    "        \n",
    "        # Fix conditional parameters\n",
    "        if not params.get('use_differential_privacy', False):\n",
    "            params['noise_multiplier'] = 0.0\n",
    "        \n",
    "        if params.get('aggregation_method') != 'fedprox':\n",
    "            params['proximal_term'] = 0.0\n",
    "        \n",
    "        try:\n",
    "            # Initialize federated simulator\n",
    "            simulator = FederatedSimulator(\n",
    "                client_datasets=self.client_datasets,\n",
    "                server_config=params\n",
    "            )\n",
    "            \n",
    "            # Run federated training\n",
    "            training_results = simulator.run_federated_training(\n",
    "                num_rounds=params['num_rounds'],\n",
    "                clients_per_round=params['clients_per_round']\n",
    "            )\n",
    "            \n",
    "            # Calculate composite score\n",
    "            composite_score = self._calculate_federated_score(training_results, params)\n",
    "            \n",
    "            # Store trial results\n",
    "            trial_result = {\n",
    "                'trial_number': trial.number,\n",
    "                'params': params,\n",
    "                'score': composite_score,\n",
    "                'training_results': training_results,\n",
    "                'convergence_round': training_results.get('convergence_round', params['num_rounds']),\n",
    "                'communication_cost': training_results.get('total_communication_cost', 0),\n",
    "                'privacy_budget_used': training_results.get('privacy_budget_used', 0)\n",
    "            }\n",
    "            \n",
    "            self.optimization_history.append(trial_result)\n",
    "            \n",
    "            # Update best parameters\n",
    "            if composite_score > self.best_score:\n",
    "                self.best_score = composite_score\n",
    "                self.best_params = params.copy()\n",
    "                \n",
    "                # Save best model\n",
    "                self._save_best_federated_model(training_results, params)\n",
    "            \n",
    "            return composite_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Federated trial {trial.number} failed: {str(e)}\")\n",
    "            return float('-inf')\n",
    "    \n",
    "    def _calculate_federated_score(self, training_results: Dict[str, Any], \n",
    "                                  params: Dict[str, Any]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate composite score for federated learning optimization.\n",
    "        \n",
    "        Args:\n",
    "            training_results: Results from federated training\n",
    "            params: Federated learning parameters\n",
    "            \n",
    "        Returns:\n",
    "            float: Composite score\n",
    "        \"\"\"\n",
    "        # Primary metrics (accuracy-based)\n",
    "        global_accuracy = training_results.get('final_global_accuracy', 0) * 0.35\n",
    "        convergence_speed = max(0, 1 - training_results.get('convergence_round', 100) / 100) * 0.15\n",
    "        client_consistency = training_results.get('client_consistency_score', 0) * 0.15\n",
    "        \n",
    "        # Communication efficiency\n",
    "        comm_efficiency = max(0, 1 - training_results.get('total_communication_cost', 1000) / 1000) * 0.15\n",
    "        \n",
    "        # Privacy preservation\n",
    "        privacy_score = 0.0\n",
    "        if params.get('use_differential_privacy', False):\n",
    "            privacy_budget = training_results.get('privacy_budget_used', 0)\n",
    "            privacy_score = max(0, 1 - privacy_budget / 10.0) * 0.1  # Reward lower privacy budget usage\n",
    "        \n",
    "        # Robustness metrics\n",
    "        robustness_score = training_results.get('robustness_to_client_dropout', 0) * 0.1\n",
    "        \n",
    "        composite_score = (\n",
    "            global_accuracy + convergence_speed + client_consistency + \n",
    "            comm_efficiency + privacy_score + robustness_score\n",
    "        )\n",
    "        \n",
    "        return composite_score\n",
    "    \n",
    "    def _save_best_federated_model(self, training_results: Dict[str, Any], \n",
    "                                  params: Dict[str, Any]):\n",
    "        \"\"\"Save the best federated model and configuration.\"\"\"\n",
    "        save_dir = Path(\"../../model-artifacts/hyperparameter_tuning/federated_best\")\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save global model\n",
    "        global_model = training_results.get('global_model')\n",
    "        if global_model:\n",
    "            torch.save(global_model.state_dict(), save_dir / \"best_global_model.pt\")\n",
    "        \n",
    "        # Save configuration\n",
    "        config = {\n",
    "            'parameters': params,\n",
    "            'training_results': training_results,\n",
    "            'optimization_timestamp': pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(save_dir / \"best_federated_config.json\", 'w') as f:\n",
    "            json.dump(config, f, indent=2, default=str)\n",
    "        \n",
    "        logger.info(f\"Best federated model saved with score: {self.best_score:.4f}\")\n",
    "    \n",
    "    def optimize_federated_params(self, n_trials: int = 50) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Optimize federated learning hyperparameters.\n",
    "        \n",
    "        Args:\n",
    "            n_trials: Number of optimization trials\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing optimization results\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting federated learning optimization with {n_trials} trials\")\n",
    "        \n",
    "        # Create study with federated-specific sampler\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=TPESampler(\n",
    "                seed=42,\n",
    "                n_startup_trials=10,\n",
    "                n_ei_candidates=24\n",
    "            ),\n",
    "            pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=5)\n",
    "        )\n",
    "        \n",
    "        # Add good starting points for federated learning\n",
    "        study.enqueue_trial({\n",
    "            'num_rounds': 50,\n",
    "            'clients_per_round': 5,\n",
    "            'local_epochs': 3,\n",
    "            'local_batch_size': 32,\n",
    "            'local_learning_rate': 0.01,\n",
    "            'aggregation_method': 'fedavg',\n",
    "            'server_learning_rate': 0.1,\n",
    "            'use_differential_privacy': False,\n",
    "            'compression_rate': 1.0,\n",
    "            'client_selection_strategy': 'random'\n",
    "        })\n",
    "        \n",
    "        study.enqueue_trial({\n",
    "            'num_rounds': 30,\n",
    "            'clients_per_round': 3,\n",
    "            'local_epochs': 5,\n",
    "            'local_batch_size': 64,\n",
    "            'local_learning_rate': 0.005,\n",
    "            'aggregation_method': 'fedprox',\n",
    "            'server_learning_rate': 0.05,\n",
    "            'use_differential_privacy': True,\n",
    "            'noise_multiplier': 0.5,\n",
    "            'compression_rate': 0.5,\n",
    "            'client_selection_strategy': 'loss_based'\n",
    "        })\n",
    "        \n",
    "        # Optimize\n",
    "        study.optimize(\n",
    "            self._objective_function,\n",
    "            n_trials=n_trials,\n",
    "            timeout=self.optimization_config.get('timeout_hours', 48) * 3600,\n",
    "            callbacks=[self._federated_callback]\n",
    "        )\n",
    "        \n",
    "        # Compile results\n",
    "        results = {\n",
    "            'best_params': study.best_params,\n",
    "            'best_score': study.best_value,\n",
    "            'n_trials': len(study.trials),\n",
    "            'optimization_history': self.optimization_history,\n",
    "            'study_statistics': self._get_federated_study_statistics(study)\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Federated optimization completed. Best score: {study.best_value:.4f}\")\n",
    "        return results\n",
    "    \n",
    "    def _federated_callback(self, study, trial):\n",
    "        \"\"\"Callback for federated optimization.\"\"\"\n",
    "        if trial.number % 5 == 0:\n",
    "            logger.info(f\"Federated trial {trial.number}: Best score = {study.best_value:.4f}\")\n",
    "    \n",
    "    def _get_federated_study_statistics(self, study) -> Dict[str, Any]:\n",
    "        \"\"\"Get federated-specific study statistics.\"\"\"\n",
    "        df = study.trials_dataframe()\n",
    "        \n",
    "        stats = {\n",
    "            'total_trials': len(study.trials),\n",
    "            'completed_trials': len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]),\n",
    "            'best_trial_number': study.best_trial.number,\n",
    "            'parameter_importance': {}\n",
    "        }\n",
    "        \n",
    "        # Calculate parameter importance if enough trials\n",
    "        if len(study.trials) > 10:\n",
    "            importance = optuna.importance.get_param_importances(study)\n",
    "            stats['parameter_importance'] = importance\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def analyze_privacy_utility_tradeoff(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze privacy-utility tradeoff across optimization trials.\n",
    "        \n",
    "        Returns:\n",
    "            Dict containing privacy-utility analysis\n",
    "        \"\"\"\n",
    "        logger.info(\"Analyzing privacy-utility tradeoff...\")\n",
    "        \n",
    "        if not self.optimization_history:\n",
    "            return {}\n",
    "        \n",
    "        df = pd.DataFrame(self.optimization_history)\n",
    "        \n",
    "        # Filter trials with differential privacy\n",
    "        dp_trials = df[df['params'].apply(lambda x: x.get('use_differential_privacy', False))]\n",
    "        no_dp_trials = df[~df['params'].apply(lambda x: x.get('use_differential_privacy', False))]\n",
    "        \n",
    "        analysis = {\n",
    "            'dp_trials_count': len(dp_trials),\n",
    "            'no_dp_trials_count': len(no_dp_trials),\n",
    "            'dp_average_score': dp_trials['score'].mean() if len(dp_trials) > 0 else 0,\n",
    "            'no_dp_average_score': no_dp_trials['score'].mean() if len(no_dp_trials) > 0 else 0,\n",
    "            'privacy_utility_correlation': 0.0,\n",
    "            'optimal_privacy_params': {}\n",
    "        }\n",
    "        \n",
    "        if len(dp_trials) > 5:\n",
    "            # Calculate correlation between privacy budget and utility\n",
    "            privacy_budgets = dp_trials['training_results'].apply(\n",
    "                lambda x: x.get('privacy_budget_used', 0)\n",
    "            )\n",
    "            scores = dp_trials['score']\n",
    "            \n",
    "            if len(privacy_budgets) > 0 and privacy_budgets.std() > 0:\n",
    "                analysis['privacy_utility_correlation'] = privacy_budgets.corr(scores)\n",
    "            \n",
    "            # Find optimal privacy parameters\n",
    "            best_dp_trial = dp_trials.loc[dp_trials['score'].idxmax()]\n",
    "            analysis['optimal_privacy_params'] = {\n",
    "                'noise_multiplier': best_dp_trial['params'].get('noise_multiplier', 0),\n",
    "                'max_grad_norm': best_dp_trial['params'].get('max_grad_norm', 0),\n",
    "                'privacy_budget_used': best_dp_trial['training_results'].get('privacy_budget_used', 0),\n",
    "                'utility_score': best_dp_trial['score']\n",
    "            }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def analyze_communication_efficiency(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze communication efficiency across optimization trials.\n",
    "        \n",
    "        Returns:\n",
    "            Dict containing communication efficiency analysis\n",
    "        \"\"\"\n",
    "        logger.info(\"Analyzing communication efficiency...\")\n",
    "        \n",
    "        if not self.optimization_history:\n",
    "            return {}\n",
    "        \n",
    "        df = pd.DataFrame(self.optimization_history)\n",
    "        \n",
    "        # Extract communication costs\n",
    "        comm_costs = df['training_results'].apply(\n",
    "            lambda x: x.get('total_communication_cost', 0)\n",
    "        )\n",
    "        \n",
    "        # Extract compression rates\n",
    "        compression_rates = df['params'].apply(\n",
    "            lambda x: x.get('compression_rate', 1.0)\n",
    "        )\n",
    "        \n",
    "        # Extract quantization bits\n",
    "        quantization_bits = df['params'].apply(\n",
    "            lambda x: x.get('quantization_bits', 32)\n",
    "        )\n",
    "        \n",
    "        analysis = {\n",
    "            'average_communication_cost': comm_costs.mean(),\n",
    "            'communication_cost_std': comm_costs.std(),\n",
    "            'compression_efficiency': compression_rates.corr(comm_costs),\n",
    "            'quantization_efficiency': quantization_bits.corr(comm_costs),\n",
    "            'optimal_compression_params': {}\n",
    "        }\n",
    "        \n",
    "        # Find optimal compression parameters\n",
    "        efficiency_scores = df['score'] / (comm_costs + 1e-6)  # Score per communication cost\n",
    "        best_efficiency_idx = efficiency_scores.idxmax()\n",
    "        best_trial = df.loc[best_efficiency_idx]\n",
    "        \n",
    "        analysis['optimal_compression_params'] = {\n",
    "            'compression_rate': best_trial['params'].get('compression_rate', 1.0),\n",
    "            'quantization_bits': best_trial['params'].get('quantization_bits', 32),\n",
    "            'communication_cost': best_trial['training_results'].get('total_communication_cost', 0),\n",
    "            'efficiency_score': efficiency_scores.loc[best_efficiency_idx]\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def analyze_client_heterogeneity_impact(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze the impact of client heterogeneity on federated performance.\n",
    "        \n",
    "        Returns:\n",
    "            Dict containing heterogeneity analysis\n",
    "        \"\"\"\n",
    "        logger.info(\"Analyzing client heterogeneity impact...\")\n",
    "        \n",
    "        if not self.optimization_history:\n",
    "            return {}\n",
    "        \n",
    "        df = pd.DataFrame(self.optimization_history)\n",
    "        \n",
    "        # Extract client consistency scores\n",
    "        consistency_scores = df['training_results'].apply(\n",
    "            lambda x: x.get('client_consistency_score', 0)\n",
    "        )\n",
    "        \n",
    "        # Extract robustness scores\n",
    "        robustness_scores = df['training_results'].apply(\n",
    "            lambda x: x.get('robustness_to_client_dropout', 0)\n",
    "        )\n",
    "        \n",
    "        # Extract aggregation methods\n",
    "        aggregation_methods = df['params'].apply(\n",
    "            lambda x: x.get('aggregation_method', 'fedavg')\n",
    "        )\n",
    "        \n",
    "        analysis = {\n",
    "            'average_consistency': consistency_scores.mean(),\n",
    "            'average_robustness': robustness_scores.mean(),\n",
    "            'consistency_robustness_correlation': consistency_scores.corr(robustness_scores),\n",
    "            'aggregation_method_performance': {},\n",
    "            'optimal_heterogeneity_params': {}\n",
    "        }\n",
    "        \n",
    "        # Analyze performance by aggregation method\n",
    "        for method in aggregation_methods.unique():\n",
    "            method_trials = df[aggregation_methods == method]\n",
    "            analysis['aggregation_method_performance'][method] = {\n",
    "                'average_score': method_trials['score'].mean(),\n",
    "                'average_consistency': method_trials['training_results'].apply(\n",
    "                    lambda x: x.get('client_consistency_score', 0)\n",
    "                ).mean(),\n",
    "                'count': len(method_trials)\n",
    "            }\n",
    "        \n",
    "        # Find optimal parameters for handling heterogeneity\n",
    "                # Find optimal parameters for handling heterogeneity\n",
    "        best_consistency_idx = consistency_scores.idxmax()\n",
    "        best_trial = df.loc[best_consistency_idx]\n",
    "        \n",
    "        analysis['optimal_heterogeneity_params'] = {\n",
    "            'aggregation_method': best_trial['params'].get('aggregation_method', 'fedavg'),\n",
    "            'local_epochs': best_trial['params'].get('local_epochs', 3),\n",
    "            'clients_per_round': best_trial['params'].get('clients_per_round', 5),\n",
    "            'consistency_score': best_trial['training_results'].get('client_consistency_score', 0)\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def visualize_federated_optimization(self, results: Dict[str, Any]) -> None:\n",
    "        \"\"\"Create visualizations for federated optimization results.\"\"\"\n",
    "        if not self.optimization_history:\n",
    "            logger.warning(\"No optimization history available for visualization\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(self.optimization_history)\n",
    "        \n",
    "        # Create comprehensive visualization\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "        fig.suptitle('Federated Learning Hyperparameter Optimization Results', fontsize=16)\n",
    "        \n",
    "        # 1. Score progression\n",
    "        axes[0, 0].plot(df['trial_number'], df['score'], 'b-', alpha=0.7, label='Trial Score')\n",
    "        axes[0, 0].plot(df['trial_number'], df['score'].cummax(), 'r-', linewidth=2, label='Best Score')\n",
    "        axes[0, 0].set_xlabel('Trial Number')\n",
    "        axes[0, 0].set_ylabel('Composite Score')\n",
    "        axes[0, 0].set_title('Optimization Progress')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Communication cost vs Score\n",
    "        comm_costs = df['training_results'].apply(lambda x: x.get('total_communication_cost', 0))\n",
    "        axes[0, 1].scatter(comm_costs, df['score'], alpha=0.6)\n",
    "        axes[0, 1].set_xlabel('Communication Cost')\n",
    "        axes[0, 1].set_ylabel('Score')\n",
    "        axes[0, 1].set_title('Communication Efficiency')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Convergence rounds distribution\n",
    "        conv_rounds = df['training_results'].apply(lambda x: x.get('convergence_round', 0))\n",
    "        axes[0, 2].hist(conv_rounds, bins=20, alpha=0.7, color='green')\n",
    "        axes[0, 2].set_xlabel('Convergence Round')\n",
    "        axes[0, 2].set_ylabel('Frequency')\n",
    "        axes[0, 2].set_title('Convergence Distribution')\n",
    "        \n",
    "        # 4. Aggregation method performance\n",
    "        agg_methods = df['params'].apply(lambda x: x.get('aggregation_method', 'fedavg'))\n",
    "        method_scores = df.groupby(agg_methods)['score'].mean()\n",
    "        axes[1, 0].bar(method_scores.index, method_scores.values, color='orange')\n",
    "        axes[1, 0].set_xlabel('Aggregation Method')\n",
    "        axes[1, 0].set_ylabel('Average Score')\n",
    "        axes[1, 0].set_title('Aggregation Method Performance')\n",
    "        \n",
    "        # 5. Privacy vs Utility (if applicable)\n",
    "        dp_trials = df[df['params'].apply(lambda x: x.get('use_differential_privacy', False))]\n",
    "        if len(dp_trials) > 0:\n",
    "            privacy_budgets = dp_trials['training_results'].apply(lambda x: x.get('privacy_budget', 0))\n",
    "            dp_scores = dp_trials['score']\n",
    "            axes[1, 1].scatter(privacy_budgets, dp_scores, alpha=0.6, color='purple')\n",
    "            axes[1, 1].set_xlabel('Privacy Budget (ε)')\n",
    "            axes[1, 1].set_ylabel('Utility Score')\n",
    "            axes[1, 1].set_title('Privacy-Utility Tradeoff')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[1, 1].text(0.5, 0.5, 'No Privacy Trials', ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "            axes[1, 1].set_title('Privacy-Utility Tradeoff')\n",
    "        \n",
    "        # 6. Client participation vs performance\n",
    "        client_participation = df['params'].apply(lambda x: x.get('clients_per_round', 0))\n",
    "        axes[1, 2].scatter(client_participation, df['score'], alpha=0.6, color='red')\n",
    "        axes[1, 2].set_xlabel('Clients per Round')\n",
    "        axes[1, 2].set_ylabel('Score')\n",
    "        axes[1, 2].set_title('Client Participation Impact')\n",
    "        axes[1, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 7. Learning rate vs convergence\n",
    "        learning_rates = df['params'].apply(lambda x: x.get('learning_rate', 0))\n",
    "        axes[2, 0].scatter(learning_rates, conv_rounds, alpha=0.6, color='brown')\n",
    "        axes[2, 0].set_xlabel('Learning Rate')\n",
    "        axes[2, 0].set_ylabel('Convergence Round')\n",
    "        axes[2, 0].set_title('Learning Rate vs Convergence')\n",
    "        axes[2, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 8. Local epochs vs global accuracy\n",
    "        local_epochs = df['params'].apply(lambda x: x.get('local_epochs', 0))\n",
    "        global_acc = df['training_results'].apply(lambda x: x.get('final_accuracy', 0))\n",
    "        axes[2, 1].scatter(local_epochs, global_acc, alpha=0.6, color='cyan')\n",
    "        axes[2, 1].set_xlabel('Local Epochs')\n",
    "        axes[2, 1].set_ylabel('Final Accuracy')\n",
    "        axes[2, 1].set_title('Local Training vs Global Performance')\n",
    "        axes[2, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 9. Parameter importance heatmap\n",
    "        param_importance = self._calculate_parameter_importance(df)\n",
    "        if param_importance is not None:\n",
    "            im = axes[2, 2].imshow(param_importance.values.reshape(-1, 1), cmap='viridis', aspect='auto')\n",
    "            axes[2, 2].set_yticks(range(len(param_importance)))\n",
    "            axes[2, 2].set_yticklabels(param_importance.index)\n",
    "            axes[2, 2].set_xticks([])\n",
    "            axes[2, 2].set_title('Parameter Importance')\n",
    "            plt.colorbar(im, ax=axes[2, 2])\n",
    "        else:\n",
    "            axes[2, 2].text(0.5, 0.5, 'Insufficient Data', ha='center', va='center', transform=axes[2, 2].transAxes)\n",
    "            axes[2, 2].set_title('Parameter Importance')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def _calculate_parameter_importance(self, df: pd.DataFrame) -> Optional[pd.Series]:\n",
    "        \"\"\"Calculate parameter importance using correlation with scores.\"\"\"\n",
    "        try:\n",
    "            # Extract numeric parameters\n",
    "            numeric_params = {}\n",
    "            for _, row in df.iterrows():\n",
    "                params = row['params']\n",
    "                for key, value in params.items():\n",
    "                    if isinstance(value, (int, float)):\n",
    "                        if key not in numeric_params:\n",
    "                            numeric_params[key] = []\n",
    "                        numeric_params[key].append(value)\n",
    "            \n",
    "            if not numeric_params:\n",
    "                return None\n",
    "            \n",
    "            # Calculate correlations\n",
    "            importance = {}\n",
    "            for param_name, values in numeric_params.items():\n",
    "                if len(set(values)) > 1:  # Parameter has variation\n",
    "                    correlation = np.corrcoef(values, df['score'])[0, 1]\n",
    "                    importance[param_name] = abs(correlation) if not np.isnan(correlation) else 0\n",
    "            \n",
    "            return pd.Series(importance).sort_values(ascending=False)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating parameter importance: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def save_optimization_results(self, filepath: str):\n",
    "        \"\"\"Save optimization results to file.\"\"\"\n",
    "        results = {\n",
    "            'optimization_history': self.optimization_history,\n",
    "            'best_params': self.best_params,\n",
    "            'best_score': self.best_score,\n",
    "            'study_summary': self.study.trials_dataframe().to_dict() if self.study else None\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2, default=str)\n",
    "        \n",
    "        logger.info(f\"Optimization results saved to {filepath}\")\n",
    "    \n",
    "    def load_optimization_results(self, filepath: str):\n",
    "        \"\"\"Load optimization results from file.\"\"\"\n",
    "        with open(filepath, 'r') as f:\n",
    "            results = json.load(f)\n",
    "        \n",
    "        self.optimization_history = results.get('optimization_history', [])\n",
    "        self.best_params = results.get('best_params', {})\n",
    "        self.best_score = results.get('best_score', 0)\n",
    "        \n",
    "        logger.info(f\"Optimization results loaded from {filepath}\")\n",
    "\n",
    "# Initialize the federated optimization framework\n",
    "print(\"BatteryMind Federated Learning Hyperparameter Optimization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create optimizer instance\n",
    "optimizer = FederatedHyperparameterOptimizer(\n",
    "    n_trials=100,\n",
    "    direction='maximize',\n",
    "    storage_url='sqlite:///federated_optimization.db',\n",
    "    study_name='batterymind_federated_optimization'\n",
    ")\n",
    "\n",
    "# Define optimization search space\n",
    "search_space = {\n",
    "    'learning_rate': (1e-5, 1e-2, 'log'),\n",
    "    'local_epochs': (1, 10, 'int'),\n",
    "    'clients_per_round': (5, 50, 'int'),\n",
    "    'aggregation_method': ['fedavg', 'fedprox', 'scaffold', 'fedopt'],\n",
    "    'use_differential_privacy': [True, False],\n",
    "    'privacy_budget': (0.1, 10.0, 'log'),\n",
    "    'clip_norm': (0.1, 10.0, 'log'),\n",
    "    'server_learning_rate': (0.1, 2.0, 'log'),\n",
    "    'momentum': (0.0, 0.9, 'float'),\n",
    "    'weight_decay': (1e-6, 1e-3, 'log'),\n",
    "    'batch_size': (16, 128, 'int'),\n",
    "    'model_complexity': ['simple', 'medium', 'complex'],\n",
    "    'communication_rounds': (50, 500, 'int'),\n",
    "    'client_dropout_rate': (0.0, 0.3, 'float'),\n",
    "    'adaptive_aggregation': [True, False],\n",
    "    'personalization_strength': (0.0, 1.0, 'float')\n",
    "}\n",
    "\n",
    "print(\"Search Space Configuration:\")\n",
    "for param, config in search_space.items():\n",
    "    print(f\"  {param}: {config}\")\n",
    "\n",
    "# Run hyperparameter optimization\n",
    "print(\"\\nStarting Hyperparameter Optimization...\")\n",
    "print(\"This may take several hours depending on the number of trials and complexity.\")\n",
    "\n",
    "start_time = time.time()\n",
    "best_params, best_score = optimizer.optimize(search_space)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nOptimization completed in {(end_time - start_time)/3600:.2f} hours\")\n",
    "print(f\"Best Score: {best_score:.6f}\")\n",
    "print(\"Best Parameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Analyze optimization results\n",
    "print(\"\\nAnalyzing Optimization Results...\")\n",
    "analysis = optimizer.analyze_optimization_results()\n",
    "\n",
    "print(\"\\nOptimization Analysis Summary:\")\n",
    "print(f\"Total Trials: {analysis['total_trials']}\")\n",
    "print(f\"Successful Trials: {analysis['successful_trials']}\")\n",
    "print(f\"Failed Trials: {analysis['failed_trials']}\")\n",
    "print(f\"Success Rate: {analysis['success_rate']:.2%}\")\n",
    "\n",
    "if 'convergence_analysis' in analysis:\n",
    "    conv_analysis = analysis['convergence_analysis']\n",
    "    print(f\"\\nConvergence Analysis:\")\n",
    "    print(f\"  Average Convergence Round: {conv_analysis['mean_convergence_round']:.1f}\")\n",
    "    print(f\"  Std Convergence Round: {conv_analysis['std_convergence_round']:.1f}\")\n",
    "    print(f\"  Fastest Convergence: {conv_analysis['min_convergence_round']}\")\n",
    "    print(f\"  Slowest Convergence: {conv_analysis['max_convergence_round']}\")\n",
    "\n",
    "if 'communication_analysis' in analysis:\n",
    "    comm_analysis = analysis['communication_analysis']\n",
    "    print(f\"\\nCommunication Analysis:\")\n",
    "    print(f\"  Average Communication Cost: {comm_analysis['mean_communication_cost']:.2f}\")\n",
    "    print(f\"  Communication Efficiency Score: {comm_analysis['efficiency_score']:.4f}\")\n",
    "\n",
    "if 'privacy_analysis' in analysis:\n",
    "    privacy_analysis = analysis['privacy_analysis']\n",
    "    print(f\"\\nPrivacy Analysis:\")\n",
    "    print(f\"  Privacy-Preserving Trials: {privacy_analysis['privacy_trials']}\")\n",
    "    print(f\"  Average Privacy Budget: {privacy_analysis['mean_privacy_budget']:.2f}\")\n",
    "    print(f\"  Privacy-Utility Tradeoff: {privacy_analysis['privacy_utility_tradeoff']:.4f}\")\n",
    "\n",
    "if 'optimal_heterogeneity_params' in analysis:\n",
    "    het_params = analysis['optimal_heterogeneity_params']\n",
    "    print(f\"\\nOptimal Heterogeneity Handling:\")\n",
    "    print(f\"  Aggregation Method: {het_params['aggregation_method']}\")\n",
    "    print(f\"  Local Epochs: {het_params['local_epochs']}\")\n",
    "    print(f\"  Clients per Round: {het_params['clients_per_round']}\")\n",
    "    print(f\"  Consistency Score: {het_params['consistency_score']:.4f}\")\n",
    "\n",
    "# Create and display comprehensive visualizations\n",
    "print(\"\\nGenerating Optimization Visualizations...\")\n",
    "optimizer.visualize_federated_optimization(analysis)\n",
    "\n",
    "# Additional analysis: Parameter correlations\n",
    "print(\"\\nParameter Correlation Analysis:\")\n",
    "if optimizer.optimization_history:\n",
    "    df = pd.DataFrame(optimizer.optimization_history)\n",
    "    \n",
    "    # Extract numeric parameters for correlation analysis\n",
    "    numeric_params = {}\n",
    "    for _, row in df.iterrows():\n",
    "        params = row['params']\n",
    "        for key, value in params.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                if key not in numeric_params:\n",
    "                    numeric_params[key] = []\n",
    "                numeric_params[key].append(value)\n",
    "    \n",
    "    if numeric_params:\n",
    "        # Add scores\n",
    "        numeric_params['score'] = df['score'].tolist()\n",
    "        \n",
    "        # Create correlation DataFrame\n",
    "        param_df = pd.DataFrame(numeric_params)\n",
    "        correlation_matrix = param_df.corr()\n",
    "        \n",
    "        # Display correlation with score\n",
    "        score_correlations = correlation_matrix['score'].drop('score').sort_values(key=abs, ascending=False)\n",
    "        print(\"\\nParameter-Score Correlations:\")\n",
    "        for param, corr in score_correlations.items():\n",
    "            print(f\"  {param}: {corr:.4f}\")\n",
    "        \n",
    "        # Create correlation heatmap\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "        plt.title('Parameter Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Performance comparison with baseline methods\n",
    "print(\"\\nPerformance Comparison with Baseline Methods:\")\n",
    "\n",
    "baseline_methods = {\n",
    "    'Standard FedAvg': {\n",
    "        'learning_rate': 0.001,\n",
    "        'local_epochs': 3,\n",
    "        'clients_per_round': 10,\n",
    "        'aggregation_method': 'fedavg',\n",
    "        'use_differential_privacy': False\n",
    "    },\n",
    "    'FedProx': {\n",
    "        'learning_rate': 0.001,\n",
    "        'local_epochs': 5,\n",
    "        'clients_per_round': 10,\n",
    "        'aggregation_method': 'fedprox',\n",
    "        'use_differential_privacy': False\n",
    "    },\n",
    "    'Private FedAvg': {\n",
    "        'learning_rate': 0.001,\n",
    "        'local_epochs': 3,\n",
    "        'clients_per_round': 10,\n",
    "        'aggregation_method': 'fedavg',\n",
    "        'use_differential_privacy': True,\n",
    "        'privacy_budget': 1.0\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Baseline Method Performance:\")\n",
    "baseline_results = {}\n",
    "for method_name, params in baseline_methods.items():\n",
    "    # Simulate baseline performance (in practice, this would run actual training)\n",
    "    simulated_score = optimizer._simulate_federated_training(params)\n",
    "    baseline_results[method_name] = simulated_score\n",
    "    print(f\"  {method_name}: {simulated_score:.4f}\")\n",
    "\n",
    "print(f\"\\nOptimized Method: {best_score:.4f}\")\n",
    "print(f\"Best Baseline: {max(baseline_results.values()):.4f}\")\n",
    "print(f\"Improvement: {((best_score - max(baseline_results.values())) / max(baseline_results.values()) * 100):.2f}%\")\n",
    "\n",
    "# Create performance comparison visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "methods = list(baseline_results.keys()) + ['Optimized']\n",
    "scores = list(baseline_results.values()) + [best_score]\n",
    "colors = ['lightblue'] * len(baseline_results) + ['red']\n",
    "\n",
    "bars = plt.bar(methods, scores, color=colors, alpha=0.7)\n",
    "plt.ylabel('Performance Score')\n",
    "plt.title('Federated Learning Method Performance Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, scores):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{score:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save optimization results\n",
    "results_file = 'federated_optimization_results.json'\n",
    "optimizer.save_optimization_results(results_file)\n",
    "print(f\"\\nOptimization results saved to {results_file}\")\n",
    "\n",
    "# Generate recommendations for production deployment\n",
    "print(\"\\nRecommendations for Production Deployment:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1. Optimal Hyperparameters:\")\n",
    "optimal_config = {\n",
    "    'learning_rate': best_params.get('learning_rate', 0.001),\n",
    "    'local_epochs': best_params.get('local_epochs', 3),\n",
    "    'clients_per_round': best_params.get('clients_per_round', 10),\n",
    "    'aggregation_method': best_params.get('aggregation_method', 'fedavg'),\n",
    "    'communication_rounds': best_params.get('communication_rounds', 100),\n",
    "    'batch_size': best_params.get('batch_size', 32)\n",
    "}\n",
    "\n",
    "for param, value in optimal_config.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(\"\\n2. Privacy Configuration:\")\n",
    "if best_params.get('use_differential_privacy', False):\n",
    "    print(f\"  Enable Differential Privacy: Yes\")\n",
    "    print(f\"  Privacy Budget (ε): {best_params.get('privacy_budget', 1.0):.2f}\")\n",
    "    print(f\"  Clip Norm: {best_params.get('clip_norm', 1.0):.2f}\")\n",
    "else:\n",
    "    print(f\"  Enable Differential Privacy: No\")\n",
    "\n",
    "print(\"\\n3. Communication Optimization:\")\n",
    "print(f\"  Clients per Round: {best_params.get('clients_per_round', 10)}\")\n",
    "print(f\"  Client Dropout Rate: {best_params.get('client_dropout_rate', 0.1):.2f}\")\n",
    "print(f\"  Adaptive Aggregation: {best_params.get('adaptive_aggregation', False)}\")\n",
    "\n",
    "print(\"\\n4. Model Configuration:\")\n",
    "print(f\"  Model Complexity: {best_params.get('model_complexity', 'medium')}\")\n",
    "print(f\"  Weight Decay: {best_params.get('weight_decay', 1e-4):.2e}\")\n",
    "print(f\"  Momentum: {best_params.get('momentum', 0.9):.2f}\")\n",
    "\n",
    "print(\"\\n5. Personalization Settings:\")\n",
    "print(f\"  Personalization Strength: {best_params.get('personalization_strength', 0.0):.2f}\")\n",
    "\n",
    "print(\"\\n6. Monitoring and Alerting:\")\n",
    "print(\"  - Monitor client participation rates\")\n",
    "print(\"  - Track communication costs per round\")\n",
    "print(\"  - Set up alerts for convergence failures\")\n",
    "print(\"  - Monitor privacy budget consumption\")\n",
    "\n",
    "print(\"\\n7. Scalability Considerations:\")\n",
    "print(\"  - Plan for client heterogeneity\")\n",
    "print(\"  - Implement fault tolerance mechanisms\")\n",
    "print(\"  - Consider asynchronous aggregation for large deployments\")\n",
    "print(\"  - Set up load balancing for server infrastructure\")\n",
    "\n",
    "print(\"\\n8. Security Recommendations:\")\n",
    "print(\"  - Implement secure aggregation protocols\")\n",
    "print(\"  - Use authenticated communication channels\")\n",
    "print(\"  - Regular security audits of federated components\")\n",
    "print(\"  - Consider homomorphic encryption for sensitive data\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEDERATED LEARNING OPTIMIZATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best Configuration Score: {best_score:.6f}\")\n",
    "print(f\"Total Optimization Time: {(end_time - start_time)/3600:.2f} hours\")\n",
    "print(f\"Trials Evaluated: {len(optimizer.optimization_history)}\")\n",
    "print(f\"Success Rate: {analysis['success_rate']:.2%}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Export configuration for deployment\n",
    "deployment_config = {\n",
    "    'model_type': 'federated_battery_health',\n",
    "    'version': '1.0.0',\n",
    "    'optimized_hyperparameters': best_params,\n",
    "    'performance_metrics': {\n",
    "        'optimization_score': best_score,\n",
    "        'convergence_rounds': analysis.get('convergence_analysis', {}).get('mean_convergence_round', 100),\n",
    "        'communication_efficiency': analysis.get('communication_analysis', {}).get('efficiency_score', 0.5)\n",
    "    },\n",
    "    'deployment_recommendations': {\n",
    "        'min_clients': 10,\n",
    "        'recommended_clients': best_params.get('clients_per_round', 10),\n",
    "        'max_clients': 100,\n",
    "        'monitoring_frequency': 'every_round',\n",
    "        'backup_strategy': 'checkpoint_every_10_rounds'\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('federated_deployment_config.json', 'w') as f:\n",
    "    json.dump(deployment_config, f, indent=2)\n",
    "\n",
    "print(\"Deployment configuration saved to: federated_deployment_config.json\")\n",
    "print(\"Ready for production deployment!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
