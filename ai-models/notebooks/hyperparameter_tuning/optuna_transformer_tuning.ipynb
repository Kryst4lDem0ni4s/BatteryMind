{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced2eee0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# BatteryMind – Hyper-parameter Tuning (Optuna) - Transformer Battery Predictor\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cell 1 – Environment & Imports\n",
    "import os, json, yaml, time, warnings, logging, random, tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from ai_models.transformers.battery_health_predictor.data_loader import (\n",
    "    BatterySequenceDataset\n",
    ")\n",
    "from ai_models.utils.logging_utils import configure_logging\n",
    "from ai_models.config import training_config_path  # YAML from config/\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "configure_logging()\n",
    "logger = logging.getLogger(\"tuning\")\n",
    "\n",
    "# Cell 2 – Load Training Configuration\n",
    "with open(training_config_path, \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "DATA_PATH = cfg[\"data\"][\"telemetry_csv\"]\n",
    "MODEL_NAME = cfg[\"model\"][\"base_checkpoint\"]\n",
    "NUM_LABELS = cfg[\"model\"][\"num_labels\"]\n",
    "\n",
    "# Cell 3 – Load & Split Dataset\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "train_df, valid_df = train_test_split(\n",
    "    df, test_size=0.1, shuffle=False  # keep temporal order\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "train_ds = BatterySequenceDataset(train_df, tokenizer)\n",
    "valid_ds = BatterySequenceDataset(valid_df, tokenizer)\n",
    "\n",
    "# Cell 4 – Objective Function for Optuna\n",
    "def objective(trial):\n",
    "    hyperparams = {\n",
    "        \"learning_rate\": trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"epochs\", 3, 15),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\n",
    "            \"batch\", [8, 16, 32]\n",
    "        ),\n",
    "        \"weight_decay\": trial.suggest_float(\"wd\", 0.0, 0.3),\n",
    "        \"warmup_ratio\": trial.suggest_float(\"warmup\", 0.0, 0.3),\n",
    "        \"gradient_accumulation_steps\": trial.suggest_int(\"accum\", 1, 4),\n",
    "        \"fp16\": True,\n",
    "    }\n",
    "    args = TrainingArguments(\n",
    "        output_dir=tempfile.mkdtemp(),\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        **hyperparams\n",
    "    )\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, num_labels=NUM_LABELS\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=valid_ds,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    metrics = trainer.train()\n",
    "    eval_metric = metrics.training_loss\n",
    "    return eval_metric\n",
    "\n",
    "# Cell 5 – Run Study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50, timeout=2*60*60)  # 50 trials / 2 h max\n",
    "\n",
    "# Cell 6 – Save Best Params\n",
    "best_params = study.best_params\n",
    "with open(\"best_transformer_params.json\", \"w\") as f:\n",
    "    json.dump(best_params, f, indent=2)\n",
    "print(\"Best params:\", best_params)\n",
    "\n",
    "# Cell 7 – Train Final Model with Best Params\n",
    "args_best = TrainingArguments(\n",
    "    output_dir=\"./transformer_best\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_steps=100,\n",
    "    fp16=True,\n",
    "    **best_params\n",
    ")\n",
    "final_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=NUM_LABELS\n",
    ")\n",
    "trainer_best = Trainer(\n",
    "    model=final_model,\n",
    "    args=args_best,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer_best.train()\n",
    "trainer_best.save_model(\"./transformer_best\")\n",
    "\n",
    "# Cell 8 – Register Model Artefact\n",
    "from ai_models.model_artifacts import (\n",
    "    get_model_manager, create_model_metadata\n",
    ")\n",
    "mm = get_model_manager()\n",
    "metadata = create_model_metadata(\n",
    "    model_id=f\"transformer_bhp_{int(time.time())}\",\n",
    "    model_type=\"transformer\",\n",
    "    version=\"1.0.1\",\n",
    "    name=\"Transformer BHP Opt-tuned\",\n",
    "    description=\"Optuna tuned Transformer Battery-Health predictor\",\n",
    "    training_metrics={\"loss\": study.best_value},\n",
    "    training_dataset=DATA_PATH,\n",
    "    training_duration_hours=study.study_duration.total_seconds()/3600,\n",
    "    hyperparameters=best_params,\n",
    "    created_by=os.getenv(\"USER\", \"notebook\")\n",
    ")\n",
    "mm.register_model(metadata, {\"model.pkl\": \"./transformer_best/pytorch_model.bin\"})\n",
    "print(\"Model registered:\", metadata.model_id)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
