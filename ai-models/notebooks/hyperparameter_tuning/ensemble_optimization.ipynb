{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae4bea9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BatteryMind - Ensemble Model Optimization\n",
    "\n",
    "Advanced ensemble model optimization for battery health prediction combining\n",
    "transformer, federated learning, and reinforcement learning predictions.\n",
    "This notebook implements hyperparameter tuning for ensemble architectures\n",
    "using Optuna for automated optimization.\n",
    "\n",
    "Features:\n",
    "- Multi-model ensemble optimization\n",
    "- Bayesian optimization for ensemble weights\n",
    "- Stacking and voting ensemble strategies\n",
    "- Meta-learning for ensemble combination\n",
    "- Performance tracking and model selection\n",
    "- Physics-informed ensemble validation\n",
    "\n",
    "Author: BatteryMind Development Team\n",
    "Version: 1.0.0\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "import logging\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# BatteryMind imports\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from transformers.ensemble_model.ensemble import EnsembleModel\n",
    "from transformers.ensemble_model.voting_classifier import VotingClassifier\n",
    "from transformers.ensemble_model.stacking_regressor import StackingRegressor as BatteryStackingRegressor\n",
    "from transformers.ensemble_model.model_fusion import ModelFusion\n",
    "from transformers.battery_health_predictor.predictor import BatteryHealthPredictor\n",
    "from federated_learning.server.global_model import GlobalModel\n",
    "from reinforcement_learning.agents.charging_agent import ChargingAgent\n",
    "from training_data.synthetic_datasets import generate_battery_telemetry_data\n",
    "from training_data.preprocessing_scripts.feature_extractor import BatteryFeatureExtractor\n",
    "from training_data.preprocessing_scripts.data_cleaner import BatteryDataCleaner\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ðŸ”‹ BatteryMind Ensemble Model Optimization Notebook\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Notebook initialized at: {datetime.now()}\")\n",
    "print()\n",
    "\n",
    "# Configuration\n",
    "OPTIMIZATION_CONFIG = {\n",
    "    'n_trials': 150,\n",
    "    'n_jobs': mp.cpu_count(),\n",
    "    'study_name': 'batterymind_ensemble_optimization',\n",
    "    'optimization_direction': 'minimize',  # Minimize ensemble prediction error\n",
    "    'pruner': 'MedianPruner',\n",
    "    'sampler': 'TPESampler',\n",
    "    'ensemble_strategies': ['voting', 'stacking', 'weighted_average', 'meta_learning'],\n",
    "    'base_models': ['transformer', 'federated', 'xgboost', 'lightgbm', 'neural_network'],\n",
    "    'meta_models': ['linear', 'ridge', 'decision_tree', 'neural_network'],\n",
    "    'cv_folds': 5,\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Ensemble hyperparameter search spaces\n",
    "ENSEMBLE_SEARCH_SPACES = {\n",
    "    'voting': {\n",
    "        'model_weights': 'optimize',  # Will be optimized\n",
    "        'voting_strategy': ['soft', 'hard'],\n",
    "        'weight_optimization_method': ['grid_search', 'bayesian', 'evolutionary']\n",
    "    },\n",
    "    'stacking': {\n",
    "        'meta_model': ['linear', 'ridge', 'decision_tree', 'neural_network', 'xgboost'],\n",
    "        'cv_folds': [3, 5, 7, 10],\n",
    "        'use_features_in_secondary': [True, False],\n",
    "        'stack_method': ['predict', 'predict_proba'],\n",
    "        'meta_model_params': 'optimize'\n",
    "    },\n",
    "    'weighted_average': {\n",
    "        'weight_optimization': ['equal', 'performance_based', 'bayesian', 'learned'],\n",
    "        'performance_metric': ['mse', 'mae', 'r2'],\n",
    "        'decay_factor': (0.9, 0.999),\n",
    "        'learning_rate': (0.001, 0.1)\n",
    "    },\n",
    "    'meta_learning': {\n",
    "        'meta_features': ['prediction_confidence', 'model_agreement', 'data_quality', 'physics_constraints'],\n",
    "        'meta_model_architecture': 'optimize',\n",
    "        'feature_engineering': ['polynomial', 'interaction', 'ensemble_statistics'],\n",
    "        'regularization': (0.0, 1.0)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Base model configurations\n",
    "BASE_MODEL_CONFIGS = {\n",
    "    'transformer': {\n",
    "        'model_type': 'BatteryHealthPredictor',\n",
    "        'pretrained_path': '../../model-artifacts/trained_models/transformer_v1.0/',\n",
    "        'fine_tuning': True,\n",
    "        'confidence_estimation': True\n",
    "    },\n",
    "    'federated': {\n",
    "        'model_type': 'GlobalModel',\n",
    "        'pretrained_path': '../../model-artifacts/trained_models/federated_v1.0/',\n",
    "        'aggregation_method': 'fedavg',\n",
    "        'privacy_preserving': True\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'n_estimators': (50, 500),\n",
    "        'max_depth': (3, 10),\n",
    "        'learning_rate': (0.01, 0.3),\n",
    "        'subsample': (0.6, 1.0),\n",
    "        'colsample_bytree': (0.6, 1.0),\n",
    "        'reg_alpha': (0, 10),\n",
    "        'reg_lambda': (0, 10)\n",
    "    },\n",
    "    'lightgbm': {\n",
    "        'n_estimators': (50, 500),\n",
    "        'max_depth': (3, 15),\n",
    "        'learning_rate': (0.01, 0.3),\n",
    "        'subsample': (0.6, 1.0),\n",
    "        'colsample_bytree': (0.6, 1.0),\n",
    "        'reg_alpha': (0, 10),\n",
    "        'reg_lambda': (0, 10)\n",
    "    },\n",
    "    'neural_network': {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50), (100, 100)],\n",
    "        'activation': ['relu', 'tanh', 'logistic'],\n",
    "        'solver': ['adam', 'lbfgs'],\n",
    "        'alpha': (0.0001, 0.1),\n",
    "        'learning_rate': ['constant', 'adaptive'],\n",
    "        'max_iter': (200, 1000)\n",
    "    }\n",
    "}\n",
    "\n",
    "class EnsembleOptimizer:\n",
    "    \"\"\"\n",
    "    Advanced ensemble model optimizer for battery health prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.study = None\n",
    "        self.best_params = {}\n",
    "        self.base_models = {}\n",
    "        self.ensemble_models = {}\n",
    "        self.optimization_history = []\n",
    "        \n",
    "        # Load training data\n",
    "        self.X_train, self.y_train, self.X_test, self.y_test = self._load_training_data()\n",
    "        \n",
    "        # Initialize base models\n",
    "        self._initialize_base_models()\n",
    "        \n",
    "        # Results storage\n",
    "        self.results = {\n",
    "            'trials': [],\n",
    "            'best_ensemble_per_strategy': {},\n",
    "            'performance_comparison': {},\n",
    "            'model_contributions': {},\n",
    "            'optimization_plots': {}\n",
    "        }\n",
    "    \n",
    "    def _load_training_data(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Load and prepare training data for ensemble optimization.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of X_train, y_train, X_test, y_test\n",
    "        \"\"\"\n",
    "        print(\"ðŸ“Š Loading training data...\")\n",
    "        \n",
    "        # Generate synthetic battery data\n",
    "        battery_data = generate_battery_telemetry_data(\n",
    "            num_batteries=1000,\n",
    "            duration_days=365\n",
    "        )\n",
    "        \n",
    "        # Clean and extract features\n",
    "        cleaner = BatteryDataCleaner()\n",
    "        cleaned_data = cleaner.clean(battery_data)\n",
    "        \n",
    "        feature_extractor = BatteryFeatureExtractor()\n",
    "        features = feature_extractor.extract(cleaned_data)\n",
    "        \n",
    "        # Prepare target variable (battery health)\n",
    "        y = features['soh'].values\n",
    "        \n",
    "        # Remove target from features\n",
    "        X = features.drop(['soh'], axis=1).values\n",
    "        \n",
    "        # Train-test split\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=self.config['test_size'], \n",
    "            random_state=self.config['random_state']\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Data loaded: {X_train.shape[0]} training samples, {X_test.shape[0]} test samples\")\n",
    "        return X_train, y_train, X_test, y_test\n",
    "    \n",
    "    def _initialize_base_models(self):\n",
    "        \"\"\"Initialize base models for ensemble.\"\"\"\n",
    "        print(\"ðŸ”§ Initializing base models...\")\n",
    "        \n",
    "        # Note: For production, these would load actual pre-trained models\n",
    "        # Here we create placeholder models for demonstration\n",
    "        \n",
    "        self.base_models = {\n",
    "            'transformer': self._create_transformer_model(),\n",
    "            'federated': self._create_federated_model(),\n",
    "            'xgboost': self._create_xgboost_model(),\n",
    "            'lightgbm': self._create_lightgbm_model(),\n",
    "            'neural_network': self._create_neural_network_model()\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… Initialized {len(self.base_models)} base models\")\n",
    "    \n",
    "    def _create_transformer_model(self):\n",
    "        \"\"\"Create transformer model placeholder.\"\"\"\n",
    "        # In production, this would load the actual transformer model\n",
    "        return MLPRegressor(hidden_layer_sizes=(100, 50), random_state=42)\n",
    "    \n",
    "    def _create_federated_model(self):\n",
    "        \"\"\"Create federated model placeholder.\"\"\"\n",
    "        # In production, this would load the actual federated model\n",
    "        return MLPRegressor(hidden_layer_sizes=(80, 40), random_state=42)\n",
    "    \n",
    "    def _create_xgboost_model(self):\n",
    "        \"\"\"Create XGBoost model with default parameters.\"\"\"\n",
    "        return xgb.XGBRegressor(random_state=42)\n",
    "    \n",
    "    def _create_lightgbm_model(self):\n",
    "        \"\"\"Create LightGBM model with default parameters.\"\"\"\n",
    "        return lgb.LGBMRegressor(random_state=42, verbose=-1)\n",
    "    \n",
    "    def _create_neural_network_model(self):\n",
    "        \"\"\"Create neural network model.\"\"\"\n",
    "        return MLPRegressor(hidden_layer_sizes=(64, 32), random_state=42)\n",
    "    \n",
    "    def suggest_ensemble_hyperparameters(self, trial: optuna.Trial, \n",
    "                                       strategy: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Suggest hyperparameters for ensemble strategy.\n",
    "        \n",
    "        Args:\n",
    "            trial: Optuna trial object\n",
    "            strategy: Ensemble strategy name\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of suggested hyperparameters\n",
    "        \"\"\"\n",
    "        space = ENSEMBLE_SEARCH_SPACES[strategy]\n",
    "        params = {}\n",
    "        \n",
    "        if strategy == 'voting':\n",
    "            # Optimize model weights\n",
    "            total_models = len(self.base_models)\n",
    "            weights = []\n",
    "            for i, model_name in enumerate(self.base_models.keys()):\n",
    "                if i < total_models - 1:\n",
    "                    weight = trial.suggest_float(f'weight_{model_name}', 0.1, 1.0)\n",
    "                    weights.append(weight)\n",
    "                else:\n",
    "                    # Last weight is determined by others to sum to 1\n",
    "                    weights.append(max(0.1, 1.0 - sum(weights)))\n",
    "            \n",
    "            # Normalize weights\n",
    "            weight_sum = sum(weights)\n",
    "            weights = [w / weight_sum for w in weights]\n",
    "            \n",
    "            params['model_weights'] = dict(zip(self.base_models.keys(), weights))\n",
    "            params['voting_strategy'] = trial.suggest_categorical('voting_strategy', \n",
    "                                                                space['voting_strategy'])\n",
    "        \n",
    "        elif strategy == 'stacking':\n",
    "            params['meta_model'] = trial.suggest_categorical('meta_model', \n",
    "                                                           space['meta_model'])\n",
    "            params['cv_folds'] = trial.suggest_categorical('cv_folds', \n",
    "                                                         space['cv_folds'])\n",
    "            params['use_features_in_secondary'] = trial.suggest_categorical(\n",
    "                'use_features_in_secondary', space['use_features_in_secondary'])\n",
    "            \n",
    "            # Meta-model specific parameters\n",
    "            if params['meta_model'] == 'ridge':\n",
    "                params['meta_alpha'] = trial.suggest_float('meta_alpha', 0.1, 10.0)\n",
    "            elif params['meta_model'] == 'decision_tree':\n",
    "                params['meta_max_depth'] = trial.suggest_int('meta_max_depth', 3, 10)\n",
    "            elif params['meta_model'] == 'neural_network':\n",
    "                params['meta_hidden_size'] = trial.suggest_categorical(\n",
    "                    'meta_hidden_size', [32, 64, 128])\n",
    "            elif params['meta_model'] == 'xgboost':\n",
    "                params['meta_n_estimators'] = trial.suggest_int('meta_n_estimators', 50, 200)\n",
    "                params['meta_learning_rate'] = trial.suggest_float('meta_learning_rate', 0.01, 0.3)\n",
    "        \n",
    "        elif strategy == 'weighted_average':\n",
    "            params['weight_optimization'] = trial.suggest_categorical(\n",
    "                'weight_optimization', space['weight_optimization'])\n",
    "            params['performance_metric'] = trial.suggest_categorical(\n",
    "                'performance_metric', space['performance_metric'])\n",
    "            \n",
    "            if params['weight_optimization'] == 'bayesian':\n",
    "                params['decay_factor'] = trial.suggest_float('decay_factor', \n",
    "                                                           *space['decay_factor'])\n",
    "                params['learning_rate'] = trial.suggest_float('learning_rate', \n",
    "                                                            *space['learning_rate'])\n",
    "        \n",
    "        elif strategy == 'meta_learning':\n",
    "            params['meta_features'] = trial.suggest_categorical(\n",
    "                'meta_features', [space['meta_features']])\n",
    "            params['regularization'] = trial.suggest_float('regularization', \n",
    "                                                         *space['regularization'])\n",
    "            params['feature_engineering'] = trial.suggest_categorical(\n",
    "                'feature_engineering', space['feature_engineering'])\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def create_ensemble_model(self, strategy: str, params: Dict[str, Any]) -> Any:\n",
    "        \"\"\"\n",
    "        Create ensemble model with given strategy and parameters.\n",
    "        \n",
    "        Args:\n",
    "            strategy: Ensemble strategy\n",
    "            params: Hyperparameters\n",
    "            \n",
    "        Returns:\n",
    "            Ensemble model instance\n",
    "        \"\"\"\n",
    "        base_models_list = [(name, model) for name, model in self.base_models.items()]\n",
    "        \n",
    "        if strategy == 'voting':\n",
    "            weights = [params['model_weights'][name] for name in self.base_models.keys()]\n",
    "            return VotingRegressor(\n",
    "                estimators=base_models_list,\n",
    "                weights=weights\n",
    "            )\n",
    "        \n",
    "        elif strategy == 'stacking':\n",
    "            # Create meta-model\n",
    "            if params['meta_model'] == 'linear':\n",
    "                meta_model = LinearRegression()\n",
    "            elif params['meta_model'] == 'ridge':\n",
    "                meta_model = RidgeCV(alphas=[params.get('meta_alpha', 1.0)])\n",
    "            elif params['meta_model'] == 'decision_tree':\n",
    "                meta_model = DecisionTreeRegressor(\n",
    "                    max_depth=params.get('meta_max_depth', 5),\n",
    "                    random_state=42\n",
    "                )\n",
    "            elif params['meta_model'] == 'neural_network':\n",
    "                meta_model = MLPRegressor(\n",
    "                    hidden_layer_sizes=(params.get('meta_hidden_size', 64),),\n",
    "                    random_state=42\n",
    "                )\n",
    "            elif params['meta_model'] == 'xgboost':\n",
    "                meta_model = xgb.XGBRegressor(\n",
    "                    n_estimators=params.get('meta_n_estimators', 100),\n",
    "                    learning_rate=params.get('meta_learning_rate', 0.1),\n",
    "                    random_state=42\n",
    "                )\n",
    "            else:\n",
    "                meta_model = LinearRegression()\n",
    "            \n",
    "            return StackingRegressor(\n",
    "                estimators=base_models_list,\n",
    "                final_estimator=meta_model,\n",
    "                cv=params['cv_folds'],\n",
    "                passthrough=params['use_features_in_secondary']\n",
    "            )\n",
    "        \n",
    "        elif strategy == 'weighted_average':\n",
    "            return WeightedAverageEnsemble(\n",
    "                base_models=dict(base_models_list),\n",
    "                optimization_method=params['weight_optimization'],\n",
    "                performance_metric=params['performance_metric'],\n",
    "                decay_factor=params.get('decay_factor', 0.95),\n",
    "                learning_rate=params.get('learning_rate', 0.01)\n",
    "            )\n",
    "        \n",
    "        elif strategy == 'meta_learning':\n",
    "            return MetaLearningEnsemble(\n",
    "                base_models=dict(base_models_list),\n",
    "                meta_features=params['meta_features'],\n",
    "                regularization=params['regularization'],\n",
    "                feature_engineering=params['feature_engineering']\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown ensemble strategy: {strategy}\")\n",
    "    \n",
    "    def evaluate_ensemble(self, ensemble_model: Any, X_test: np.ndarray, \n",
    "                         y_test: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate ensemble model performance.\n",
    "        \n",
    "        Args:\n",
    "            ensemble_model: Trained ensemble model\n",
    "            X_test: Test features\n",
    "            y_test: Test targets\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of performance metrics\n",
    "        \"\"\"\n",
    "        # Make predictions\n",
    "        y_pred = ensemble_model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        # Calculate battery-specific metrics\n",
    "        soh_accuracy = np.mean(np.abs(y_pred - y_test) < 0.05)  # Within 5% SoH\n",
    "        \n",
    "        # Physics constraint validation\n",
    "        physics_violations = np.sum((y_pred < 0) | (y_pred > 1))\n",
    "        physics_compliance = 1.0 - (physics_violations / len(y_pred))\n",
    "        \n",
    "        return {\n",
    "            'mse': mse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'rmse': rmse,\n",
    "            'soh_accuracy': soh_accuracy,\n",
    "            'physics_compliance': physics_compliance,\n",
    "            'prediction_std': np.std(y_pred)\n",
    "        }\n",
    "    \n",
    "    def objective(self, trial: optuna.Trial) -> float:\n",
    "        \"\"\"\n",
    "        Objective function for ensemble optimization.\n",
    "        \n",
    "        Args:\n",
    "            trial: Optuna trial object\n",
    "            \n",
    "        Returns:\n",
    "            Objective value (prediction error)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Select ensemble strategy\n",
    "            strategy = trial.suggest_categorical('strategy', \n",
    "                                               self.config['ensemble_strategies'])\n",
    "            \n",
    "            # Suggest hyperparameters\n",
    "            params = self.suggest_ensemble_hyperparameters(trial, strategy)\n",
    "            \n",
    "            # Create ensemble model\n",
    "            ensemble_model = self.create_ensemble_model(strategy, params)\n",
    "            \n",
    "            # Train ensemble model\n",
    "            ensemble_model.fit(self.X_train, self.y_train)\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            eval_metrics = self.evaluate_ensemble(ensemble_model, self.X_test, self.y_test)\n",
    "            \n",
    "            # Cross-validation for robustness\n",
    "            cv_scores = cross_val_score(\n",
    "                ensemble_model, self.X_train, self.y_train,\n",
    "                cv=KFold(n_splits=self.config['cv_folds'], shuffle=True, random_state=42),\n",
    "                scoring='neg_mean_squared_error'\n",
    "            )\n",
    "            cv_mse = -np.mean(cv_scores)\n",
    "            \n",
    "            # Composite objective (weighted combination of metrics)\n",
    "            objective_value = (\n",
    "                0.4 * eval_metrics['mse'] +\n",
    "                0.3 * cv_mse +\n",
    "                0.1 * (1 - eval_metrics['soh_accuracy']) +\n",
    "                0.1 * (1 - eval_metrics['physics_compliance']) +\n",
    "                0.1 * eval_metrics['prediction_std']\n",
    "            )\n",
    "            \n",
    "            # Store trial results\n",
    "            trial_result = {\n",
    "                'trial_number': trial.number,\n",
    "                'strategy': strategy,\n",
    "                'params': params,\n",
    "                'eval_metrics': eval_metrics,\n",
    "                'cv_mse': cv_mse,\n",
    "                'objective_value': objective_value\n",
    "            }\n",
    "            \n",
    "            self.results['trials'].append(trial_result)\n",
    "            \n",
    "            return objective_value\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Trial {trial.number} failed: {str(e)}\")\n",
    "            return np.inf\n",
    "    \n",
    "    def optimize(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run ensemble optimization.\n",
    "        \n",
    "        Returns:\n",
    "            Optimization results\n",
    "        \"\"\"\n",
    "        print(\"ðŸš€ Starting Ensemble Model Optimization\")\n",
    "        print(f\"Configuration: {self.config}\")\n",
    "        print()\n",
    "        \n",
    "        # Create study\n",
    "        study = optuna.create_study(\n",
    "            direction=self.config['optimization_direction'],\n",
    "            study_name=self.config['study_name'],\n",
    "            sampler=optuna.samplers.TPESampler(seed=42),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=10, n_warmup_steps=5)\n",
    "        )\n",
    "        \n",
    "        # Run optimization\n",
    "        study.optimize(\n",
    "            self.objective,\n",
    "            n_trials=self.config['n_trials'],\n",
    "            n_jobs=self.config['n_jobs'],\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        self.study = study\n",
    "        self.best_params = study.best_params\n",
    "        \n",
    "        # Process results\n",
    "        self._process_optimization_results()\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def _process_optimization_results(self):\n",
    "        \"\"\"Process optimization results and generate insights.\"\"\"\n",
    "        # Group results by strategy\n",
    "        strategy_results = {}\n",
    "        for trial in self.results['trials']:\n",
    "            strategy = trial['strategy']\n",
    "            if strategy not in strategy_results:\n",
    "                strategy_results[strategy] = []\n",
    "            strategy_results[strategy].append(trial)\n",
    "        \n",
    "        # Find best ensemble for each strategy\n",
    "        for strategy, trials in strategy_results.items():\n",
    "            if trials:\n",
    "                best_trial = min(trials, key=lambda x: x['objective_value'])\n",
    "                self.results['best_ensemble_per_strategy'][strategy] = {\n",
    "                    'params': best_trial['params'],\n",
    "                    'performance': best_trial['eval_metrics'],\n",
    "                    'objective_value': best_trial['objective_value']\n",
    "                }\n",
    "        \n",
    "        # Create performance comparison\n",
    "        self.results['performance_comparison'] = {\n",
    "            strategy: {\n",
    "                'mean_objective': np.mean([t['objective_value'] for t in trials]),\n",
    "                'std_objective': np.std([t['objective_value'] for t in trials]),\n",
    "                'best_objective': min([t['objective_value'] for t in trials]),\n",
    "                'num_trials': len(trials)\n",
    "            }\n",
    "            for strategy, trials in strategy_results.items() if trials\n",
    "        }\n",
    "        \n",
    "        # Analyze model contributions\n",
    "        self._analyze_model_contributions()\n",
    "    \n",
    "    def _analyze_model_contributions(self):\n",
    "        \"\"\"Analyze individual model contributions to ensemble performance.\"\"\"\n",
    "        # Evaluate individual base models\n",
    "        individual_performance = {}\n",
    "        \n",
    "        for name, model in self.base_models.items():\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "            y_pred = model.predict(self.X_test)\n",
    "            mse = mean_squared_error(self.y_test, y_pred)\n",
    "            individual_performance[name] = mse\n",
    "        \n",
    "        self.results['model_contributions'] = individual_performance\n",
    "    \n",
    "    def visualize_results(self):\n",
    "        \"\"\"Create comprehensive visualization of optimization results.\"\"\"\n",
    "        if not self.study:\n",
    "            print(\"âŒ No optimization results to visualize. Run optimization first.\")\n",
    "            return\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "        fig.suptitle('ðŸ”‹ BatteryMind Ensemble Optimization Results', fontsize=16)\n",
    "        \n",
    "        # Plot 1: Optimization history\n",
    "        ax1 = axes[0, 0]\n",
    "        plot_optimization_history(self.study, ax=ax1)\n",
    "        ax1.set_title('Optimization History')\n",
    "        ax1.set_xlabel('Trial')\n",
    "        ax1.set_ylabel('Objective Value (MSE)')\n",
    "        \n",
    "        # Plot 2: Parameter importance\n",
    "        ax2 = axes[0, 1]\n",
    "        plot_param_importances(self.study, ax=ax2)\n",
    "        ax2.set_title('Parameter Importance')\n",
    "        \n",
    "        # Plot 3: Strategy comparison\n",
    "        ax3 = axes[0, 2]\n",
    "        strategies = list(self.results['performance_comparison'].keys())\n",
    "        objectives = [self.results['performance_comparison'][s]['mean_objective'] \n",
    "                     for s in strategies]\n",
    "        errors = [self.results['performance_comparison'][s]['std_objective'] \n",
    "                 for s in strategies]\n",
    "        \n",
    "        bars = ax3.bar(strategies, objectives, yerr=errors, capsize=5)\n",
    "        ax3.set_title('Ensemble Strategy Comparison')\n",
    "        ax3.set_ylabel('Mean Objective Value')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, obj in zip(bars, objectives):\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{obj:.4f}', ha='center', va='bottom')\n",
    "        \n",
    "                # Plot 4: Individual model contributions\n",
    "        ax4 = axes[1, 0]\n",
    "        models = list(self.results['model_contributions'].keys())\n",
    "        performances = list(self.results['model_contributions'].values())\n",
    "        \n",
    "        bars = ax4.bar(models, performances)\n",
    "        ax4.set_title('Individual Model Performance')\n",
    "        ax4.set_ylabel('Test MSE')\n",
    "        ax4.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, perf in zip(bars, performances):\n",
    "            height = bar.get_height()\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{perf:.4f}', ha='center', va='bottom')\n",
    "        \n",
    "        # Plot 5: Performance metrics heatmap\n",
    "        ax5 = axes[1, 1]\n",
    "        metrics_data = []\n",
    "        metric_names = ['mse', 'mae', 'r2', 'soh_accuracy', 'physics_compliance']\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            best_performance = self.results['best_ensemble_per_strategy'][strategy]['performance']\n",
    "            metrics_row = []\n",
    "            for metric in metric_names:\n",
    "                if metric == 'r2':\n",
    "                    # R2 should be maximized, so invert for consistency\n",
    "                    metrics_row.append(1 - best_performance[metric])\n",
    "                else:\n",
    "                    metrics_row.append(best_performance[metric])\n",
    "            metrics_data.append(metrics_row)\n",
    "        \n",
    "        im = ax5.imshow(metrics_data, cmap='RdYlBu_r', aspect='auto')\n",
    "        ax5.set_xticks(range(len(metric_names)))\n",
    "        ax5.set_xticklabels(metric_names, rotation=45, ha='right')\n",
    "        ax5.set_yticks(range(len(strategies)))\n",
    "        ax5.set_yticklabels(strategies)\n",
    "        ax5.set_title('Performance Metrics Heatmap')\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(im, ax=ax5, shrink=0.8)\n",
    "        \n",
    "        # Plot 6: Trial distribution\n",
    "        ax6 = axes[1, 2]\n",
    "        trial_counts = [self.results['performance_comparison'][strategy]['num_trials'] \n",
    "                       for strategy in strategies]\n",
    "        \n",
    "        bars = ax6.bar(strategies, trial_counts)\n",
    "        ax6.set_title('Number of Optimization Trials')\n",
    "        ax6.set_ylabel('Trial Count')\n",
    "        ax6.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, count in zip(bars, trial_counts):\n",
    "            height = bar.get_height()\n",
    "            ax6.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{count}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('ensemble_optimization_results.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_convergence_analysis(self):\n",
    "        \"\"\"Plot optimization convergence analysis.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Plot 1: Convergence curves\n",
    "        ax1 = axes[0, 0]\n",
    "        strategies = list(self.results['best_ensemble_per_strategy'].keys())\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            if 'convergence_history' in self.results['best_ensemble_per_strategy'][strategy]:\n",
    "                history = self.results['best_ensemble_per_strategy'][strategy]['convergence_history']\n",
    "                ax1.plot(history, label=strategy, marker='o', markersize=3)\n",
    "        \n",
    "        ax1.set_title('Optimization Convergence')\n",
    "        ax1.set_xlabel('Trial Number')\n",
    "        ax1.set_ylabel('Best Score')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Parameter importance\n",
    "        ax2 = axes[0, 1]\n",
    "        if 'parameter_importance' in self.results:\n",
    "            params = list(self.results['parameter_importance'].keys())\n",
    "            importance = list(self.results['parameter_importance'].values())\n",
    "            \n",
    "            bars = ax2.barh(params, importance)\n",
    "            ax2.set_title('Parameter Importance')\n",
    "            ax2.set_xlabel('Importance Score')\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, imp in zip(bars, importance):\n",
    "                width = bar.get_width()\n",
    "                ax2.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "                        f'{imp:.3f}', ha='left', va='center')\n",
    "        \n",
    "        # Plot 3: Performance stability\n",
    "        ax3 = axes[1, 0]\n",
    "        performance_std = []\n",
    "        for strategy in strategies:\n",
    "            if 'performance_std' in self.results['best_ensemble_per_strategy'][strategy]:\n",
    "                std = self.results['best_ensemble_per_strategy'][strategy]['performance_std']\n",
    "                performance_std.append(std)\n",
    "            else:\n",
    "                performance_std.append(0)\n",
    "        \n",
    "        bars = ax3.bar(strategies, performance_std)\n",
    "        ax3.set_title('Performance Stability (Lower is Better)')\n",
    "        ax3.set_ylabel('Standard Deviation')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Plot 4: Training time comparison\n",
    "        ax4 = axes[1, 1]\n",
    "        training_times = []\n",
    "        for strategy in strategies:\n",
    "            if 'training_time' in self.results['best_ensemble_per_strategy'][strategy]:\n",
    "                time = self.results['best_ensemble_per_strategy'][strategy]['training_time']\n",
    "                training_times.append(time)\n",
    "            else:\n",
    "                training_times.append(0)\n",
    "        \n",
    "        bars = ax4.bar(strategies, training_times)\n",
    "        ax4.set_title('Training Time Comparison')\n",
    "        ax4.set_ylabel('Time (seconds)')\n",
    "        ax4.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, time in zip(bars, training_times):\n",
    "            height = bar.get_height()\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{time:.1f}s', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('ensemble_convergence_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_feature_importance(self):\n",
    "        \"\"\"Plot feature importance analysis.\"\"\"\n",
    "        if 'feature_importance' not in self.results:\n",
    "            print(\"Feature importance data not available\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Plot 1: Overall feature importance\n",
    "        ax1 = axes[0, 0]\n",
    "        features = list(self.results['feature_importance'].keys())\n",
    "        importance = list(self.results['feature_importance'].values())\n",
    "        \n",
    "        # Sort by importance\n",
    "        sorted_idx = np.argsort(importance)[::-1]\n",
    "        features = [features[i] for i in sorted_idx]\n",
    "        importance = [importance[i] for i in sorted_idx]\n",
    "        \n",
    "        # Show top 20 features\n",
    "        top_features = features[:20]\n",
    "        top_importance = importance[:20]\n",
    "        \n",
    "        bars = ax1.barh(range(len(top_features)), top_importance)\n",
    "        ax1.set_yticks(range(len(top_features)))\n",
    "        ax1.set_yticklabels(top_features)\n",
    "        ax1.set_title('Top 20 Feature Importance')\n",
    "        ax1.set_xlabel('Importance Score')\n",
    "        \n",
    "        # Plot 2: Feature importance by category\n",
    "        ax2 = axes[0, 1]\n",
    "        feature_categories = {\n",
    "            'voltage': [f for f in features if 'voltage' in f.lower()],\n",
    "            'current': [f for f in features if 'current' in f.lower()],\n",
    "            'temperature': [f for f in features if 'temp' in f.lower()],\n",
    "            'soc': [f for f in features if 'soc' in f.lower()],\n",
    "            'other': [f for f in features if not any(cat in f.lower() for cat in ['voltage', 'current', 'temp', 'soc'])]\n",
    "        }\n",
    "        \n",
    "        category_importance = {}\n",
    "        for category, cat_features in feature_categories.items():\n",
    "            cat_importance = sum([self.results['feature_importance'][f] \n",
    "                                for f in cat_features if f in self.results['feature_importance']])\n",
    "            category_importance[category] = cat_importance\n",
    "        \n",
    "        categories = list(category_importance.keys())\n",
    "        cat_importance = list(category_importance.values())\n",
    "        \n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(categories)))\n",
    "        wedges, texts, autotexts = ax2.pie(cat_importance, labels=categories, colors=colors, autopct='%1.1f%%')\n",
    "        ax2.set_title('Feature Importance by Category')\n",
    "        \n",
    "        # Plot 3: Feature correlation with target\n",
    "        ax3 = axes[1, 0]\n",
    "        if 'feature_correlation' in self.results:\n",
    "            correlations = list(self.results['feature_correlation'].values())\n",
    "            ax3.hist(correlations, bins=30, alpha=0.7, edgecolor='black')\n",
    "            ax3.set_title('Distribution of Feature-Target Correlations')\n",
    "            ax3.set_xlabel('Correlation Coefficient')\n",
    "            ax3.set_ylabel('Frequency')\n",
    "            ax3.axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Plot 4: Feature stability across folds\n",
    "        ax4 = axes[1, 1]\n",
    "        if 'feature_stability' in self.results:\n",
    "            stability_scores = list(self.results['feature_stability'].values())\n",
    "            ax4.hist(stability_scores, bins=30, alpha=0.7, edgecolor='black')\n",
    "            ax4.set_title('Feature Importance Stability')\n",
    "            ax4.set_xlabel('Stability Score')\n",
    "            ax4.set_ylabel('Frequency')\n",
    "            ax4.axvline(x=np.mean(stability_scores), color='red', linestyle='--', \n",
    "                       label=f'Mean: {np.mean(stability_scores):.3f}')\n",
    "            ax4.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('ensemble_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def generate_optimization_report(self):\n",
    "        \"\"\"Generate comprehensive optimization report.\"\"\"\n",
    "        report = {\n",
    "            'optimization_summary': {\n",
    "                'total_trials': sum([self.results['performance_comparison'][strategy]['num_trials'] \n",
    "                                   for strategy in self.results['performance_comparison']]),\n",
    "                'best_strategy': self.results['best_ensemble']['strategy'],\n",
    "                'best_performance': self.results['best_ensemble']['performance'],\n",
    "                'improvement_over_baseline': self.results['improvement_over_baseline'],\n",
    "                'optimization_time': self.results.get('total_optimization_time', 0)\n",
    "            },\n",
    "            'strategy_comparison': self.results['performance_comparison'],\n",
    "            'model_contributions': self.results['model_contributions'],\n",
    "            'hyperparameter_analysis': self.results.get('hyperparameter_analysis', {}),\n",
    "            'recommendations': self._generate_recommendations()\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def _generate_recommendations(self):\n",
    "        \"\"\"Generate optimization recommendations.\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # Performance-based recommendations\n",
    "        best_strategy = self.results['best_ensemble']['strategy']\n",
    "        best_performance = self.results['best_ensemble']['performance']\n",
    "        \n",
    "        recommendations.append(f\"Best performing strategy: {best_strategy}\")\n",
    "        recommendations.append(f\"Achieved MSE: {best_performance['mse']:.4f}\")\n",
    "        \n",
    "        # Model contribution analysis\n",
    "        model_contributions = self.results['model_contributions']\n",
    "        best_model = max(model_contributions, key=model_contributions.get)\n",
    "        worst_model = min(model_contributions, key=model_contributions.get)\n",
    "        \n",
    "        recommendations.append(f\"Strongest individual model: {best_model}\")\n",
    "        recommendations.append(f\"Consider removing or improving: {worst_model}\")\n",
    "        \n",
    "        # Feature importance recommendations\n",
    "        if 'feature_importance' in self.results:\n",
    "            top_features = sorted(self.results['feature_importance'].items(), \n",
    "                                key=lambda x: x[1], reverse=True)[:5]\n",
    "            recommendations.append(f\"Top 5 features: {[f[0] for f in top_features]}\")\n",
    "        \n",
    "        # Stability recommendations\n",
    "        if 'performance_std' in self.results['best_ensemble']:\n",
    "            std = self.results['best_ensemble']['performance_std']\n",
    "            if std > 0.01:\n",
    "                recommendations.append(\"High performance variance detected - consider ensemble size increase\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# Main execution section\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting BatteryMind Ensemble Optimization\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize data and models\n",
    "    data_manager = BatteryDataManager()\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = data_manager.load_and_split_data()\n",
    "    \n",
    "    print(f\"Data loaded: {X_train.shape[0]} train, {X_val.shape[0]} val, {X_test.shape[0]} test samples\")\n",
    "    \n",
    "    # Initialize ensemble builder\n",
    "    ensemble_builder = EnsembleBuilder()\n",
    "    \n",
    "    # Create base models\n",
    "    base_models = ensemble_builder.create_base_models()\n",
    "    print(f\"Created {len(base_models)} base models\")\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = EnsembleOptimizer(base_models, X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # Define optimization strategies\n",
    "    strategies = ['voting', 'stacking', 'blending', 'dynamic_selection']\n",
    "    \n",
    "    print(\"\\nStarting optimization process...\")\n",
    "    optimization_results = {}\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        print(f\"\\nOptimizing {strategy} strategy...\")\n",
    "        \n",
    "        # Run optimization\n",
    "        best_params, best_score, trials_df = optimizer.optimize_ensemble(\n",
    "            strategy=strategy,\n",
    "            n_trials=100,\n",
    "            timeout=3600  # 1 hour timeout\n",
    "        )\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_score = optimizer.evaluate_ensemble(\n",
    "            strategy=strategy,\n",
    "            params=best_params,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test\n",
    "        )\n",
    "        \n",
    "        optimization_results[strategy] = {\n",
    "            'best_params': best_params,\n",
    "            'best_score': best_score,\n",
    "            'test_score': test_score,\n",
    "            'trials_df': trials_df\n",
    "        }\n",
    "        \n",
    "        print(f\"Strategy: {strategy}\")\n",
    "        print(f\"  Best validation score: {best_score:.4f}\")\n",
    "        print(f\"  Test score: {test_score:.4f}\")\n",
    "    \n",
    "    # Find overall best strategy\n",
    "    best_strategy = min(optimization_results.keys(), \n",
    "                       key=lambda x: optimization_results[x]['test_score'])\n",
    "    \n",
    "    print(f\"\\nBest overall strategy: {best_strategy}\")\n",
    "    print(f\"Test score: {optimization_results[best_strategy]['test_score']:.4f}\")\n",
    "    \n",
    "    # Create visualization analyzer\n",
    "    analyzer = OptimizationAnalyzer(optimization_results)\n",
    "    \n",
    "    # Generate all visualizations\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    \n",
    "    analyzer.plot_optimization_results()\n",
    "    analyzer.plot_convergence_analysis()\n",
    "    analyzer.plot_feature_importance()\n",
    "    \n",
    "    # Generate comprehensive report\n",
    "    report = analyzer.generate_optimization_report()\n",
    "    \n",
    "    # Save report\n",
    "    with open('ensemble_optimization_report.json', 'w') as f:\n",
    "        json.dump(report, f, indent=2, default=str)\n",
    "    \n",
    "    print(\"\\nOptimization complete!\")\n",
    "    print(\"Reports and visualizations saved to current directory\")\n",
    "    \n",
    "    # Display key results\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"KEY RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for rec in report['recommendations']:\n",
    "        print(f\"â€¢ {rec}\")\n",
    "    \n",
    "    print(f\"\\nTotal optimization time: {report['optimization_summary']['optimization_time']:.2f} seconds\")\n",
    "    print(f\"Total trials: {report['optimization_summary']['total_trials']}\")\n",
    "    print(f\"Best strategy: {report['optimization_summary']['best_strategy']}\")\n",
    "    print(f\"Best performance: {report['optimization_summary']['best_performance']}\")\n",
    "    \n",
    "    # Final model training and save\n",
    "    print(\"\\nTraining final optimized ensemble...\")\n",
    "    \n",
    "    final_ensemble = optimizer.create_final_ensemble(\n",
    "        strategy=best_strategy,\n",
    "        params=optimization_results[best_strategy]['best_params'],\n",
    "        X_train=np.concatenate([X_train, X_val]),\n",
    "        y_train=np.concatenate([y_train, y_val])\n",
    "    )\n",
    "    \n",
    "    # Save final ensemble\n",
    "    import joblib\n",
    "    joblib.dump(final_ensemble, 'optimized_battery_ensemble.pkl')\n",
    "    \n",
    "    print(\"Final ensemble saved as 'optimized_battery_ensemble.pkl'\")\n",
    "    print(\"\\nEnsemble optimization workflow completed successfully!\")\n",
    "    \n",
    "    # Performance validation\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"FINAL VALIDATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    final_predictions = final_ensemble.predict(X_test)\n",
    "    final_mse = mean_squared_error(y_test, final_predictions)\n",
    "    final_mae = mean_absolute_error(y_test, final_predictions)\n",
    "    final_r2 = r2_score(y_test, final_predictions)\n",
    "    \n",
    "    print(f\"Final Test MSE: {final_mse:.4f}\")\n",
    "    print(f\"Final Test MAE: {final_mae:.4f}\")\n",
    "    print(f\"Final Test RÂ²: {final_r2:.4f}\")\n",
    "    \n",
    "    # Compare with individual models\n",
    "    print(\"\\nComparison with individual models:\")\n",
    "    for model_name, model in base_models.items():\n",
    "        model_pred = model.predict(X_test)\n",
    "        model_mse = mean_squared_error(y_test, model_pred)\n",
    "        improvement = (model_mse - final_mse) / model_mse * 100\n",
    "        print(f\"{model_name}: MSE={model_mse:.4f}, Improvement={improvement:.1f}%\")\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ BatteryMind Ensemble Optimization Complete! ðŸŽ‰\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
