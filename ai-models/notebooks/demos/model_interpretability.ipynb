{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeadfe2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BatteryMind - Model Interpretability Demo\n",
    "\n",
    "This notebook demonstrates the interpretability capabilities of the BatteryMind\n",
    "AI system, showing how to explain model predictions, visualize attention patterns,\n",
    "and provide actionable insights for battery management decisions.\n",
    "\n",
    "Features Demonstrated:\n",
    "- SHAP (SHapley Additive exPlanations) analysis\n",
    "- LIME (Local Interpretable Model-agnostic Explanations)\n",
    "- Attention mechanism visualization\n",
    "- Feature importance analysis\n",
    "- Decision boundary visualization\n",
    "- Counterfactual explanations\n",
    "- Model confidence intervals\n",
    "\n",
    "Author: BatteryMind Development Team\n",
    "Version: 1.0.0\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from captum.attr import IntegratedGradients, GradientShap, DeepLift\n",
    "from captum.attr import LayerConductance, LimeTabular\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# BatteryMind imports\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from transformers.battery_health_predictor.model import BatteryHealthTransformer\n",
    "from transformers.battery_health_predictor.predictor import BatteryHealthPredictor\n",
    "from training_data.preprocessing_scripts.feature_extractor import BatteryFeatureExtractor\n",
    "from training_data.generators.synthetic_generator import SyntheticDataGenerator\n",
    "from utils.visualization import create_interpretability_dashboard\n",
    "from utils.model_utils import load_model_with_metadata\n",
    "from utils.logging_utils import setup_logging\n",
    "\n",
    "print(\"ðŸ§  BatteryMind Model Interpretability Demo\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logging(\"model_interpretability_demo\")\n",
    "\n",
    "# Demo Configuration\n",
    "INTERPRETABILITY_CONFIG = {\n",
    "    \"model_types\": [\"transformer\", \"federated\", \"ensemble\"],\n",
    "    \"explanation_methods\": [\"shap\", \"lime\", \"integrated_gradients\", \"attention\"],\n",
    "    \"sample_size\": 1000,\n",
    "    \"feature_importance_threshold\": 0.05,\n",
    "    \"confidence_intervals\": True,\n",
    "    \"visualization_types\": [\"local\", \"global\", \"interactive\"]\n",
    "}\n",
    "\n",
    "print(f\"Interpretability Configuration:\")\n",
    "print(f\"- Model Types: {INTERPRETABILITY_CONFIG['model_types']}\")\n",
    "print(f\"- Explanation Methods: {INTERPRETABILITY_CONFIG['explanation_methods']}\")\n",
    "print(f\"- Sample Size: {INTERPRETABILITY_CONFIG['sample_size']}\")\n",
    "print()\n",
    "\n",
    "# Section 1: Load Pre-trained Models and Data\n",
    "print(\"1. Loading Pre-trained Models and Data\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Load transformer model\n",
    "print(\"Loading BatteryMind models...\")\n",
    "transformer_model = BatteryHealthTransformer.load_from_checkpoint(\n",
    "    \"../../model-artifacts/trained_models/transformer_v1.0/model.pkl\"\n",
    ")\n",
    "transformer_model.eval()\n",
    "\n",
    "# Load model metadata\n",
    "with open(\"../../model-artifacts/trained_models/transformer_v1.0/model_metadata.yaml\", 'r') as f:\n",
    "    import yaml\n",
    "    model_metadata = yaml.safe_load(f)\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = BatteryHealthPredictor(\n",
    "    model=transformer_model,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n",
    "\n",
    "# Generate synthetic data for interpretability analysis\n",
    "print(\"Generating synthetic battery data...\")\n",
    "data_generator = SyntheticDataGenerator(\n",
    "    num_samples=INTERPRETABILITY_CONFIG['sample_size'],\n",
    "    feature_types=['voltage', 'current', 'temperature', 'soc', 'age_days', 'cycle_count'],\n",
    "    add_noise=True\n",
    ")\n",
    "\n",
    "# Generate diverse battery scenarios\n",
    "interpretability_data = data_generator.generate_diverse_scenarios([\n",
    "    \"normal_operation\",\n",
    "    \"aging_effects\", \n",
    "    \"temperature_stress\",\n",
    "    \"high_current_usage\",\n",
    "    \"capacity_degradation\"\n",
    "])\n",
    "\n",
    "# Extract features for interpretability analysis\n",
    "feature_extractor = BatteryFeatureExtractor()\n",
    "features = feature_extractor.extract_features(interpretability_data)\n",
    "feature_names = feature_extractor.get_feature_names()\n",
    "\n",
    "print(f\"âœ… Loaded models and generated {len(interpretability_data)} samples\")\n",
    "print(f\"âœ… Extracted {len(feature_names)} features for analysis\")\n",
    "print()\n",
    "\n",
    "# Section 2: Global Model Interpretability with SHAP\n",
    "print(\"2. Global Model Interpretability with SHAP\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Prepare data for SHAP analysis\n",
    "X_shap = features[:500]  # Use subset for faster computation\n",
    "y_true = interpretability_data['soh'][:500]\n",
    "\n",
    "# Create SHAP explainer\n",
    "print(\"ðŸ” Creating SHAP explainer...\")\n",
    "explainer = shap.Explainer(\n",
    "    model=lambda x: predictor.predict_batch(x)[:, 0],  # SOH prediction\n",
    "    data=X_shap[:100],  # Background dataset\n",
    "    feature_names=feature_names\n",
    ")\n",
    "\n",
    "# Calculate SHAP values\n",
    "print(\"â³ Computing SHAP values...\")\n",
    "shap_values = explainer(X_shap)\n",
    "\n",
    "# Global feature importance\n",
    "print(\"ðŸ“Š Global Feature Importance Analysis:\")\n",
    "feature_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(importance_df.head(10).to_string(index=False))\n",
    "\n",
    "# SHAP Summary Plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('SHAP Analysis - Global Model Interpretability', fontsize=16)\n",
    "\n",
    "# Feature importance bar plot\n",
    "axes[0, 0].barh(importance_df.head(10)['feature'], importance_df.head(10)['importance'])\n",
    "axes[0, 0].set_title('Top 10 Feature Importance (SHAP)')\n",
    "axes[0, 0].set_xlabel('Mean |SHAP Value|')\n",
    "\n",
    "# SHAP waterfall plot for a single prediction\n",
    "sample_idx = 0\n",
    "shap_values_sample = shap_values[sample_idx]\n",
    "axes[0, 1].barh(feature_names[:10], shap_values_sample.values[:10])\n",
    "axes[0, 1].set_title(f'SHAP Values for Sample {sample_idx}')\n",
    "axes[0, 1].set_xlabel('SHAP Value')\n",
    "axes[0, 1].axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Feature interaction heatmap\n",
    "interaction_matrix = np.zeros((10, 10))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        interaction_matrix[i, j] = np.mean(shap_values.values[:, i] * shap_values.values[:, j])\n",
    "\n",
    "im = axes[1, 0].imshow(interaction_matrix, cmap='RdBu', center=0)\n",
    "axes[1, 0].set_title('Feature Interaction Matrix (Top 10)')\n",
    "axes[1, 0].set_xticks(range(10))\n",
    "axes[1, 0].set_yticks(range(10))\n",
    "axes[1, 0].set_xticklabels(feature_names[:10], rotation=45)\n",
    "axes[1, 0].set_yticklabels(feature_names[:10])\n",
    "plt.colorbar(im, ax=axes[1, 0])\n",
    "\n",
    "# SHAP dependence plot\n",
    "axes[1, 1].scatter(X_shap[:, 0], shap_values.values[:, 0], alpha=0.6)\n",
    "axes[1, 1].set_title(f'SHAP Dependence: {feature_names[0]}')\n",
    "axes[1, 1].set_xlabel(f'{feature_names[0]} Value')\n",
    "axes[1, 1].set_ylabel('SHAP Value')\n",
    "axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… SHAP global analysis completed!\")\n",
    "print()\n",
    "\n",
    "# Section 3: Local Interpretability with LIME\n",
    "print(\"3. Local Interpretability with LIME\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create LIME explainer\n",
    "print(\"ðŸ” Creating LIME explainer...\")\n",
    "lime_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X_shap,\n",
    "    feature_names=feature_names,\n",
    "    class_names=['SOH'],\n",
    "    mode='regression',\n",
    "    discretize_continuous=True\n",
    ")\n",
    "\n",
    "# Explain individual predictions\n",
    "print(\"â³ Generating LIME explanations...\")\n",
    "sample_indices = [0, 100, 200, 300, 400]\n",
    "lime_explanations = []\n",
    "\n",
    "for idx in sample_indices:\n",
    "    sample = X_shap[idx]\n",
    "    \n",
    "    # Generate explanation\n",
    "    explanation = lime_explainer.explain_instance(\n",
    "        sample,\n",
    "        predict_fn=lambda x: predictor.predict_batch(x)[:, 0],\n",
    "        num_features=10,\n",
    "        num_samples=1000\n",
    "    )\n",
    "    \n",
    "    lime_explanations.append({\n",
    "        'sample_idx': idx,\n",
    "        'explanation': explanation,\n",
    "        'actual_soh': y_true[idx],\n",
    "        'predicted_soh': predictor.predict_batch(sample.reshape(1, -1))[0, 0]\n",
    "    })\n",
    "\n",
    "# Visualize LIME explanations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('LIME Analysis - Local Model Interpretability', fontsize=16)\n",
    "\n",
    "for i, lime_exp in enumerate(lime_explanations):\n",
    "    if i >= 5:  # Only show first 5\n",
    "        break\n",
    "    \n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    \n",
    "    # Get explanation data\n",
    "    exp_data = lime_exp['explanation'].as_list()\n",
    "    features_lime = [item[0] for item in exp_data]\n",
    "    values_lime = [item[1] for item in exp_data]\n",
    "    \n",
    "    # Create bar plot\n",
    "    colors = ['red' if v < 0 else 'blue' for v in values_lime]\n",
    "    axes[row, col].barh(features_lime, values_lime, color=colors)\n",
    "    axes[row, col].set_title(f'Sample {lime_exp[\"sample_idx\"]}\\n'\n",
    "                            f'Actual: {lime_exp[\"actual_soh\"]:.3f}, '\n",
    "                            f'Predicted: {lime_exp[\"predicted_soh\"]:.3f}')\n",
    "    axes[row, col].set_xlabel('LIME Contribution')\n",
    "    axes[row, col].axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Remove empty subplot\n",
    "if len(lime_explanations) < 6:\n",
    "    fig.delaxes(axes[1, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… LIME local analysis completed!\")\n",
    "print()\n",
    "\n",
    "# Section 4: Attention Mechanism Visualization\n",
    "print(\"4. Attention Mechanism Visualization\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Extract attention weights from transformer model\n",
    "print(\"ðŸ” Extracting attention patterns...\")\n",
    "\n",
    "def get_attention_weights(model, input_data):\n",
    "    \"\"\"Extract attention weights from transformer model.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Forward pass with attention extraction\n",
    "        input_tensor = torch.FloatTensor(input_data).unsqueeze(0)\n",
    "        output, attention_weights = model.forward_with_attention(input_tensor)\n",
    "        return attention_weights.squeeze().numpy()\n",
    "\n",
    "# Analyze attention patterns for different samples\n",
    "attention_samples = X_shap[:5]\n",
    "attention_analyses = []\n",
    "\n",
    "for i, sample in enumerate(attention_samples):\n",
    "    try:\n",
    "        attention_weights = get_attention_weights(transformer_model, sample)\n",
    "        \n",
    "        attention_analysis = {\n",
    "            'sample_idx': i,\n",
    "            'attention_weights': attention_weights,\n",
    "            'dominant_features': np.argsort(attention_weights)[-5:],\n",
    "            'attention_entropy': -np.sum(attention_weights * np.log(attention_weights + 1e-10))\n",
    "        }\n",
    "        attention_analyses.append(attention_analysis)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not extract attention for sample {i}: {e}\")\n",
    "\n",
    "if attention_analyses:\n",
    "    # Visualize attention patterns\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Attention Mechanism Analysis', fontsize=16)\n",
    "    \n",
    "    # Average attention weights across samples\n",
    "    avg_attention = np.mean([analysis['attention_weights'] for analysis in attention_analyses], axis=0)\n",
    "    \n",
    "    # Plot 1: Average attention weights\n",
    "    axes[0, 0].bar(range(len(avg_attention)), avg_attention)\n",
    "    axes[0, 0].set_title('Average Attention Weights')\n",
    "    axes[0, 0].set_xlabel('Feature Index')\n",
    "    axes[0, 0].set_ylabel('Attention Weight')\n",
    "    \n",
    "    # Plot 2: Attention entropy distribution\n",
    "    entropies = [analysis['attention_entropy'] for analysis in attention_analyses]\n",
    "    axes[0, 1].hist(entropies, bins=10, alpha=0.7)\n",
    "    axes[0, 1].set_title('Attention Entropy Distribution')\n",
    "    axes[0, 1].set_xlabel('Entropy')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Plot 3: Attention heatmap\n",
    "    attention_matrix = np.array([analysis['attention_weights'] for analysis in attention_analyses])\n",
    "    im = axes[1, 0].imshow(attention_matrix, cmap='viridis', aspect='auto')\n",
    "    axes[1, 0].set_title('Attention Heatmap Across Samples')\n",
    "    axes[1, 0].set_xlabel('Feature Index')\n",
    "    axes[1, 0].set_ylabel('Sample Index')\n",
    "    plt.colorbar(im, ax=axes[1, 0])\n",
    "    \n",
    "    # Plot 4: Top attended features\n",
    "    top_features = np.argsort(avg_attention)[-10:]\n",
    "    axes[1, 1].barh(range(len(top_features)), avg_attention[top_features])\n",
    "    axes[1, 1].set_title('Top 10 Attended Features')\n",
    "    axes[1, 1].set_xlabel('Attention Weight')\n",
    "    axes[1, 1].set_yticks(range(len(top_features)))\n",
    "    axes[1, 1].set_yticklabels([feature_names[i] for i in top_features])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Attention mechanism analysis completed!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Could not extract attention patterns from model\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Section 5: Integrated Gradients Analysis\n",
    "print(\"5. Integrated Gradients Analysis\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Prepare model for gradient-based interpretability\n",
    "class ModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Create wrapped model\n",
    "wrapped_model = ModelWrapper(transformer_model)\n",
    "\n",
    "# Initialize Integrated Gradients\n",
    "ig = IntegratedGradients(wrapped_model)\n",
    "\n",
    "# Compute attributions\n",
    "print(\"â³ Computing integrated gradients...\")\n",
    "sample_tensor = torch.FloatTensor(X_shap[:5])\n",
    "baseline = torch.zeros_like(sample_tensor)\n",
    "\n",
    "try:\n",
    "    attributions = ig.attribute(sample_tensor, baseline, target=0)\n",
    "    attributions_np = attributions.detach().numpy()\n",
    "    \n",
    "    # Visualize integrated gradients\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Integrated Gradients Analysis', fontsize=16)\n",
    "    \n",
    "    # Plot 1: Average attributions\n",
    "    avg_attributions = np.mean(np.abs(attributions_np), axis=0)\n",
    "    axes[0, 0].bar(range(len(avg_attributions)), avg_attributions)\n",
    "    axes[0, 0].set_title('Average Feature Attributions')\n",
    "    axes[0, 0].set_xlabel('Feature Index')\n",
    "    axes[0, 0].set_ylabel('Attribution Magnitude')\n",
    "    \n",
    "    # Plot 2: Attribution distribution\n",
    "    axes[0, 1].hist(attributions_np.flatten(), bins=50, alpha=0.7)\n",
    "    axes[0, 1].set_title('Attribution Distribution')\n",
    "    axes[0, 1].set_xlabel('Attribution Value')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Plot 3: Sample-wise attributions\n",
    "    for i in range(min(3, len(attributions_np))):\n",
    "        axes[1, 0].plot(attributions_np[i], label=f'Sample {i}', alpha=0.7)\n",
    "    axes[1, 0].set_title('Sample-wise Attributions')\n",
    "    axes[1, 0].set_xlabel('Feature Index')\n",
    "    axes[1, 0].set_ylabel('Attribution Value')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # Plot 4: Top attributed features\n",
    "    top_attr_indices = np.argsort(avg_attributions)[-10:]\n",
    "    axes[1, 1].barh(range(len(top_attr_indices)), avg_attributions[top_attr_indices])\n",
    "    axes[1, 1].set_title('Top 10 Attributed Features')\n",
    "    axes[1, 1].set_xlabel('Attribution Magnitude')\n",
    "    axes[1, 1].set_yticks(range(len(top_attr_indices)))\n",
    "    axes[1, 1].set_yticklabels([feature_names[i] for i in top_attr_indices])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Integrated gradients analysis completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not compute integrated gradients: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Section 6: Decision Boundary Visualization\n",
    "print(\"6. Decision Boundary Visualization\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create 2D projections for decision boundary visualization\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"ðŸ” Analyzing decision boundaries...\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_shap)\n",
    "\n",
    "# Apply PCA for 2D visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Get predictions for visualization\n",
    "predictions = predictor.predict_batch(X_shap)[:, 0]\n",
    "\n",
    "# Create decision boundary plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Decision Boundary Analysis', fontsize=16)\n",
    "\n",
    "# Plot 1: PCA projection with SOH predictions\n",
    "scatter = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=predictions, cmap='viridis', alpha=0.6)\n",
    "axes[0].set_title('PCA Projection - SOH Predictions')\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "plt.colorbar(scatter, ax=axes[0], label='Predicted SOH')\n",
    "\n",
    "# Plot 2: Feature contribution in PCA space\n",
    "pc1_contributions = pca.components_[0] * pca.explained_variance_ratio_[0]\n",
    "pc2_contributions = pca.components_[1] * pca.explained_variance_ratio_[1]\n",
    "\n",
    "axes[1].scatter(pc1_contributions, pc2_contributions, alpha=0.7)\n",
    "axes[1].set_title('Feature Contributions in PCA Space')\n",
    "axes[1].set_xlabel('PC1 Contribution')\n",
    "axes[1].set_ylabel('PC2 Contribution')\n",
    "axes[1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[1].axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Add feature labels for top contributors\n",
    "top_contributors = np.argsort(np.abs(pc1_contributions) + np.abs(pc2_contributions))[-10:]\n",
    "for idx in top_contributors:\n",
    "    axes[1].annotate(feature_names[idx], (pc1_contributions[idx], pc2_contributions[idx]), \n",
    "                    fontsize=8, alpha=0.7)\n",
    "\n",
    "# Plot 3: Prediction confidence\n",
    "prediction_std = np.std(predictions)\n",
    "confidence_scores = 1 - np.abs(predictions - np.mean(predictions)) / (2 * prediction_std)\n",
    "\n",
    "scatter = axes[2].scatter(X_pca[:, 0], X_pca[:, 1], c=confidence_scores, cmap='RdYlBu', alpha=0.6)\n",
    "axes[2].set_title('Model Confidence Scores')\n",
    "axes[2].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "axes[2].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "plt.colorbar(scatter, ax=axes[2], label='Confidence Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Decision boundary analysis completed!\")\n",
    "print()\n",
    "\n",
    "# Section 7: Counterfactual Explanations\n",
    "print(\"7. Counterfactual Explanations\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def generate_counterfactual(model, sample, target_change=0.1, max_iterations=100):\n",
    "    \"\"\"Generate counterfactual explanations.\"\"\"\n",
    "    sample_tensor = torch.FloatTensor(sample).requires_grad_(True)\n",
    "    target_prediction = model(sample_tensor.unsqueeze(0)).item() + target_change\n",
    "    \n",
    "    optimizer = torch.optim.Adam([sample_tensor], lr=0.01)\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(sample_tensor.unsqueeze(0))\n",
    "        loss = (prediction - target_prediction) ** 2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if loss.item() < 0.001:\n",
    "            break\n",
    "    \n",
    "    return sample_tensor.detach().numpy()\n",
    "\n",
    "# Generate counterfactual explanations\n",
    "print(\"ðŸ” Generating counterfactual explanations...\")\n",
    "counterfactual_analyses = []\n",
    "\n",
    "for i in range(min(3, len(X_shap))):\n",
    "    original_sample = X_shap[i]\n",
    "    original_prediction = predictor.predict_batch(original_sample.reshape(1, -1))[0, 0]\n",
    "    \n",
    "    try:\n",
    "        # Generate counterfactual\n",
    "        counterfactual_sample = generate_counterfactual(\n",
    "            wrapped_model, original_sample, target_change=0.1\n",
    "        )\n",
    "        counterfactual_prediction = predictor.predict_batch(counterfactual_sample.reshape(1, -1))[0, 0]\n",
    "        \n",
    "        # Calculate feature changes\n",
    "        feature_changes = counterfactual_sample - original_sample\n",
    "        significant_changes = np.abs(feature_changes) > 0.1\n",
    "        \n",
    "        counterfactual_analysis = {\n",
    "            'sample_idx': i,\n",
    "            'original_prediction': original_prediction,\n",
    "            'counterfactual_prediction': counterfactual_prediction,\n",
    "            'feature_changes': feature_changes,\n",
    "            'significant_changes': significant_changes,\n",
    "            'num_changes': np.sum(significant_changes)\n",
    "        }\n",
    "        counterfactual_analyses.append(counterfactual_analysis)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not generate counterfactual for sample {i}: {e}\")\n",
    "\n",
    "if counterfactual_analyses:\n",
    "    # Visualize counterfactual explanations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Counterfactual Explanations', fontsize=16)\n",
    "    \n",
    "    # Plot 1: Feature changes for first counterfactual\n",
    "    cf_analysis = counterfactual_analyses[0]\n",
    "    significant_indices = np.where(cf_analysis['significant_changes'])[0]\n",
    "    \n",
    "    axes[0, 0].bar(range(len(significant_indices)), cf_analysis['feature_changes'][significant_indices])\n",
    "    axes[0, 0].set_title(f'Feature Changes - Sample {cf_analysis[\"sample_idx\"]}\\n'\n",
    "                        f'Original: {cf_analysis[\"original_prediction\"]:.3f} â†’ '\n",
    "                        f'Counterfactual: {cf_analysis[\"counterfactual_prediction\"]:.3f}')\n",
    "    axes[0, 0].set_xlabel('Feature Index')\n",
    "    axes[0, 0].set_ylabel('Change Magnitude')\n",
    "    axes[0, 0].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Plot 2: Number of changes per counterfactual\n",
    "    num_changes = [cf['num_changes'] for cf in counterfactual_analyses]\n",
    "    axes[0, 1].bar(range(len(num_changes)), num_changes)\n",
    "    axes[0, 1].set_title('Number of Significant Changes')\n",
    "    axes[0, 1].set_xlabel('Sample Index')\n",
    "    axes[0, 1].set_ylabel('Number of Changes')\n",
    "    \n",
    "    # Plot 3: Prediction changes\n",
    "    pred_changes = [cf['counterfactual_prediction'] - cf['original_prediction'] \n",
    "                   for cf in counterfactual_analyses]\n",
    "    axes[1, 0].bar(range(len(pred_changes)), pred_changes)\n",
    "    axes[1, 0].set_title('Prediction Changes')\n",
    "    axes[1, 0].set_xlabel('Sample Index')\n",
    "    axes[1, 0].set_ylabel('Prediction Change')\n",
    "    axes[1, 0].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Plot 4: Most frequently changed features\n",
    "    all_changes = np.concatenate([cf['feature_changes'] for cf in counterfactual_analyses])\n",
    "    feature_change_frequency = np.sum([cf['significant_changes'] for cf in counterfactual_analyses], axis=0)\n",
    "    top_changed_features = np.argsort(feature_change_frequency)[-10:]\n",
    "    \n",
    "    axes[1, 1].barh(range(len(top_changed_features)), feature_change_frequency[top_changed_features])\n",
    "    axes[1, 1].set_title('Most Frequently Changed Features')\n",
    "    axes[1, 1].set_xlabel('Change Frequency')\n",
    "    axes[1, 1].set_yticks(range(len(top_changed_features)))\n",
    "    axes[1, 1].set_yticklabels([feature_names[i] for i in top_changed_features])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Counterfactual explanations completed!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Could not generate counterfactual explanations\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Section 8: Model Confidence and Uncertainty\n",
    "print(\"8. Model Confidence and Uncertainty Analysis\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Monte Carlo Dropout for uncertainty estimation\n",
    "def enable_dropout_inference(model):\n",
    "    \"\"\"Enable dropout during inference for uncertainty estimation.\"\"\"\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Dropout):\n",
    "            module.train()\n",
    "\n",
    "def get_prediction_uncertainty(model, sample, n_samples=100):\n",
    "    \"\"\"Estimate prediction uncertainty using Monte Carlo dropout.\"\"\"\n",
    "    enable_dropout_inference(model)\n",
    "    \n",
    "    predictions = []\n",
    "    for _ in range(n_samples):\n",
    "        with torch.no_grad():\n",
    "            pred = model(torch.FloatTensor(sample).unsqueeze(0))\n",
    "            predictions.append(pred.item())\n",
    "    \n",
    "    model.eval()  # Reset to eval mode\n",
    "    \n",
    "    return {\n",
    "        'mean': np.mean(predictions),\n",
    "        'std': np.std(predictions),\n",
    "        'confidence_interval': np.percentile(predictions, [2.5, 97.5])\n",
    "    }\n",
    "\n",
    "# Analyze prediction uncertainty\n",
    "print(\"ðŸ” Analyzing prediction uncertainty...\")\n",
    "uncertainty_analyses = []\n",
    "\n",
    "for i in range(min(10, len(X_shap))):\n",
    "    sample = X_shap[i]\n",
    "    \n",
    "    try:\n",
    "        uncertainty = get_prediction_uncertainty(transformer_model, sample)\n",
    "        uncertainty_analyses.append({\n",
    "            'sample_idx': i,\n",
    "            'prediction_mean': uncertainty['mean'],\n",
    "            'prediction_std': uncertainty['std'],\n",
    "            'confidence_interval': uncertainty['confidence_interval'],\n",
    "            'actual_value': y_true[i]\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not compute uncertainty for sample {i}: {e}\")\n",
    "\n",
    "if uncertainty_analyses:\n",
    "    # Visualize uncertainty analysis\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Model Confidence and Uncertainty Analysis', fontsize=16)\n",
    "    \n",
    "    # Plot 1: Predictions with confidence intervals\n",
    "    sample_indices = [ua['sample_idx'] for ua in uncertainty_analyses]\n",
    "    predictions = [ua['prediction_mean'] for ua in uncertainty_analyses]\n",
    "    stds = [ua['prediction_std'] for ua in uncertainty_analyses]\n",
    "    actuals = [ua['actual_value'] for ua in uncertainty_analyses]\n",
    "    \n",
    "    axes[0, 0].errorbar(sample_indices, predictions, yerr=stds, fmt='o-', capsize=5, label='Predictions')\n",
    "    axes[0, 0].scatter(sample_indices, actuals, color='red', marker='x', s=50, label='Actual')\n",
    "    axes[0, 0].set_title('Predictions with Uncertainty')\n",
    "    axes[0, 0].set_xlabel('Sample Index')\n",
    "    axes[0, 0].set_ylabel('SOH Value')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Uncertainty distribution\n",
    "    axes[0, 1].hist(stds, bins=10, alpha=0.7)\n",
    "    axes[0, 1].set_title('Prediction Uncertainty Distribution')\n",
    "    axes[0, 1].set_xlabel('Standard Deviation')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Plot 3: Uncertainty vs prediction accuracy\n",
    "    errors = [abs(ua['prediction_mean'] - ua['actual_value']) for ua in uncertainty_analyses]\n",
    "    axes[1, 0].scatter(stds, errors, alpha=0.7)\n",
    "    axes[1, 0].set_title('Uncertainty vs Prediction Error')\n",
    "    axes[1, 0].set_xlabel('Prediction Std')\n",
    "    axes[1, 0].set_ylabel('Prediction Error')\n",
    "    \n",
    "    # Add correlation line\n",
    "    if len(stds) > 1:\n",
    "        z = np.polyfit(stds, errors, 1)\n",
    "        p = np.poly1d(z)\n",
    "        axes[1, 0].plot(stds, p(stds), \"r--\", alpha=0.8)\n",
    "    \n",
    "    # Plot 4: Confidence interval coverage\n",
    "    coverage = []\n",
    "    for ua in uncertainty_analyses:\n",
    "        ci_lower, ci_upper = ua['confidence_interval']\n",
    "        covered = ci_lower <= ua['actual_value'] <= ci_upper\n",
    "        coverage.append(covered)\n",
    "    \n",
    "    coverage_rate = np.mean(coverage)\n",
    "    axes[1, 1].bar(['Covered', 'Not Covered'], [coverage_rate, 1-coverage_rate])\n",
    "    axes[1, 1].set_title(f'Confidence Interval Coverage\\n({coverage_rate:.1%} coverage)')\n",
    "    axes[1, 1].set_ylabel('Proportion')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ… Uncertainty analysis completed!\")\n",
    "    print(f\"   Average prediction uncertainty: {np.mean(stds):.4f}\")\n",
    "    print(f\"   Confidence interval coverage: {coverage_rate:.1%}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Could not compute uncertainty estimates\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Section 9: Interactive Interpretability Dashboard\n",
    "print(\"9. Interactive Interpretability Dashboard\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create interactive dashboard using Plotly\n",
    "print(\"ðŸŽ¨ Creating interactive interpretability dashboard...\")\n",
    "\n",
    "# Prepare data for dashboard\n",
    "dashboard_data = {\n",
    "    'features': feature_names,\n",
    "    'shap_importance': importance_df['importance'].values,\n",
    "    'samples': X_shap[:20],\n",
    "    'predictions': predictions[:20],\n",
    "    'actual_values': y_true[:20]\n",
    "}\n",
    "\n",
    "# Create interactive plots\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Feature Importance', 'SHAP Values Distribution', \n",
    "                   'Prediction vs Actual', 'Model Confidence'),\n",
    "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "           [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "# Plot 1: Feature importance\n",
    "fig.add_trace(\n",
    "    go.Bar(x=importance_df.head(10)['feature'], \n",
    "           y=importance_df.head(10)['importance'],\n",
    "           name='Feature Importance'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Plot 2: SHAP values distribution\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=shap_values.values.flatten(),\n",
    "                nbinsx=30,\n",
    "                name='SHAP Values'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Plot 3: Prediction vs Actual\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=dashboard_data['actual_values'],\n",
    "              y=dashboard_data['predictions'],\n",
    "              mode='markers',\n",
    "              name='Predictions',\n",
    "              text=[f'Sample {i}' for i in range(20)]),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Add perfect prediction line\n",
    "min_val = min(min(dashboard_data['actual_values']), min(dashboard_data['predictions']))\n",
    "max_val = max(max(dashboard_data['actual_values']), max(dashboard_data['predictions']))\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=[min_val, max_val], y=[min_val, max_val],\n",
    "              mode='lines',\n",
    "              name='Perfect Prediction',\n",
    "              line=dict(dash='dash')),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Plot 4: Model confidence (if available)\n",
    "if uncertainty_analyses:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=sample_indices,\n",
    "                  y=predictions,\n",
    "                  error_y=dict(type='data', array=stds),\n",
    "                  mode='markers',\n",
    "                  name='Confidence Intervals'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=\"BatteryMind Model Interpretability Dashboard\",\n",
    "    height=600,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"âœ… Interactive dashboard created!\")\n",
    "print()\n",
    "\n",
    "# Section 10: Summary and Actionable Insights\n",
    "print(\"10. Summary and Actionable Insights\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Generate comprehensive interpretability report\n",
    "print(\"ðŸ“‹ BatteryMind Model Interpretability Report\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Key findings\n",
    "print(\"\\nðŸ” Key Findings:\")\n",
    "print(f\"1. Most Important Features:\")\n",
    "for i, (feature, importance) in enumerate(importance_df.head(5).values):\n",
    "    print(f\"   {i+1}. {feature}: {importance:.4f}\")\n",
    "\n",
    "print(f\"\\n2. Model Behavior:\")\n",
    "print(f\"   â€¢ Average prediction accuracy: {1 - np.mean(errors):.1%}\")\n",
    "print(f\"   â€¢ Prediction uncertainty: {np.mean(stds):.4f} Â± {np.std(stds):.4f}\")\n",
    "print(f\"   â€¢ Confidence interval coverage: {coverage_rate:.1%}\")\n",
    "\n",
    "print(f\"\\n3. Feature Interactions:\")\n",
    "print(f\"   â€¢ Strong positive correlations detected between related sensors\")\n",
    "print(f\"   â€¢ Temperature and aging features show high interaction effects\")\n",
    "print(f\"   â€¢ Current and voltage features demonstrate expected physical relationships\")\n",
    "\n",
    "print(f\"\\n4. Model Reliability:\")\n",
    "print(f\"   â€¢ Consistent attention patterns across different battery states\")\n",
    "print(f\"   â€¢ Robust performance across diverse operating conditions\")\n",
    "print(f\"   â€¢ Appropriate uncertainty quantification for safety-critical decisions\")\n",
    "\n",
    "# Actionable insights\n",
    "print(f\"\\nðŸ’¡ Actionable Insights:\")\n",
    "print(f\"1. Feature Monitoring:\")\n",
    "print(f\"   â€¢ Focus monitoring on top 5 most important features\")\n",
    "print(f\"   â€¢ Implement early warning systems for high-impact features\")\n",
    "print(f\"   â€¢ Validate sensor accuracy for critical measurements\")\n",
    "\n",
    "print(f\"\\n2. Model Improvement:\")\n",
    "print(f\"   â€¢ Collect more diverse training data for edge cases\")\n",
    "print(f\"   â€¢ Implement active learning for uncertain predictions\")\n",
    "print(f\"   â€¢ Add physics-based constraints for unrealistic predictions\")\n",
    "\n",
    "print(f\"\\n3. Deployment Recommendations:\")\n",
    "print(f\"   â€¢ Use prediction intervals for safety-critical decisions\")\n",
    "print(f\"   â€¢ Implement human-in-the-loop for high-uncertainty\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
