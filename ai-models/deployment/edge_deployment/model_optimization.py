"""
BatteryMind Model Optimization for Edge Deployment

Advanced model optimization techniques for deploying AI models on resource-constrained
edge devices. This module provides comprehensive optimization including quantization,
pruning, knowledge distillation, and model compression techniques.

Key Features:
- Multi-precision quantization (FP32â†’FP16/INT8/INT4)
- Structured and unstructured neural network pruning
- Knowledge distillation for model compression
- ONNX model optimization and graph fusion
- Hardware-specific optimizations
- Performance profiling and validation

Author: BatteryMind Development Team
Version: 1.0.0
"""

import logging
import time
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Any, Union, Tuple, Callable
from dataclasses import dataclass
from enum import Enum
import json
import tempfile
import shutil

# Core ML libraries
import numpy as np
import torch
import torch.nn as nn
import torch.quantization as torch_quant
import onnx
import onnxruntime as ort
from onnx import optimizer
import onnxsim

# Model compression libraries
try:
    import torch_pruning as tp
except ImportError:
    tp = None
    warnings.warn("torch_pruning not available, pruning features will be limited")

try:
    from pytorch_quantization import nn as quant_nn
    from pytorch_quantization.tensor_quant import QuantDescriptor
except ImportError:
    quant_nn = None
    warnings.warn("pytorch_quantization not available, advanced quantization features disabled")

# Configure logging
logger = logging.getLogger(__name__)

class OptimizationStrategy(Enum):
    """Optimization strategy types."""
    QUANTIZATION_ONLY = "quantization_only"
    PRUNING_ONLY = "pruning_only"
    COMBINED = "combined"
    KNOWLEDGE_DISTILLATION = "knowledge_distillation"
    PROGRESSIVE = "progressive"

class CompressionTechnique(Enum):
    """Model compression techniques."""
    MAGNITUDE_PRUNING = "magnitude_pruning"
    STRUCTURED_PRUNING = "structured_pruning"
    DYNAMIC_QUANTIZATION = "dynamic_quantization"
    STATIC_QUANTIZATION = "static_quantization"
    QAT = "quantization_aware_training"

@dataclass
class OptimizationMetrics:
    """Metrics for optimization results."""
    
    original_size_mb: float = 0.0
    optimized_size_mb: float = 0.0
    compression_ratio: float = 0.0
    original_latency_ms: float = 0.0
    optimized_latency_ms: float = 0.0
    speedup_ratio: float = 0.0
    accuracy_retention: float = 0.0
    memory_reduction_mb: float = 0.0
    power_reduction_percent: float = 0.0

class ModelOptimizer:
    """
    Advanced model optimizer for edge deployment.
    """
    
    def __init__(self, config):
        """
        Initialize model optimizer.
        
        Args:
            config: EdgeDeploymentConfig instance
        """
        self.config = config
        self.device_profile = config.device_profile
        self.opt_profile = config.optimization_profile
        
        # Optimization state
        self.optimization_history = []
        self.current_metrics = OptimizationMetrics()
        
        # Supported formats
        self.supported_formats = ['.pth', '.pt', '.onnx', '.pb']
        
        logger.info("ModelOptimizer initialized")
        logger.info(f"Target device: {self.device_profile.device_type.value}")
        logger.info(f"Optimization level: {self.opt_profile.optimization_level.value}")
    
    def optimize(self, 
                model_path: Optional[str] = None,
                output_path: Optional[str] = None,
                validation_data: Optional[np.ndarray] = None) -> str:
        """
        Main optimization method.
        
        Args:
            model_path: Path to source model
            output_path: Path for optimized model
            validation_data: Data for validation during optimization
            
        Returns:
            Path to optimized model
        """
        model_path = model_path or self.config.source_model_path
        output_path = output_path or self.config.output_model_path
        
        if not model_path or not Path(model_path).exists():
            raise FileNotFoundError(f"Model not found: {model_path}")
        
        logger.info(f"Starting optimization of {model_path}")
        start_time = time.time()
        
        # Detect model format
        model_format = self._detect_model_format(model_path)
        
        # Select optimization strategy
        strategy = self._select_optimization_strategy()
        
        # Perform optimization based on strategy
        if strategy == OptimizationStrategy.QUANTIZATION_ONLY:
            optimized_path = self._optimize_quantization_only(model_path, output_path)
        
        elif strategy == OptimizationStrategy.PRUNING_ONLY:
            optimized_path = self._optimize_pruning_only(model_path, output_path)
        
        elif strategy == OptimizationStrategy.COMBINED:
            optimized_path = self._optimize_combined(model_path, output_path, validation_data)
        
        elif strategy == OptimizationStrategy.PROGRESSIVE:
            optimized_path = self._optimize_progressive(model_path, output_path, validation_data)
        
        else:
            optimized_path = self._optimize_default(model_path, output_path)
        
        # Validate optimized model
        if validation_data is not None:
            self._validate_optimization(model_path, optimized_path, validation_data)
        
        # Record optimization
        optimization_time = time.time() - start_time
        self._record_optimization(model_path, optimized_path, strategy, optimization_time)
        
        logger.info(f"Optimization completed in {optimization_time:.2f}s")
        logger.info(f"Optimized model saved to: {optimized_path}")
        
        return optimized_path
    
    def _detect_model_format(self, model_path: str) -> str:
        """Detect model format from file extension."""
        suffix = Path(model_path).suffix.lower()
        
        if suffix in ['.pth', '.pt']:
            return 'pytorch'
        elif suffix == '.onnx':
            return 'onnx'
        elif suffix == '.pb':
            return 'tensorflow'
        else:
            raise ValueError(f"Unsupported model format: {suffix}")
    
    def _select_optimization_strategy(self) -> OptimizationStrategy:
        """Select optimization strategy based on configuration."""
        
        opt_level = self.opt_profile.optimization_level
        
        if opt_level == self.opt_profile.optimization_level.BASIC:
            return OptimizationStrategy.QUANTIZATION_ONLY
        
        elif opt_level == self.opt_profile.optimization_level.STANDARD:
            return OptimizationStrategy.COMBINED
        
        elif opt_level == self.opt_profile.optimization_level.AGGRESSIVE:
            return OptimizationStrategy.PROGRESSIVE
        
        elif opt_level == self.opt_profile.optimization_level.MAXIMUM:
            return OptimizationStrategy.PROGRESSIVE
        
        else:
            return OptimizationStrategy.QUANTIZATION_ONLY
    
    def _optimize_quantization_only(self, model_path: str, output_path: str) -> str:
        """Optimize using quantization only."""
        
        logger.info("Applying quantization optimization")
        
        if model_path.endswith('.onnx'):
            return self._quantize_onnx_model(model_path, output_path)
        
        elif model_path.endswith(('.pth', '.pt')):
            return self._quantize_pytorch_model(model_path, output_path)
        
        else:
            raise ValueError(f"Quantization not supported for {model_path}")
    
    def _optimize_pruning_only(self, model_path: str, output_path: str) -> str:
        """Optimize using pruning only."""
        
        logger.info("Applying pruning optimization")
        
        if model_path.endswith(('.pth', '.pt')):
            return self._prune_pytorch_model(model_path, output_path)
        
        else:
            # Convert to PyTorch for pruning, then back to original format
            temp_pt_path = self._convert_to_pytorch(model_path)
            pruned_pt_path = self._prune_pytorch_model(temp_pt_path, None)
            return self._convert_from_pytorch(pruned_pt_path, output_path, 
                                            self._detect_model_format(model_path))
    
    def _optimize_combined(self, model_path: str, output_path: str, 
                          validation_data: Optional[np.ndarray] = None) -> str:
        """Optimize using combined pruning and quantization."""
        
        logger.info("Applying combined optimization (pruning + quantization)")
        
        # Create temporary paths
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_pruned_path = Path(temp_dir) / "pruned_model"
            
            # Step 1: Pruning
            if model_path.endswith(('.pth', '.pt')):
                pruned_path = self._prune_pytorch_model(model_path, str(temp_pruned_path))
            else:
                # Convert to PyTorch for pruning
                temp_pt_path = self._convert_to_pytorch(model_path)
                pruned_path = self._prune_pytorch_model(temp_pt_path, str(temp_pruned_path))
            
            # Step 2: Quantization
            if validation_data is not None:
                # Fine-tune after pruning if validation data available
                finetuned_path = self._finetune_model(pruned_path, validation_data)
                final_path = self._quantize_pytorch_model(finetuned_path, output_path)
            else:
                final_path = self._quantize_pytorch_model(pruned_path, output_path)
            
            return final_path
    
    def _optimize_progressive(self, model_path: str, output_path: str,
                            validation_data: Optional[np.ndarray] = None) -> str:
        """Progressive optimization with iterative refinement."""
        
        logger.info("Applying progressive optimization")
        
        current_model_path = model_path
        
        with tempfile.TemporaryDirectory() as temp_dir:
            
            # Stage 1: Light pruning
            stage1_path = Path(temp_dir) / "stage1_pruned"
            light_pruning_config = self.opt_profile
            light_pruning_config.pruning_ratio = 0.1  # Light pruning first
            
            current_model_path = self._prune_pytorch_model(current_model_path, str(stage1_path))
            
            # Validate and adjust if needed
            if validation_data is not None:
                accuracy = self._validate_model_accuracy(current_model_path, validation_data)
                if accuracy < self.opt_profile.target_accuracy_retention * 0.9:
                    logger.warning("Accuracy drop detected, reducing optimization aggressiveness")
                    # Revert to lighter optimization
                    return self._optimize_quantization_only(model_path, output_path)
            
            # Stage 2: Quantization
            stage2_path = Path(temp_dir) / "stage2_quantized"
            current_model_path = self._quantize_pytorch_model(current_model_path, str(stage2_path))
            
            # Stage 3: Additional pruning if targets not met
            if self._check_size_target(current_model_path):
                stage3_path = Path(temp_dir) / "stage3_final"
                additional_pruning_config = self.opt_profile
                additional_pruning_config.pruning_ratio = 0.2  # More aggressive
                current_model_path = self._prune_pytorch_model(current_model_path, str(stage3_path))
            
            # Final optimization pass
            final_path = self._apply_final_optimizations(current_model_path, output_path)
            
            return final_path
    
    def _optimize_default(self, model_path: str, output_path: str) -> str:
        """Default optimization strategy."""
        logger.info("Applying default optimization")
        return self._optimize_quantization_only(model_path, output_path)
    
    def _quantize_onnx_model(self, model_path: str, output_path: str) -> str:
        """Quantize ONNX model."""
        
        from onnxruntime.quantization import quantize_dynamic, QuantType
        
        try:
            # Dynamic quantization for CPU deployment
            if self.opt_profile.target_precision == self.opt_profile.target_precision.INT8:
                quantize_dynamic(
                    model_input=model_path,
                    model_output=output_path,
                    weight_type=QuantType.QUInt8
                )
            else:
                # For other precisions, use model optimization
                model = onnx.load(model_path)
                
                # Apply graph optimizations
                optimized_model = optimizer.optimize(model, ['eliminate_deadend'])
                
                # Simplify the model
                simplified_model, check = onnxsim.simplify(optimized_model)
                
                if check:
                    onnx.save(simplified_model, output_path)
                else:
                    onnx.save(optimized_model, output_path)
            
            logger.info(f"ONNX model quantized and saved to {output_path}")
            return output_path
            
        except Exception as e:
            logger.error(f"ONNX quantization failed: {e}")
            # Fallback: just copy the model
            shutil.copy2(model_path, output_path)
            return output_path
    
    def _quantize_pytorch_model(self, model_path: str, output_path: str) -> str:
        """Quantize PyTorch model."""
        
        try:
            # Load PyTorch model
            model = torch.load(model_path, map_location='cpu')
            model.eval()
            
            # Apply dynamic quantization
            if self.opt_profile.target_precision == self.opt_profile.target_precision.INT8:
                quantized_model = torch_quant.quantize_dynamic(
                    model,
                    {nn.Linear, nn.Conv2d, nn.LSTM, nn.GRU},
                    dtype=torch.qint8
                )
            
            elif self.opt_profile.target_precision == self.opt_profile.target_precision.FP16:
                quantized_model = model.half()
            
            else:
                quantized_model = model  # Keep FP32
            
            # Save quantized model
            torch.save(quantized_model, output_path)
            
            logger.info(f"PyTorch model quantized and saved to {output_path}")
            return output_path
            
        except Exception as e:
            logger.error(f"PyTorch quantization failed: {e}")
            # Fallback: just copy the model
            shutil.copy2(model_path, output_path)
            return output_path
    
    def _prune_pytorch_model(self, model_path: str, output_path: str) -> str:
        """Prune PyTorch model."""
        
        if tp is None:
            logger.warning("torch_pruning not available, skipping pruning")
            shutil.copy2(model_path, output_path)
            return output_path
        
        try:
            # Load model
            model = torch.load(model_path, map_location='cpu')
            model.eval()
            
            # Create example input for dependency analysis
            example_inputs = torch.randn(1, 3, 224, 224)  # Adjust based on model
            
            # Initialize pruner
            imp = tp.importance.MagnitudeImportance(p=2)
            
            # Create pruner
            pruner = tp.pruner.MagnitudePruner(
                model,
                example_inputs,
                importance=imp,
                global_pruning=True,
                pruning_ratio=self.opt_profile.pruning_ratio,
                ignored_layers=[],
            )
            
            # Prune the model
            pruner.step()
            
            # Save pruned model
            torch.save(model, output_path)
            
            logger.info(f"PyTorch model pruned ({self.opt_profile.pruning_ratio:.1%}) and saved to {output_path}")
            return output_path
            
        except Exception as e:
            logger.error(f"PyTorch pruning failed: {e}")
            # Fallback: just copy the model
            shutil.copy2(model_path, output_path)
            return output_path
    
    def _finetune_model(self, model_path: str, validation_data: np.ndarray) -> str:
        """Fine-tune model after optimization."""
        
        logger.info("Fine-tuning optimized model")
        
        # This is a placeholder for fine-tuning logic
        # In practice, you would implement model-specific fine-tuning
        
        return model_path  # Return original path for now
    
    def _apply_final_optimizations(self, model_path: str, output_path: str) -> str:
        """Apply final optimization passes."""
        
        # Convert to ONNX for deployment if not already
        if not model_path.endswith('.onnx'):
            onnx_path = self._convert_to_onnx(model_path, output_path)
            
            # Apply ONNX optimizations
            model = onnx.load(onnx_path)
            optimized_model = optimizer.optimize(model, ['eliminate_deadend', 'fuse_consecutive_transposes'])
            onnx.save(optimized_model, output_path)
            
            return output_path
        else:
            shutil.copy2(model_path, output_path)
            return output_path
    
    def _convert_to_pytorch(self, model_path: str) -> str:
        """Convert model to PyTorch format."""
        # Placeholder for format conversion
        # In practice, implement specific converters
        return model_path
    
    def _convert_from_pytorch(self, model_path: str, output_path: str, target_format: str) -> str:
        """Convert from PyTorch to target format."""
        # Placeholder for format conversion
        shutil.copy2(model_path, output_path)
        return output_path
    
    def _convert_to_onnx(self, model_path: str, output_path: str) -> str:
        """Convert model to ONNX format."""
        
        try:
            if model_path.endswith(('.pth', '.pt')):
                # Load PyTorch model
                model = torch.load(model_path, map_location='cpu')
                model.eval()
                
                # Create dummy input
                dummy_input = torch.randn(1, 3, 224, 224)  # Adjust based on model
                
                # Export to ONNX
                torch.onnx.export(
                    model,
                    dummy_input,
                    output_path,
                    export_params=True,
                    opset_version=11,
                    do_constant_folding=True,
                    input_names=['input'],
                    output_names=['output'],
                    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
                )
                
                logger.info(f"Model converted to ONNX: {output_path}")
                return output_path
            
            else:
                shutil.copy2(model_path, output_path)
                return output_path
                
        except Exception as e:
            logger.error(f"ONNX conversion failed: {e}")
            shutil.copy2(model_path, output_path)
            return output_path
    
    def _validate_optimization(self, original_path: str, optimized_path: str, 
                             validation_data: np.ndarray) -> None:
        """Validate optimization results."""
        
        logger.info("Validating optimization results")
        
        # Compare model sizes
        original_size = Path(original_path).stat().st_size / 1024 / 1024  # MB
        optimized_size = Path(optimized_path).stat().st_size / 1024 / 1024  # MB
        
        self.current_metrics.original_size_mb = original_size
        self.current_metrics.optimized_size_mb = optimized_size
        self.current_metrics.compression_ratio = original_size / optimized_size
        
        # Validate accuracy if validation data provided
        if validation_data is not None:
            original_accuracy = self._validate_model_accuracy(original_path, validation_data)
            optimized_accuracy = self._validate_model_accuracy(optimized_path, validation_data)
            
            self.current_metrics.accuracy_retention = optimized_accuracy / original_accuracy
            
            logger.info(f"Original accuracy: {original_accuracy:.4f}")
            logger.info(f"Optimized accuracy: {optimized_accuracy:.4f}")
            logger.info(f"Accuracy retention: {self.current_metrics.accuracy_retention:.4f}")
        
        logger.info(f"Size reduction: {original_size:.2f}MB â†’ {optimized_size:.2f}MB "
                   f"({self.current_metrics.compression_ratio:.2f}x compression)")
    
    def _validate_model_accuracy(self, model_path: str, validation_data: np.ndarray) -> float:
        """Validate model accuracy on validation data."""
        
        try:
            if model_path.endswith('.onnx'):
                session = ort.InferenceSession(model_path)
                input_name = session.get_inputs()[0].name
                
                # Run inference on validation data
                predictions = []
                for sample in validation_data[:100]:  # Use subset for speed
                    pred = session.run(None, {input_name: sample.reshape(1, -1)})
                    predictions.append(pred[0])
                
                # Calculate dummy accuracy (in practice, use real labels)
                accuracy = 0.95  # Placeholder
                
            else:
                # For PyTorch models
                model = torch.load(model_path, map_location='cpu')
                model.eval()
                
                # Calculate accuracy (placeholder)
                accuracy = 0.95
            
            return accuracy
            
        except Exception as e:
            logger.error(f"Accuracy validation failed: {e}")
            return 0.0
    
    def _check_size_target(self, model_path: str) -> bool:
        """Check if size target is met."""
        size_mb = Path(model_path).stat().st_size / 1024 / 1024
        return size_mb > self.opt_profile.max_memory_mb
    
    def _record_optimization(self, original_path: str, optimized_path: str,
                           strategy: OptimizationStrategy, optimization_time: float) -> None:
        """Record optimization results."""
        
        record = {
            'timestamp': time.time(),
            'original_path': original_path,
            'optimized_path': optimized_path,
            'strategy': strategy.value,
            'optimization_time': optimization_time,
            'metrics': self.current_metrics.__dict__,
            'config': {
                'optimization_level': self.opt_profile.optimization_level.value,
                'target_precision': self.opt_profile.target_precision.value,
                'pruning_ratio': self.opt_profile.pruning_ratio,
                'max_memory_mb': self.opt_profile.max_memory_mb
            }
        }
        
        self.optimization_history.append(record)
        
        # Save optimization report
        report_path = Path(optimized_path).parent / "optimization_report.json"
        with open(report_path, 'w') as f:
            json.dump(record, f, indent=2)
        
        logger.info(f"Optimization report saved to {report_path}")
    
    def get_optimization_metrics(self) -> OptimizationMetrics:
        """Get current optimization metrics."""
        return self.current_metrics
    
    def get_optimization_history(self) -> List[Dict[str, Any]]:
        """Get optimization history."""
        return self.optimization_history
    
    def benchmark_model(self, model_path: str, num_iterations: int = 100) -> Dict[str, float]:
        """Benchmark model performance."""
        
        logger.info(f"Benchmarking model: {model_path}")
        
        try:
            if model_path.endswith('.onnx'):
                session = ort.InferenceSession(model_path)
                input_shape = session.get_inputs()[0].shape
                input_name = session.get_inputs()[0].name
                
                # Create random input
                if input_shape[0] == 'batch_size' or input_shape[0] is None:
                    input_shape[0] = 1
                
                dummy_input = np.random.randn(*input_shape).astype(np.float32)
                
                # Warm up
                for _ in range(10):
                    _ = session.run(None, {input_name: dummy_input})
                
                # Benchmark
                start_time = time.time()
                for _ in range(num_iterations):
                    _ = session.run(None, {input_name: dummy_input})
                end_time = time.time()
                
                avg_latency = (end_time - start_time) / num_iterations * 1000  # ms
                throughput = 1000.0 / avg_latency  # FPS
                
                # Memory usage
                import psutil
                process = psutil.Process()
                memory_mb = process.memory_info().rss / 1024 / 1024
                
                return {
                    'avg_latency_ms': avg_latency,
                    'throughput_fps': throughput,
                    'memory_usage_mb': memory_mb,
                    'model_size_mb': Path(model_path).stat().st_size / 1024 / 1024
                }
            
            else:
                # For PyTorch models
                model = torch.load(model_path, map_location='cpu')
                model.eval()
                
                # Create dummy input
                dummy_input = torch.randn(1, 3, 224, 224)
                
                # Warm up
                with torch.no_grad():
                    for _ in range(10):
                        _ = model(dummy_input)
                
                # Benchmark
                start_time = time.time()
                with torch.no_grad():
                    for _ in range(num_iterations):
                        _ = model(dummy_input)
                end_time = time.time()
                
                avg_latency = (end_time - start_time) / num_iterations * 1000  # ms
                throughput = 1000.0 / avg_latency  # FPS
                
                return {
                    'avg_latency_ms': avg_latency,
                    'throughput_fps': throughput,
                    'model_size_mb': Path(model_path).stat().st_size / 1024 / 1024
                }
                
        except Exception as e:
            logger.error(f"Benchmarking failed: {e}")
            return {}

# Utility functions
def create_optimization_report(optimizer: ModelOptimizer) -> Dict[str, Any]:
    """Create comprehensive optimization report."""
    
    metrics = optimizer.get_optimization_metrics()
    history = optimizer.get_optimization_history()
    
    report = {
        'optimization_summary': {
            'total_optimizations': len(history),
            'best_compression_ratio': max([h['metrics']['compression_ratio'] for h in history], default=0),
            'average_accuracy_retention': np.mean([h['metrics']['accuracy_retention'] for h in history if h['metrics']['accuracy_retention'] > 0]),
            'total_size_saved_mb': sum([h['metrics']['memory_reduction_mb'] for h in history]),
        },
        'current_metrics': metrics.__dict__,
        'optimization_history': history,
        'device_profile': optimizer.device_profile.__dict__,
        'optimization_profile': optimizer.opt_profile.__dict__,
        'timestamp': time.time()
    }
    
    return report

def compare_models(original_path: str, optimized_path: str) -> Dict[str, Any]:
    """Compare original and optimized models."""
    
    original_size = Path(original_path).stat().st_size / 1024 / 1024
    optimized_size = Path(optimized_path).stat().st_size / 1024 / 1024
    
    comparison = {
        'size_comparison': {
            'original_mb': original_size,
            'optimized_mb': optimized_size,
            'compression_ratio': original_size / optimized_size,
            'size_reduction_percent': (1 - optimized_size / original_size) * 100
        },
        'format_comparison': {
            'original_format': Path(original_path).suffix,
            'optimized_format': Path(optimized_path).suffix
        }
    }
    
    return comparison
